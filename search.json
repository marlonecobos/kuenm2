[{"path":"https://marlonecobos.github.io/kuenm2/articles/basic_data_cleaning.html","id":"description","dir":"Articles","previous_headings":"","what":"Description","title":"Occurrence Data Cleaning","text":"modeling technique, ecological niche modeling depends quality input data, particularly species occurrence records. Cleaning data critical step minimize biases, reduce errors, ensure meaningful model outcomes. vignette introduces tools available kuenm2 package facilitate cleaning occurrence data prior modeling. guides users loading inspecting data, applying cleaning functions, saving cleaned datasets, within reproducible R workflow. want highlight additional data cleaning filtering steps (e.g., spatial thinning) may necessary depending type model modeling approach user intends adopt. tools presented designed assist basic steps preparing data modeling.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/articles/basic_data_cleaning.html","id":"getting-ready","dir":"Articles","previous_headings":"","what":"Getting ready","title":"Occurrence Data Cleaning","text":"kuenm2 installed yet, please . See Main guide installation instructions. Load kuenm2 required packages, define working directory (needed). general, setting working directory R considered good practice, provides better control files read saved . users working within R project, recommend setting working directory, since least one file saved later stages guide.","code":"# Load packages library(kuenm2) library(terra)  # Current directory getwd()  # Define new directory #setwd(\"YOUR/DIRECTORY\")  # uncomment this line if setting a new directory"},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/articles/basic_data_cleaning.html","id":"import-data","dir":"Articles","previous_headings":"Cleaning data","what":"Import data","title":"Occurrence Data Cleaning","text":"use occurrence records provided within kuenm2 package. example data sets package derived Trindade & Marques (2024). occ_data_noclean object contains 51 valid occurrences Myrcia hatschbachii (tree endemic Southern Brazil) group erroneous records demonstrate cleaning steps.  raster layer group layers include package also used example. bioclimatic variable WorldClim 2.1 10 arc-minute resolution. layer masked using polygon generated drawing minimum convex polygon around records 300 km buffer.   Visualize occurrences records geography:","code":"# Import occurrences data(occ_data_noclean, package = \"kuenm2\")  # Check data structure str(occ_data_noclean) #> 'data.frame':    64 obs. of  3 variables: #>  $ species: chr  \"Myrcia hatschbachii\" \"Myrcia hatschbachii\" \"Myrcia hatschbachii\" \"Myrcia hatschbachii\" ... #>  $ x      : num  -51.3 -50.6 -49.3 -49.8 -50.2 ... #>  $ y      : num  -29 -27.6 -27.8 -26.9 -28.2 ... # Import raster layers var <- rast(system.file(\"extdata\", \"Current_variables.tif\", package = \"kuenm2\"))  # Keep only one layer var <- var$bio_1  # Check variable plot(var) # Visualize occurrences on one variable ## Create an extent based on the layer and the records to see all errors vext <- ext(var)  # extent of layer pext <- apply(occ_data_noclean[, 2:3], 2, range, na.rm = TRUE)  # extent of records  allext <- ext(c(min(pext[1, 1], vext[1]), max(pext[2, 1], vext[2]),                  min(pext[1, 2], vext[3]), max(pext[2, 2], vext[4]))) + 1  # plotting records on the variable plot(var, ext = allext, main = \"Bio 1\") points(occ_data_noclean[, c(\"x\", \"y\")])"},{"path":"https://marlonecobos.github.io/kuenm2/articles/basic_data_cleaning.html","id":"basic-cleaning-steps","dir":"Articles","previous_headings":"Cleaning data","what":"Basic cleaning steps","title":"Occurrence Data Cleaning","text":"basic data cleaning steps implemented kuenm2 help : remove missing data, eliminate duplicates, exclude typically (though always) erroneous coordinates 0 longitude 0 latitude, filter records low coordinate precision based number decimal places. example cleaning missing data. example uses data.frame containing columns “Species”, “x”, “y” (“x” “y” represent longitude latitude, respectively). data.frame includes additional columns considered identifying missing values, users can specify columns use via columns argument (default = NULL, includes columns). function, recommend consulting documentation detailed explanations (e.g., help(remove_missing)).  code uses previous results continues process cleaning data removing duplicates. argument columns can used explained . See full documentation help(remove_duplicates).  Continue process removing coordinates values 0 (zero) longitude latitude (always needed, location valid working marine species). See full documentation help(remove_corrdinates_00).  following lines code take previous result remove coordinates low precision. longitude latitude contain decimal places, may rounded, can problematic areas. step recommended users know coordinate rounding issue. filtering process can also applied longitude latitude independently. See full documentation help(filter_decimal_precision).  Users can perform steps single function follows:","code":"# remove missing data mis <- remove_missing(data = occ_data_noclean, columns = NULL, remove_na = TRUE,                       remove_empty = TRUE)  # quick check nrow(occ_data_noclean) #> [1] 64 nrow(mis) #> [1] 60 # remove exact duplicates mis_dup <- remove_duplicates(data = mis, columns = NULL, keep_all_columns = TRUE)  # quick check nrow(mis) #> [1] 60 nrow(mis_dup) #> [1] 57 # remove records with 0 for x and y coordinates mis_dup_00 <- remove_corrdinates_00(data = mis_dup, x = \"x\", y = \"y\")  # quick check nrow(mis_dup) #> [1] 57 nrow(mis_dup_00) #> [1] 56 # remove coordinates with low decimal precision. mis_dup_00_dec <- filter_decimal_precision(data = mis_dup_00, x = \"x\", y = \"y\",                                             decimal_precision = 2)  # quick check nrow(mis_dup_00) #> [1] 56 nrow(mis_dup_00_dec) #> [1] 51 # all basinc cleaning steps clean_init <- initial_cleaning(data = occ_data_noclean, species = \"species\",                                 x = \"x\", y = \"y\", remove_na = TRUE,                                 remove_empty = TRUE, remove_duplicates = TRUE,                                 by_decimal_precision = TRUE,                                decimal_precision = 2)  # quick check nrow(occ_data_noclean)  # original data #> [1] 64 nrow(clean_init)  # data after all basic cleaning steps #> [1] 51  # a final plot to check par(mfrow = c(2, 2))  ## initial data plot(var, ext = allext, main = \"Initial data\") points(occ_data_noclean[, c(\"x\", \"y\")])  ## data after basic cleaning steps plot(var, ext = allext, main = \"After basic cleaning\") points(clean_init[, c(\"x\", \"y\")])  plot(var, main = \"After basic cleaning (zoom)\") points(clean_init[, c(\"x\", \"y\")])"},{"path":"https://marlonecobos.github.io/kuenm2/articles/basic_data_cleaning.html","id":"other-cleaning-steps","dir":"Articles","previous_headings":"Cleaning data","what":"Other cleaning steps","title":"Occurrence Data Cleaning","text":"Two additional cleaning steps implemented kuenm2, removing cell duplicates moving points valid cells. Removing cell duplicates involves excluding records exact coordinate duplicates located within pixel. process randomly selects one record cell retain. See full documentation help(remove_cell_duplicates).  following lines code help adjust records fall just outside valid raster cells prevent data loss. Given nature resolution raster layers, valid records sometimes perceived outside boundaries cells data. cases, alternative move records nearest valid cell. distance limit applied avoid relocating records far study area. See example use approach. See full documentation help(move_2closest_cell).  function advanced_cleaning facilitates two processes single step:","code":"# exclude duplicates based on raster cell (pixel) celldup <- remove_cell_duplicates(data = clean_init, x = \"x\", y = \"y\",                                   raster_layer = var)  # quick check nrow(clean_init)  # data after all basic cleaning steps #> [1] 51 nrow(celldup)  # plus removing cell duplicates #> [1] 42 # move records to valid pixels moved <- move_2closest_cell(data = celldup, x = \"x\", y = \"y\",                              raster_layer = var, move_limit_distance = 10) #> Moving occurrences to closest pixels...  # quick check nrow(celldup)  # basic cleaning and no cell duplicates #> [1] 42 nrow(moved[moved$condition != \"Not_moved\", ])  # plus moved to valid cells #> [1] 41 # move records to valid pixels clean_data <- advanced_cleaning(data = clean_init, x = \"x\", y = \"y\",                                  raster_layer = var, cell_duplicates = TRUE,                                 move_points_inside = TRUE,                                  move_limit_distance = 10) #> Moving occurrences to closest pixels...  # exclude points not moved clean_data <- clean_data[clean_data$condition != \"Not_moved\", 1:3]  # quick check nrow(occ_data_noclean)  # original data #> [1] 64 nrow(clean_init)  # data after all basic cleaning steps #> [1] 51 nrow(clean_data)  # data after all basic cleaning steps #> [1] 41  # a final plot to check par(mfrow = c(3, 2))  ## initial data plot(var, ext = allext, main = \"Initial\") points(occ_data_noclean[, c(\"x\", \"y\")])  ## data after basic cleaning steps plot(var, ext = allext, main = \"Basic cleaning\") points(clean_init[, c(\"x\", \"y\")])  plot(var, main = \"Basic cleaning (zoom)\") points(clean_init[, c(\"x\", \"y\")])  ## data after basic cleaning steps plot(var, main = \"Final data\") points(clean_data[, c(\"x\", \"y\")])  ## zoom to a particular area, initial data plot(var, xlim = c(-48, -50), ylim = c(-26, -25),  main = \"Initial (zoom +)\") points(occ_data_noclean[, c(\"x\", \"y\")])  ## zoom to a particular area, final data plot(var, xlim = c(-48, -50), ylim = c(-26, -25),  main = \"Final (zoom +)\") points(clean_data[, c(\"x\", \"y\")])"},{"path":"https://marlonecobos.github.io/kuenm2/articles/basic_data_cleaning.html","id":"saving-results","dir":"Articles","previous_headings":"","what":"Saving results","title":"Occurrence Data Cleaning","text":"results data cleaning steps kuenm2 simple data.frames may include additional columns fewer records original dataset. easy way save results writing CSV files. Although multiple options exist saving type data, another useful alternative save RDS file directory. See examples :","code":"# Save as CSV write.csv(clean_data, file = \"Clean_data.csv\", row.names = FALSE)  # Save as RDS saveRDS(clean_data, file = \"Clean_data.rds\")"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_calibration.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Model Calibration","text":"Model calibration computationally challenging process automated kuenm2. step, candidate models trained tested using k-fold cross-validation approach. , models selected based multiple criteria warranty models used later steps robust among candidates. main function used step calibration(). start calibration process, need prepared_data object. details data preparation, please refer vignette prepare data model calibration. start, let’s create two prepared_data object: one using maxnet algorithm, another GLM:","code":"#Load packages library(kuenm2) library(terra) #> terra 1.8.60  # Import occurrences data(occ_data, package = \"kuenm2\")  # Import raster layers var <- rast(system.file(\"extdata\", \"Current_variables.tif\", package = \"kuenm2\"))  # Prepare data for maxnet model d_maxnet <- prepare_data(algorithm = \"maxnet\",                          occ = occ_data,                          x = \"x\", y = \"y\",                          raster_variables = var,                          species = \"Myrcia hatschbachii\",                          categorical_variables = \"SoilType\",                           partition_method = \"kfolds\",                           n_replicates = 4,                          n_background = 1000,                          features = c(\"l\", \"q\", \"lq\", \"lqp\"),                          r_multiplier = c(0.1, 1, 2))  # Prepare data for glm model d_glm <- prepare_data(algorithm = \"glm\",                       occ = occ_data,                       x = \"x\", y = \"y\",                       raster_variables = var,                       species = \"Myrcia hatschbachii\",                       categorical_variables = \"SoilType\",                        partition_method = \"bootstrap\",                        n_replicates = 10,                        train_proportion = 0.7,                       n_background = 300,                       features = c(\"l\", \"q\", \"p\", \"lq\", \"lqp\"),                       r_multiplier = NULL) #Not necessary with glms"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_calibration.html","id":"calibration","dir":"Articles","previous_headings":"","what":"Calibration","title":"Model Calibration","text":"calibration() function fits evaluates candidate models considering follow metrics: Omission error: calculated using models trained separate testing data subsets. Users can specify multiple omission rates considered (e.g., c(5%, 10%)), though one can used threshold selecting best models. Partial ROC: calculated following Peterson et al. (2008). Model complexity (AIC): assessed using models generated complete set occurrences. Unimodality (optional): Assessed beta coefficients quadratic terms, following Arias-Giraldo & Cobos (2024).  summary, calibrate evaluate models, function requires prepared_data object following definitions: Omission Errors: Values ranging 0 100, representing percentage potential error attributed various sources uncertainty data. values utilized calculation omission rates partial ROC. Omission Rate Model Selection: specific omission error threshold used select models. value defines maximum omission rate candidate model can considered selection. Removal Concave Curves: specification whether exclude candidate models exhibit concave curves. Optional arguments allow modifications changing delta AIC threshold model selection (default 2), determining whether add presence samples background (default TRUE), whether employ user-specified weights. comprehensive description arguments, refer ?calibration. example, evaluate models considering two omission errors (5% 10%), model selection based 5% omission error. improve computational speed, can set parallel TRUE specify number cores utilize; candidate models distributed among cores. detect number available cores machine, run parallel::detectCores().","code":""},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_calibration.html","id":"maxnet-models","dir":"Articles","previous_headings":"Calibration","what":"Maxnet Models","title":"Model Calibration","text":"Let’s calibrate maxnet models: calibration() function returns calibration_results object, list containing various essential pieces information model calibration process. elements calibration_results object can explored indexing . example, evaluation metrics stored within calibration_results element:  can also examine details selected models:  printed, calibration_results object provides summary model selection process. includes total number candidate models considered, number models failed fit, number models exhibiting concave curves (along indication whether removed). Additionally, reports number models excluded due non-significant partial ROC (pROC) values, high omission error rates, elevated AIC values. Finally, summary metrics selected models presented.  example, 300 candidate maxnet models fitted, two selected based significant pROC value, low omission error (<10%), low AIC score (<2).","code":"#Calibrate maxnet models m_maxnet <- calibration(data = d_maxnet,                   error_considered = c(5, 10),                  omission_rate = 10,                  parallel = FALSE, #Set TRUE to run in parallel                  ncores = 1) #Define number of cores to run in parallel # Task 1/1: fitting and evaluating models: #   |=====================================================================| 100% #  # Model selection step: # Selecting best among 300 models. # Calculating pROC... #  # Filtering 300 models. # Removing 0 model(s) because they failed to fit. # 135 models were selected with omission rate below 10%. # Selecting 2 final model(s) with delta AIC <2. # Validating pROC of selected models... #   |=====================================================================| 100% # All selected models have significant pROC values. # See first rows of the summary of calibration results head(m_maxnet$calibration_results$Summary[,c(\"ID\", \"Omission_rate_at_10.mean\", \"AICc\",                                       \"Is_concave\")]) #>   ID Omission_rate_at_10.mean     AICc Is_concave #> 1  1                   0.0978 665.8779      FALSE #> 2  2                   0.0978 665.9493      FALSE #> 3  3                   0.0978 665.8956      FALSE #> 4  4                   0.1378 678.2084      FALSE #> 5  5                   0.1378 678.1407      FALSE #> 6  6                   0.1170 678.1182      FALSE # See first rows of the summary of calibration results m_maxnet$selected_models[,c(\"ID\", \"Formulas\", \"R_multiplier\",                       \"Omission_rate_at_10.mean\", \"AICc\", \"Is_concave\")] #>      ID #> 192 192 #> 219 219 #>                                                                                      Formulas #> 192                        ~bio_1 + bio_7 + bio_15 + I(bio_1^2) + I(bio_7^2) + I(bio_15^2) -1 #> 219 ~bio_1 + bio_7 + bio_12 + bio_15 + I(bio_1^2) + I(bio_7^2) + I(bio_12^2) + I(bio_15^2) -1 #>     R_multiplier Omission_rate_at_10.mean     AICc Is_concave #> 192          0.1                   0.0769 608.8669      FALSE #> 219          0.1                   0.0962 610.0462      FALSE print(m_maxnet) #> calibration_results object summary (maxnet) #> ============================================================= #> Species: Myrcia hatschbachii  #> Number of candidate models: 300  #>   - Models removed because they failed to fit: 0  #>   - Models identified with concave curves: 39  #>   - Model with concave curves not removed  #>   - Models removed with non-significant values of pROC: 0  #>   - Models removed with omission error > 10%: 165  #>   - Models removed with delta AIC > 2: 133  #> Selected models: 2  #>   - Up to 5 printed here: #>      ID #> 192 192 #> 219 219 #>                                                                                      Formulas #> 192                        ~bio_1 + bio_7 + bio_15 + I(bio_1^2) + I(bio_7^2) + I(bio_15^2) -1 #> 219 ~bio_1 + bio_7 + bio_12 + bio_15 + I(bio_1^2) + I(bio_7^2) + I(bio_12^2) + I(bio_15^2) -1 #>     Features R_multiplier pval_pROC_at_10.mean Omission_rate_at_10.mean #> 192       lq          0.1                    0                   0.0769 #> 219       lq          0.1                    0                   0.0962 #>         dAIC Parameters #> 192 0.000000          6 #> 219 1.179293          7"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_calibration.html","id":"glm-models","dir":"Articles","previous_headings":"Calibration","what":"GLM Models","title":"Model Calibration","text":"Now, let’s calibrate GLM Models see algorith achieve different selected models: Now, instead two selected models, one:","code":"#Calibrate maxnet models m_glm <- calibration(data = d_glm,                       error_considered = c(5, 10),                      omission_rate = 10,                      parallel = FALSE, #Set TRUE to run in parallel                      ncores = 1) #Define number of cores to run in parallel # Task 1/1: fitting and evaluating models: #   |=====================================================================| 100% # Model selection step: # Selecting best among 122 models. # Calculating pROC... #  # Filtering 122 models. # Removing 0 model(s) because they failed to fit. # 21 models were selected with omission rate below 10%. # Selecting 1 final model(s) with delta AIC <2. # Validating pROC of selected models... #   |=====================================================================| 100% # All selected models have significant pROC values. m_glm #> calibration_results object summary (glm) #> ============================================================= #> Species: Myrcia hatschbachii  #> Number of candidate models: 122  #>   - Models removed because they failed to fit: 0  #>   - Models identified with concave curves: 18  #>   - Model with concave curves not removed  #>   - Models removed with non-significant values of pROC: 0  #>   - Models removed with omission error > 10%: 101  #>   - Models removed with delta AIC > 2: 20  #> Selected models: 1  #>   - Up to 5 printed here: #>    ID                                                        Formulas Features #> 85 85 ~bio_1 + bio_7 + bio_12 + I(bio_1^2) + I(bio_7^2) + I(bio_12^2)       lq #>    pval_pROC_at_10.mean Omission_rate_at_10.mean dAIC Parameters #> 85                    0                   0.0904    0          6"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_calibration.html","id":"concave-curves","dir":"Articles","previous_headings":"Calibration","what":"Concave curves","title":"Model Calibration","text":"worth noting maxnet glm algorithm, models identified concave curves. Concave (bimodal) curves indicate peak suitability found extremes variable range. example, shown right panel figure , higher suitability observed driest wettest regions, lower suitabilities occurring intermediate precipitation levels.   example, none selected models concave curves:  However, occasionally, model concave curves might selected sufficiently low omission rate AIC values. ensure none selected models concave curves, can set remove_concave = TRUE within calibration() function. Let’s test maxnet algorithm: Note process now divided two tasks: Task 1/2: candidate models include quadratic terms fitted. Maxent models (using maxnet algorithm), function first fits candidate model highest regularization multiplier (e.g., 5) formula. approach used particular formula produces concave response high regularization value, also produce concave responses lower regularization values. checking model highest regularization first, function can skip fitting models formula lower regularization values, saving time computation. Task 2/2: step, function fits evaluates two groups models: Models without quadratic terms. Models quadratic terms, formulas produce concave responses Task 1/2 (.e., passed concavity check higher regularization multiplier).","code":"#Selected maxnet models m_maxnet$selected_models[,c(\"ID\", \"Formulas\", \"Is_concave\")] #>      ID #> 192 192 #> 219 219 #>                                                                                      Formulas #> 192                        ~bio_1 + bio_7 + bio_15 + I(bio_1^2) + I(bio_7^2) + I(bio_15^2) -1 #> 219 ~bio_1 + bio_7 + bio_12 + bio_15 + I(bio_1^2) + I(bio_7^2) + I(bio_12^2) + I(bio_15^2) -1 #>     Is_concave #> 192      FALSE #> 219      FALSE  #Selected glm models m_glm$selected_models[,c(\"ID\", \"Formulas\", \"Is_concave\")] #>    ID                                                        Formulas #> 85 85 ~bio_1 + bio_7 + bio_12 + I(bio_1^2) + I(bio_7^2) + I(bio_12^2) #>    Is_concave #> 85       TRUE m_unimodal <- calibration(data = d_maxnet,                            remove_concave = TRUE, # Ensures concave models are not selected                           error_considered = c(5, 10),                           omission_rate = 10) # Task 1/2: checking for concave responses in models: #   |=====================================================================| 100% #  # Task 2/2: fitting and evaluating models with no concave responses: #   |=====================================================================| 100% #  # Model selection step: # Selecting best among 300 models. # Calculating pROC... #  # Filtering 300 models. # Removing 0 model(s) because they failed to fit. # Removing 39 model(s) with concave curves. # 110 models were selected with omission rate below 10%. # Selecting 2 final model(s) with delta AIC <2. # Validating pROC of selected models... #   |=====================================================================| 100% # All selected models have significant pROC values."},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_calibration.html","id":"re-selecting-models","dir":"Articles","previous_headings":"","what":"Re-selecting models","title":"Model Calibration","text":"model selection procedure conducted internally calibration process. However, possible re-select models considering omission rates (since calculated calibration), model complexity (delta AIC) concave curves. default, calibration() calculates pROC values selected models optimize computational time. Consequently, pROC values non-selected models filled NA.  pROC calculated models calibration(), select_models() function requires prepared_data used calibration step, compute_proc must set TRUE. instance, let’s re-select maxnet models calibration results, applying omission rate 5% instead 10%:  calibration_results object provided, select_models() return calibration_results output selected models summary updated. Note now different selected models maxnet algoritm:  can also provide data.frame containing evaluation metrics candidate model directly select_models(). data.frame available output calibration() function object$calibration_results$Summary. case, function return list containing selected models along summaries model selection process.","code":"# See first rows of the summary of calibration results (pROC values) head(m_maxnet$calibration_results$Summary[,c(\"ID\", \"Mean_AUC_ratio_at_10.mean\",                                  \"pval_pROC_at_10.mean\")]) #>   ID Mean_AUC_ratio_at_10.mean pval_pROC_at_10.mean #> 1  1                        NA                   NA #> 2  2                        NA                   NA #> 3  3                        NA                   NA #> 4  4                        NA                   NA #> 5  5                        NA                   NA #> 6  6                        NA                   NA # See pROC values of selected models m_maxnet$selected_models[,c(\"ID\", \"Mean_AUC_ratio_at_10.mean\",                                  \"pval_pROC_at_10.mean\")] #>      ID Mean_AUC_ratio_at_10.mean pval_pROC_at_10.mean #> 192 192                  1.497376                    0 #> 219 219                  1.502309                    0 #Re-select maxnet models new_m_maxnet <- select_models(calibration_results = m_maxnet,                                data = d_maxnet, #Necessary for computing pROC                               compute_proc = TRUE,                                omission_rate = 5) # New omission rate #> Selecting best among 300 models. #> Calculating pROC... #>  #> Filtering 300 models. #> Removing 0 model(s) because they failed to fit. #> 116 models were selected with omission rate below 5%. #> Selecting 2 final model(s) with delta AIC <2. #> Validating pROC of selected models... #>  #> All selected models have significant pROC values. print(new_m_maxnet) #> calibration_results object summary (maxnet) #> ============================================================= #> Species: Myrcia hatschbachii  #> Number of candidate models: 300  #>   - Models removed because they failed to fit: 0  #>   - Models identified with concave curves: 39  #>   - Model with concave curves not removed  #>   - Models removed with non-significant values of pROC: 0  #>   - Models removed with omission error > 5%: 184  #>   - Models removed with delta AIC > 2: 114  #> Selected models: 2  #>   - Up to 5 printed here: #>      ID                                                           Formulas #> 159 159                        ~bio_1 + bio_7 + I(bio_1^2) + I(bio_7^2) -1 #> 189 189 ~bio_1 + bio_7 + bio_12 + I(bio_1^2) + I(bio_7^2) + I(bio_12^2) -1 #>     Features R_multiplier pval_pROC_at_5.mean Omission_rate_at_5.mean      dAIC #> 159       lq          0.1                   0                  0.0192 0.8581936 #> 189       lq          0.1                   0                  0.0192 0.0000000 #>     Parameters #> 159          4 #> 189          6 new_m_maxnet$selected_models[,c(\"ID\", \"Formulas\", \"R_multiplier\",                       \"Omission_rate_at_5.mean\", \"Mean_AUC_ratio_at_5.mean\",                      \"AICc\", \"Is_concave\")] #>      ID                                                           Formulas #> 159 159                        ~bio_1 + bio_7 + I(bio_1^2) + I(bio_7^2) -1 #> 189 189 ~bio_1 + bio_7 + bio_12 + I(bio_1^2) + I(bio_7^2) + I(bio_12^2) -1 #>     R_multiplier Omission_rate_at_5.mean Mean_AUC_ratio_at_5.mean     AICc #> 159          0.1                  0.0192                 1.480752 622.7677 #> 189          0.1                  0.0192                 1.512442 621.9095 #>     Is_concave #> 159      FALSE #> 189      FALSE #Re-select models using data.frame new_summary <- select_models(candidate_models = m_maxnet$calibration_results$Summary,                              data = d_maxnet, #Necessary for computing pROC                              compute_proc = TRUE,                               omission_rate = 5) #> Selecting best among 300 models. #> Calculating pROC... #>  #> Filtering 300 models. #> Removing 0 model(s) because they failed to fit. #> 116 models were selected with omission rate below 5%. #> Selecting 2 final model(s) with delta AIC <2. #> Validating pROC of selected models... #>  #> All selected models have significant pROC values.  #Get class of object class(new_summary) #> [1] \"list\"  #See selected models new_summary$selected_models[,c(\"ID\", \"Formulas\", \"R_multiplier\",                       \"Omission_rate_at_5.mean\", \"Mean_AUC_ratio_at_5.mean\",                      \"AICc\", \"Is_concave\")] #>      ID                                                           Formulas #> 159 159                        ~bio_1 + bio_7 + I(bio_1^2) + I(bio_7^2) -1 #> 189 189 ~bio_1 + bio_7 + bio_12 + I(bio_1^2) + I(bio_7^2) + I(bio_12^2) -1 #>     R_multiplier Omission_rate_at_5.mean Mean_AUC_ratio_at_5.mean     AICc #> 159          0.1                  0.0192                 1.479944 622.7677 #> 189          0.1                  0.0192                 1.511803 621.9095 #>     Is_concave #> 159      FALSE #> 189      FALSE"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_calibration.html","id":"saving-a-calibration_results-object","dir":"Articles","previous_headings":"","what":"Saving a calibration_results object","title":"Model Calibration","text":"calibrating selecting best-performing models, can proceed fit final models (see vignette model exploration) using calibration_results object. object essentially list, users can save local directory using saveRDS(). Saving object facilitates loading back R session later using readRDS(). See example :","code":"# Set directory to save (here, in a temporary directory) dir_to_save <- file.path(tempdir())  # Save the data saveRDS(m_maxnet, file.path(dir_to_save, \"Candidates_maxnet.rds\"))  # Import data m_maxnet <- readRDS(file.path(dir_to_save, \"Candidates_maxnet.rds\"))"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_exploration.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Fit and Explore Selected Models","text":"best performing models selected, users need fit models (using fit_selected()) order explore characteristics continue next steps. Fitted models can used assess variable importance models, well explore variable response curves. can also evaluate selected models using independent presence records used model calibration. fit selected models, need calibration_results object. details model calibration, please refer vignette Model Calibration. calibration_results object generated vignette available data example package. Let’s load . object contains results candidate models calibrated using maxnet algorithm. package also provides similar example using glm algorithm, works exactly way. Note calibration_results object stores information related calibration process model evaluation—include fitted maxnet (glm) models . obtain final fitted models, need use fit_selected() function. default, function fits full model (.e., without replicates without splitting data training testing sets). However, can configure fit final models replicates desired. example, ’ll fit final models using replication settings (4-fold cross-validation) used Model Calibration vignette.  fit_selected() function returns fitted_models object, list contains essential information fitted models, required subsequent steps. can explore contents fitted_models object indexing elements. example, fitted maxnet (glm) model objects stored within Models element. Note Models nested list: selected model (case, models 192 219), includes replicates (fitted replicates) full model. fitted_models object also stores thresholds can used binarize models suitable unsuitable areas. thresholds correspond omission error used model selection (e.g., 5% 10%). can access omission error used calculate thresholds directly object: omission error used calculate thresholds 10%, meaning predictions binarized, approximately 10% presence records used calibrate models fall cells predicted values threshold. thresholds summarized two ways: mean median across replicates model, consensus mean median across selected models (one model selected): Now, can use fitted_models object generate response curves compute variable importance.","code":"#Load packages library(kuenm2) library(terra) #> terra 1.8.60  #Import calib_results_maxnet data(\"calib_results_maxnet\", package = \"kuenm2\") #Print calibration result calib_results_maxnet #> calibration_results object summary (maxnet) #> ============================================================= #> Species: Myrcia hatschbachii  #> Number of candidate models: 300  #>   - Models removed because they failed to fit: 0  #>   - Models identified with concave curves: 39  #>   - Model with concave curves not removed  #>   - Models removed with non-significant values of pROC: 0  #>   - Models removed with omission error > 10%: 165  #>   - Models removed with delta AIC > 2: 133  #> Selected models: 2  #>   - Up to 5 printed here: #>      ID #> 192 192 #> 219 219 #>                                                                                      Formulas #> 192                        ~bio_1 + bio_7 + bio_15 + I(bio_1^2) + I(bio_7^2) + I(bio_15^2) -1 #> 219 ~bio_1 + bio_7 + bio_12 + bio_15 + I(bio_1^2) + I(bio_7^2) + I(bio_12^2) + I(bio_15^2) -1 #>     Features R_multiplier pval_pROC_at_10.mean Omission_rate_at_10.mean #> 192       lq          0.1                    0                   0.0769 #> 219       lq          0.1                    0                   0.0962 #>         dAIC Parameters #> 192 0.000000          6 #> 219 1.179293          7 #Import calib_results_glm data(\"calib_results_glm\", package = \"kuenm2\") #Print calibration result calib_results_glm #> calibration_results object summary (glm) #> ============================================================= #> Species: Myrcia hatschbachii  #> Number of candidate models: 122  #>   - Models removed because they failed to fit: 0  #>   - Models identified with concave curves: 18  #>   - Model with concave curves not removed  #>   - Models removed with non-significant values of pROC: 0  #>   - Models removed with omission error > 10%: 101  #>   - Models removed with delta AIC > 2: 20  #> Selected models: 1  #>   - Up to 5 printed here: #>    ID                                                        Formulas Features #> 85 85 ~bio_1 + bio_7 + bio_12 + I(bio_1^2) + I(bio_7^2) + I(bio_12^2)       lq #>    pval_pROC_at_10.mean Omission_rate_at_10.mean dAIC Parameters #> 85                    0                   0.0904    0          6 # Fit selected models using calibration results fm <- fit_selected(calibration_results = calib_results_maxnet,                     partition_method = \"kfolds\", n_replicates = 4) # Fitting replicates... #   |========================================================================| 100% # Fitting full models... #   |========================================================================| 100% #See names of selected models names(fm$Models) #> [1] \"Model_192\" \"Model_219\"  #See models inside Model 192 names(fm$Models$Model_192) #> [1] \"Rep_1\"      \"Rep_2\"      \"Rep_3\"      \"Rep_4\"      \"Full_model\" #Get omission error used to select models and calculate the thesholds fm$omission_rate #> [1] 10 fm$thresholds #> $Model_192 #> $Model_192$mean #> [1] 0.3114352 #>  #> $Model_192$median #> [1] 0.2585349 #>  #>  #> $Model_219 #> $Model_219$mean #> [1] 0.3094853 #>  #> $Model_219$median #> [1] 0.2605331 #>  #>  #> $consensus #> $consensus$mean #> [1] 0.3095083 #>  #> $consensus$median #> [1] 0.259534 #>  #>  #> $type #> [1] \"cloglog\""},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_exploration.html","id":"response-curve","dir":"Articles","previous_headings":"","what":"Response curve","title":"Fit and Explore Selected Models","text":"response curves illustrate environmental variable influences predicted suitability, keeping variables constant. default, curves generated variables set mean values (mode categorical variables), calculated combined set presence background localities (averages_from = \"pr_bg\"). can change behavior use presence localities setting averages_from = \"pr\". Let’s check variables available plot examining coefficients full models: variables bio_1, bio_7, bio_12 bio_15have non-zero coefficient values, means contribute model available generating response curves. default, response curves computed using selected models. resulting plots include line mean response, along shaded area representing 95% confidence interval.  can also specify selected models used generate response curves:  dashed lines indicate range variable within calibration data. default, plot extends beyond limits based variable’s minimum maximum values extrapolation_factor (extrapolation = TRUE). default extrapolation set 10% variable’s range (.e., range × 0.1). extrapolation = FALSE, extrapolation occurs, plot limits match calibration data range exactly. can increase extrapolation factor allow broader range beyond observed data. response curve plotted extrapolation factor 2:  Note response curve now extends beyond observed data range (indicated dashed lines). Optionally, can manually set lower upper limits variables. example, since bio_12 represents annual precipitation negative values meaningful, can set lower limit 0:  Now, lower limit plot bio_12 set 0. Since specify upper limit, plot uses extrapolation factor (, 0.1) define upper limit. Optionally, can add original presence background points plot setting add_point = TRUE:","code":"#Get variables with non-zero coefficients in the models fm$Models[[1]]$Full_model$betas #From the first model selected #>        bio_1        bio_7       bio_15   I(bio_1^2)   I(bio_7^2)  I(bio_15^2)  #> 11.572321659  0.215970079  0.369077970 -0.356605446 -0.020306099 -0.006200151 fm$Models[[2]]$Full_model$betas #From the second model selected #>         bio_1        bio_12        bio_15    I(bio_1^2)    I(bio_7^2)  #>  1.178814e+01  1.638570e-02  3.406100e-01 -3.625405e-01 -1.440450e-02  #>   I(bio_12^2)   I(bio_15^2)  #> -5.261860e-06 -5.623987e-03 par(mfrow = c(2, 2)) #Set grid of plot response_curve(models = fm, variable = \"bio_1\") response_curve(models = fm, variable = \"bio_7\") response_curve(models = fm, variable = \"bio_12\") response_curve(models = fm, variable = \"bio_15\") on.exit() #Reinitiate grid par(mfrow = c(2, 2)) #Set grid of plot response_curve(models = fm, variable = \"bio_1\",                modelID = \"Model_192\", main = \"Model_192\") response_curve(models = fm, variable = \"bio_1\",                 modelID = \"Model_219\", main = \"Model_219\") response_curve(models = fm, variable = \"bio_7\",                 modelID = \"Model_192\", main = \"Model_192\") response_curve(models = fm, variable = \"bio_7\",                 modelID = \"Model_219\", main = \"Model_219\") on.exit() #Reinitiate grid par(mfrow = c(2, 2)) #Set grid of plot response_curve(models = fm, variable = \"bio_1\", extrapolation_factor = 2) response_curve(models = fm, variable = \"bio_7\", extrapolation_factor = 2) response_curve(models = fm, variable = \"bio_12\", extrapolation_factor = 2) response_curve(models = fm, variable = \"bio_15\", extrapolation_factor = 2) on.exit() #Reinitiate grid response_curve(models = fm, variable = \"bio_12\",                 extrapolation_factor = 0.1,                 l_limit = 0) response_curve(models = fm, variable = \"bio_1\",                 add_points = TRUE)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_exploration.html","id":"variable-importance","dir":"Articles","previous_headings":"","what":"Variable importance","title":"Fit and Explore Selected Models","text":"relative importance predictor variables can calculated using explained deviance var_importance() function. process starts fitting full model (maxnet glm), includes predictor variables. , function fits separate models excluding one variable time, assessing removal affects model performance. systematically evaluating impact predictor’s exclusion, function provides insights individual contribution variable model’s overall performance explanatory power. default, function runs single core. can enable parallel processing setting parallel = TRUE specifying number cores ncores. Note parallelization speeds computation many variables (7) large calibration dataset (15,000 presence background points). default, variable importance computed selected models: function returns data.frame relative contribution variable. multiple models included fitted object, additional column identifies distinct model. can visualize variable importance using plot_importance() function. fitted_models object contains one selected model, plot displays boxplot contributions, along mean contribution number (N) fitted models.  variable importance computed single model, plot displays barplot instead boxplot:","code":"# Calculate variable importance imp <- variable_importance(models = fm)  # Calculating variable contribution for model 1 of 2 #   |======================================================================| 100% # Calculating variable contribution for model 2 of 2 #   |======================================================================| 100% imp #>      predictor contribution    Models #> 1   I(bio_1^2)  0.330546900 Model_192 #> 2        bio_1  0.295001412 Model_192 #> 3       bio_15  0.209668355 Model_192 #> 4  I(bio_15^2)  0.156926301 Model_192 #> 5   I(bio_7^2)  0.006279520 Model_192 #> 6        bio_7  0.001577512 Model_192 #> 7   I(bio_1^2)  0.368476226 Model_219 #> 8        bio_1  0.332782286 Model_219 #> 9       bio_15  0.155708606 Model_219 #> 10 I(bio_15^2)  0.095186390 Model_219 #> 11 I(bio_12^2)  0.022722557 Model_219 #> 12      bio_12  0.021793617 Model_219 #> 13  I(bio_7^2)  0.003330318 Model_219 plot_importance(imp) # Calculate variable importance for a specific selected Model imp_192<- variable_importance(models = fm, modelID = \"Model_192\",                                 progress_bar = FALSE) #Plot variable contribution for model 192 plot_importance(imp_192, main = \"Variable importance - Model 192\")"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_exploration.html","id":"evaluate-models-with-independent-data","dir":"Articles","previous_headings":"","what":"Evaluate models with independent data","title":"Fit and Explore Selected Models","text":"can evaluate selected models using independent set presence records used model calibration. approach especially useful new records become available working invasive species. latter case, models first calibrated using presence data native area subsequently evaluated data invaded area. independent_eval() function computes omission rate - .e., proportion independent records fall unsuitable areas, “unsuitable” defined according omission threshold used model calibration selection (e.g., 10%) - well partial ROC. also assesses whether environmental conditions independent data analogous (.e., within range) calibration data. example independent data, let’s use new_occ data example provided package. contains coordinates Myrcia hatschbachii sourced NeotropicTree (Oliveira-Filho, 2017), part occ_dataused train models. predicting model new set occurrence records, need extract environmental conditions locals. Let’s import variables used fit models extract new_occ conditions: Especially using independent records fall outside calibration area (e.g., invaded region another continent), common environmental conditions new records non-analogous (.e., outside range) calibration data. better illustrate case, let’s add three fake records, variables non-analogous values, either higher upper limit lower lower limit observed calibration data. Now, let’s evaluate independent dataset (keep mind last three records fake): output list three elements. first one, evaluation, presents evaluation metrics (omission rate pROC) selected model, well overall consensus. can see selected models significant pROC values, show higher omission rates (around 40% independent records fall areas predicted unsuitable) compared threshold specified model calibration (10%). perform_mop set TRUE, function also returns mop_results, list containing output mop::mop() function. main results MOP analysis appended predictions element list. records, following information provided: mop_distance: distance (.e., dissimilarity) environmental conditions calibration data location independent record. inside_range: wheter environmental conditions location independent record fall within calibration range. n_var_out: number variables location independent record non-analogous (.e., fall outside calibration range). towards_low: names variables values lower minimum observed calibration data. towards_high: names variables values higher maximum observed calibration data. Note two three fake records added new_data non-analogous environmental conditions. One falls location bio_7 bio_1 values lower minimum observed calibration data, bio_12 value higher maximum. Another record location bio_12 calibration range, bio_1 exceeds upper limit. setreturn_predictions = TRUE, function also returns continuous predictions selected model general consensus: set return_binary = TRUE, function also returns binary predictions, values classified suitable (1) unsuitable (0), based threshold calculated using omission rate applied model evaluation selection: results help determine whether independent data incorporated calibration dataset re-run models. omission rates independent records high, pROC values non-significant, records fall locations non-analogous environmental conditions, suggests may good idea re-run models, including independent data time.","code":"# Import independent records data(\"new_occ\", package = \"kuenm2\") # See data structure str(new_occ) #> Classes 'data.table' and 'data.frame':   82 obs. of  3 variables: #>  $ species: chr  \"Myrcia hatschbachii\" \"Myrcia hatschbachii\" \"Myrcia hatschbachii\" \"Myrcia hatschbachii\" ... #>  $ x      : num  -48.3 -49.1 -49.9 -49.4 -49.9 ... #>  $ y      : num  -25.2 -25 -24.5 -24.5 -24.8 ... #>  - attr(*, \".internal.selfref\")=<externalptr> # Import raster layers var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  # Extract variables to occurrences new_data <- extract_occurrence_variables(occ = new_occ, x = \"x\", y = \"y\",                                          raster_variables = var) # See data structure str(new_data) #> 'data.frame':    82 obs. of  8 variables: #>  $ pr_bg   : num  1 1 1 1 1 1 1 1 1 1 ... #>  $ x       : num  -48.3 -49.1 -49.9 -49.4 -49.9 ... #>  $ y       : num  -25.2 -25 -24.5 -24.5 -24.8 ... #>  $ bio_1   : num  20.2 18 16.6 17.8 16.7 ... #>  $ bio_7   : num  16.7 18.2 19.9 19.4 20.1 ... #>  $ bio_12  : num  2015 1456 1526 1414 1578 ... #>  $ bio_15  : num  43.8 33.7 29.1 32.2 26.5 ... #>  $ SoilType: num  6 6 6 6 6 10 10 10 10 10 ... #Add some fake data beyond the limits of calibration ranges fake_data <- data.frame(\"pr_bg\" = c(1, 1, 1),                         \"x\" = c(NA, NA, NA),                         \"y\" = c(NA, NA, NA),                         \"bio_1\" = c(10, 15, 23),                         \"bio_7\" = c(12, 16, 20),                         \"bio_12\" = c(2300, 2000, 1000),                         \"bio_15\" = c(30, 40, 50),                         \"SoilType\" = c(1, 1, 1)) # Bind data new_data <- rbind(new_data, fake_data) # Evaluate models with independent data res_ind <- independent_eval(fitted_models = fitted_model_maxnet,                             new_data = new_data) res_ind$evaluation #>               Model consensus Omission_rate_at_10 Mean_AUC_ratio pval_pROC #> 1         Model_192      mean           0.4352941       1.166789         0 #> 2         Model_192    median           0.4000000       1.139045         0 #> 3         Model_219      mean           0.4235294       1.190714         0 #> 4         Model_219    median           0.4000000       1.138885         0 #> 5 General_consensus    median           0.4000000       1.136853         0 #> 6 General_consensus      mean           0.4352941       1.180920         0 # Show the mop results for the last 5 independent records res_ind$predictions$continuous[81:85 ,c(\"mop_distance\", \"inside_range\", \"n_var_out\",                                    \"towards_low\", \"towards_high\")] #>    mop_distance inside_range n_var_out  towards_low towards_high #> 81     7.753558         TRUE         0         <NA>         <NA> #> 82     6.622770         TRUE         0         <NA>         <NA> #> 83   157.782905        FALSE         3 bio_7, bio_1       bio_12 #> 84    21.045482         TRUE         0         <NA>         <NA> #> 85   183.905420        FALSE         2       bio_12        bio_1 # Show the continuous predictions for the last 5 independent records # Round to two decimal places round(res_ind$predictions$continuous[81:85, 1:6], 2) #>    Model_192.mean Model_192.median Model_219.mean Model_219.median #> 81           0.57             0.63           0.58             0.65 #> 82           0.54             0.60           0.55             0.62 #> 83           1.00             1.00           0.96             1.00 #> 84           1.00             1.00           1.00             1.00 #> 85           0.00             0.00           0.00             0.00 #>    General_consensus.median General_consensus.mean #> 81                     0.63                   0.58 #> 82                     0.61                   0.54 #> 83                     1.00                   0.98 #> 84                     1.00                   1.00 #> 85                     0.00                   0.00 # Show the continuous predictions for the last 5 independent records res_ind$predictions$binary[81:85, 1:6] #>    Model_192.mean Model_192.median Model_219.mean Model_219.median #> 81              1                1              1                1 #> 82              1                1              1                1 #> 83              1                1              1                1 #> 84              1                1              1                1 #> 85              0                0              0                0 #>    General_consensus.mean General_consensus.median #> 81                      1                        1 #> 82                      1                        1 #> 83                      1                        1 #> 84                      1                        1 #> 85                      0                        0"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_exploration.html","id":"saving-a-fitted_models-object","dir":"Articles","previous_headings":"","what":"Saving a fitted_models object","title":"Fit and Explore Selected Models","text":"fitting best-performing models fit_selected(), can proceed predict models single multiple scenarios. object essentially list, users can save local directory using saveRDS(). Saving object facilitates loading back R session later using readRDS(). See example :","code":"# Set directory to save (here, in a temporary directory) dir_to_save <- file.path(tempdir())  # Save the data: saveRDS(fm, file.path(dir_to_save, \"fitted_models.rds\"))  # Import data fm <- readRDS(file.path(dir_to_save, \"fitted_models.rds\"))"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_predictions.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Predict models to single scenario","text":"selected models fit using fit_selected(), projections single multiple scenarios can performed. predict_selected() function designed projections single scenarios. predict using selected models, fitted_models object required. detailed information model fitting, please consult vignette Fit Explore Selected Models. fitted_models object generated vignette included example dataset within package. Let’s load . compare results, let’s import fitted_models object generated using GLM algorithm:","code":"#Load packages library(kuenm2) library(terra) #> terra 1.8.60  #Import calib_results_maxnet data(\"fitted_model_maxnet\", package = \"kuenm2\") #Print calibration result fitted_model_maxnet #> fitted_models object summary #> ============================ #> Species: Myrcia hatschbachii  #> Algortihm: maxnet  #> Number of fitted models: 2  #> Models fitted with 4 replicates #Import calib_results_maxnet data(\"fitted_model_glm\", package = \"kuenm2\") #Print calibration result fitted_model_glm #> fitted_models object summary #> ============================ #> Species: Myrcia hatschbachii  #> Algortihm: glm  #> Number of fitted models: 1  #> Models fitted with 10 replicates"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_predictions.html","id":"predict-selected-models-for-a-single-scenario","dir":"Articles","previous_headings":"","what":"Predict Selected Models for a Single Scenario","title":"Predict models to single scenario","text":"predict selected models single scenario, need fitted_models object corresponding predictor variables. predictor variables can provided either SpatRaster data.frame. names variables (columns data.frame) must precisely match used model calibration used running PCA (do_pca = TRUE set prepare_data() function; see Prepare Data Model Calibration details).","code":""},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_predictions.html","id":"predict-to-spatraster","dir":"Articles","previous_headings":"Predict Selected Models for a Single Scenario","what":"Predict to SpatRaster","title":"Predict models to single scenario","text":"Let’s use raster variables used prepare data calibrate models. included example data within package:  Let’s check variables used calibrate models. available calibration_data element object: first column, “pr_bg”, indicates presence (1) absence (0) records, columns represent environmental variables. case, variables bio_1, bio_7, bio_12, bio_15, SoilType. variables present SpatRaster (var) imported. Therefore, can now predict models raster. Let’s begin predicting maxnet model: default, function computes consensus metrics (mean, median, range, standard deviation) model across replicates (one model selected), well general consensus across models. case, output list containing SpatRaster predictions replicate, along consensus results model overall general consensus: Let’s plot general consensus:  can also plot results replicate consensus model:    comparison, let’s predict GLM model:","code":"# Import raster layers var <- rast(system.file(\"extdata\", \"Current_variables.tif\", package = \"kuenm2\")) #Plot raster layers plot(var) # Variables used to calibrate maxnet models colnames(fitted_model_maxnet$calibration_data) #> [1] \"pr_bg\"    \"bio_1\"    \"bio_7\"    \"bio_12\"   \"bio_15\"   \"SoilType\"  #Variables used to calibrate glm models colnames(fitted_model_glm$calibration_data) #> [1] \"pr_bg\"    \"bio_1\"    \"bio_7\"    \"bio_12\"   \"bio_15\"   \"SoilType\" p_maxnet <- predict_selected(models = fitted_model_maxnet,                               raster_variables = var,                              progress_bar = FALSE) #See objects in the output of predict_selected names(p_maxnet) #> [1] \"Model_192\"         \"Model_219\"         \"General_consensus\" plot(p_maxnet$General_consensus) #Predictions for each replicate from model 192 plot(p_maxnet$Model_192$Replicates) #Consensus across each replicate from model 192 plot(p_maxnet$Model_192$Model_consensus) # Predict glm model p_glm <- predict_selected(models = fitted_model_glm,                            raster_variables = var,                           progress_bar = FALSE) #See selected models that were predicted names(p_glm) #> [1] \"Model_85\"          \"General_consensus\"  #Compare general consensus (mean) between maxnet and glm par(mfrow= c(1, 2)) #Set grid to plot plot(p_maxnet$General_consensus$mean, main = \"Maxnet\") plot(p_glm$General_consensus$mean, main = \"GLM\") on.exit() #Reinitiate grid"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_predictions.html","id":"predict-to-data-frame","dir":"Articles","previous_headings":"Predict Selected Models for a Single Scenario","what":"Predict to data.frame","title":"Predict models to single scenario","text":"Instead SpatRaster, can also predict models data.frame stores variable values. see example, let’s convert raster variables var data.frame: Note column stores values variable. Let’s predict Maxnet models data.frame: Now, instead SpatRaster objects, function returns data.frame objects predictions:","code":"var_df <- as.data.frame(var) head(var_df) #>       bio_1    bio_7 bio_12   bio_15 SoilType #> 11 22.77717 18.12400   1180 48.03594       NA #> 12 22.76711 17.74400   1191 49.31194       10 #> 13 22.68580 17.46575   1206 51.51922       10 #> 14 22.50121 17.84525   1228 53.90265       10 #> 15 22.07609 18.14125   1254 54.10397       10 #> 16 21.88485 18.80800   1276 54.07279       10 p_df <- predict_selected(models = fitted_model_maxnet,                           raster_variables = var_df, #Now, a data.frame                          progress_bar = FALSE) #Results by replicate of the model 192 head(p_df$Model_192$Replicates) #>         Rep_1        Rep_2        Rep_3        Rep_4 #> 1 0.006521501 0.0006209852 0.0005883615 9.831561e-05 #> 2 0.006446437 0.0005356316 0.0005713501 9.009486e-05 #> 3 0.006233583 0.0003396879 0.0004975279 6.967025e-05 #> 4 0.005797668 0.0001458500 0.0003605775 4.303576e-05 #> 5 0.008513515 0.0002034105 0.0006532983 8.529550e-05 #> 6 0.009240381 0.0001753492 0.0006784553 9.035171e-05  #Consensus across replicates of the model 192 head(p_df$Model_192$Model_consensus) #>         median       range        mean       stdev #> 1 0.0006046734 0.006423186 0.001957291 0.003052184 #> 2 0.0005534909 0.006356342 0.001910878 0.003031621 #> 3 0.0004186079 0.006163912 0.001785117 0.002970901 #> 4 0.0002532137 0.005754632 0.001586783 0.002810372 #> 5 0.0004283544 0.008428219 0.002363880 0.004107054 #> 6 0.0004269022 0.009150029 0.002546134 0.004470371  #General consensus across all models head(p_df$General_consensus) #>         median        mean       stdev       range #> 1 0.0006049792 0.001882943 0.002691096 0.006423186 #> 2 0.0005534909 0.001847258 0.002690840 0.006356342 #> 3 0.0004186079 0.001737935 0.002664381 0.006163912 #> 4 0.0002532730 0.001562733 0.002558575 0.005757458 #> 5 0.0004283544 0.002337815 0.003756587 0.008435158 #> 6 0.0004274250 0.002539356 0.004130584 0.009165160"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_predictions.html","id":"binarize-models","dir":"Articles","previous_headings":"Predict Selected Models for a Single Scenario","what":"Binarize Models","title":"Predict models to single scenario","text":"fitted_models object stores thresholds can used binarize models suitable unsuitable areas. thresholds correspond omission error rate used model selection (e.g., 5% 10%). can access omission error rate used calculate thresholds directly object: models, 10% omission error rate used calculate thresholds. means predictions binarized, approximately 10% presence records used model calibration fall areas classified unsuitable. thresholds summarized two ways: mean median across replicates model, consensus mean median across selected models (one model selected). Let’s check thresholds general consensus: Let’s use thresholds binarize models (functionality available predicting SpatRaster):","code":"#Get omission error used to select models and calculate the thesholds ## For maxnet model fitted_model_maxnet$omission_rate #> [1] 10  ## For glm model fitted_model_glm$omission_rate #> [1] 10 #For maxnet fitted_model_maxnet$thresholds$consensus #> $mean #> [1] 0.3095083 #>  #> $median #> [1] 0.259534  #For glm fitted_model_glm$thresholds$consensus #> $mean #>        82  #> 0.1387445  #>  #> $median #>        82  #> 0.1118301 #Get the thersholds for models (general consensus) thr_mean_maxnet <- fitted_model_maxnet$thresholds$consensus$mean #Maxnet thr_mean_glm <- fitted_model_glm$thresholds$consensus$mean #glm  #Binarize models mean_maxnet_bin <- (p_maxnet$General_consensus$mean > thr_mean_maxnet) * 1 mean_glm_bin <- (p_glm$General_consensus$mean > thr_mean_glm) * 1  #Compare results par(mfrow= c(1, 2)) #Set grid to plot plot(mean_maxnet_bin, main = \"Maxnet\") plot(mean_glm_bin, main = \"GLM\") on.exit() #Reinitiate grid"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_predictions.html","id":"clamping-variables","dir":"Articles","previous_headings":"Predict Selected Models for a Single Scenario","what":"Clamping Variables","title":"Predict models to single scenario","text":"default, predictions performed free extrapolation (extrapolation_type = \"E\"). can problematic peak suitability occurs extremes predictor’s range. example, let’s examine response curve Maxnet model bio_7 (Temperature Annual Range):  Note higher suitability occurs low values temperature range. However, lower limit calibration data used fit models (dashed line) 15.7ºC. premise suitability increase stabilize lower values bio_7 extrapolation model (area left dashed line). ’s possible suitability begin decrease extremely low values, rendering extrapolation inaccurate, calibration data insufficient model predict . One way address clamping variables. means values outside calibration range (lower value upper value) set respective lower upper limits calibration range. example, calibration data Maxnet models, lower upper limits bio_7 15.7ºC 23.3ºC, respectively: observe effect clamping variable, let’s create hypothetical (extreme) scenario bio_7 extremely low values:  Let’s predict Maxnet models new scenario free extrapolation (extrapolation_type = \"E\") clamped variables (extrapolation_type = \"EC\"):  Note clamp variables, regions extremely low values (hypothetical) bio_7 exhibit lower predicted suitabilities compared free extrapolation allowed. default, extrapolation_type = \"EC\" set, predictor variables clamped. can specify variables clamp using var_to_clamp argument.","code":"response_curve(models = fitted_model_maxnet, variable = \"bio_7\",                 extrapolation_factor = 1) range(fitted_model_maxnet$calibration_data$bio_7) #> [1] 15.71120 23.30475 #From bio_7, reduce values new_bio7 <- var$bio_7 - 7 #Create new scenario new_var <- var #Replace bio_7 with new_bio7 in this scenario new_var$bio_7 <- new_bio7  #Plot the differences par(mfrow = c(1,2)) plot(var$bio_7, main = \"Original bio_7\", range = c(5, 25)) plot(new_var$bio_7, main = \"New bio_7\", range = c(5, 25)) on.exit() #Reinitiate grid #Predict to hypothetical scenario with free extrapolation p_free_extrapolation <- predict_selected(models = fitted_model_maxnet,                                           raster_variables = new_var, #New scenario                                          consensus = \"mean\",                                          extrapolation_type = \"E\", #Free extrapolation (Default)                                          progress_bar = FALSE)  #Predict to hypothetical scenario with clamping p_clamping <- predict_selected(models = fitted_model_maxnet,                                 raster_variables = new_var, #New scenario                                consensus = \"mean\",                                extrapolation_type = \"EC\", #Extrapolation with clamping                                progress_bar = FALSE)  #Get and see differences p_difference <- p_free_extrapolation$General_consensus$mean - p_clamping$General_consensus$mean  #Plot the differences par(mfrow = c(2,2)) plot(p_free_extrapolation$General_consensus$mean, main = \"Free extrapolation\",      zlim = c(0, 1)) plot(p_clamping$General_consensus$mean, main = \"Clamping\",      zlim = c(0, 1)) plot(p_difference, main = \"Difference\") plot(new_bio7, main = \"Hypothetical bio_7\", type = \"interval\") on.exit() #Reinitiate grid"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_predictions.html","id":"no-extrapolation","dir":"Articles","previous_headings":"Predict Selected Models for a Single Scenario","what":"No Extrapolation","title":"Predict models to single scenario","text":"rigorous approach predict extrapolation, regions outside limits calibration data assigned suitability value 0. Let’s predict Maxnet models using hypothetical scenario created previous step observe difference:  example, almost entire predicted area shows zero suitability, except small patch. occurred , hypothetical scenario, nearly entire region bio_7 values lower calibration data (minimum 15ºC). region suitability predicted greater 0 bio_7 values fall within limits calibration data. default, extrapolation_type = \"NE\" set, predictor variables considered process. can specify subset variables considered extrapolation using var_to_clamp argument.","code":"#Predict to hypothetical scenario with no extrapolation p_no_extrapolation <- predict_selected(models = fitted_model_maxnet,                                         raster_variables = new_var, #New scenario                                        consensus = \"mean\",                                        extrapolation_type = \"NE\", #No extrapolation                                        progress_bar = FALSE) #Plot the differences par(mfrow = c(2,2)) plot(p_free_extrapolation$General_consensus$mean, main = \"Free extrapolation\",      zlim = c(0, 1)) plot(p_clamping$General_consensus$mean, main = \"Clamping\",      zlim = c(0, 1)) plot(p_no_extrapolation$General_consensus$mean, main = \"No extrapolation\",      zlim = c(0, 1)) plot(new_bio7, main = \"Hypothetical bio_7\", type = \"interval\") on.exit() #Reinitiate grid"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_predictions.html","id":"output-type","dir":"Articles","previous_headings":"Predict Selected Models for a Single Scenario","what":"Output Type","title":"Predict models to single scenario","text":"Maximum entropy models (maxnet) produce four different types output predictions: raw, cumulative, logistic, cloglog. described Merow et al. 2013 Phillips et al. 2017. four output types monotonically related. Therefore, rank-based metrics model fit (e.g., omission rate partial ROC) identical. However, output types different scaling, leads distinct interpretations visually different prediction maps. Raw (exponential) output interpreted Relative Occurrence Rate (ROR). ROR sums 1 predicted calibration data. Cumulative output assigns location sum raw values less equal raw value location, rescales range 0 100. Cumulative output can interpreted terms omission rate thresholding value c predict suitable/unsuitable cell omit approximately c% presences. Cloglog output (Default) transforms raw values scale relative suitability ranging 0 1, using logistic transformation based user-specified parameter ‘τ\\tau’, represents probability presence ‘average’ presence locations. context, tau value defaults τ≈0.632\\tau \\approx 0.632. Logistic output similar Cloglog, assumes τ=0.5\\tau = 0.5. Let’s examine differences four output types Maxnet models:","code":"p_cloglog <- predict_selected(models = fitted_model_maxnet, raster_variables = var,                                type = \"cloglog\", progress_bar = FALSE) p_logistic <- predict_selected(models = fitted_model_maxnet, raster_variables = var,                                type = \"logistic\", progress_bar = FALSE) p_cumulative <- predict_selected(models = fitted_model_maxnet, raster_variables = var,                                type = \"cumulative\", progress_bar = FALSE) p_raw <- predict_selected(models = fitted_model_maxnet, raster_variables = var,                                type = \"raw\", progress_bar = FALSE)  #Plot the differences par(mfrow = c(2,2)) plot(p_cloglog$General_consensus$mean, main = \"Cloglog (Default)\",      zlim = c(0, 1)) plot(p_logistic$General_consensus$mean, main = \"Logistic\",      zlim = c(0, 1)) plot(p_cumulative$General_consensus$mean, main = \"Cumulative\",      zlim = c(0, 1)) plot(p_raw$General_consensus$mean, main = \"Raw\",      zlim = c(0, 1)) on.exit() #Reinitiate grid"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_predictions.html","id":"saving-predictions","dir":"Articles","previous_headings":"","what":"Saving Predictions","title":"Predict models to single scenario","text":"can save predictions disk setting write_files = TRUE. option enabled, must provide directory path out_dir argument. raster_variables SpatRaster, function save output files GeoTIFF (.tif) files. raster_variables data.frame, function save output files Comma Separated Value (.csv) files. Alternatively, can use writeRaster() save specific output predictions manually. example, save mean layer general consensus results:","code":"p_save <- predict_selected(models = fitted_model_maxnet,                             raster_variables = var,                             write_files = TRUE, #To save to the disk                            write_replicates = TRUE, #To save predictions for each replicate                            out_dir = tempdir(), #A path to save the resuls (here, the temporary directory)                            progress_bar = FALSE) writeRaster(p_maxnet$General_consensus$mean,              filename = file.path(tempdir(), \"Mean_consensus.tif\"))"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_projections.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Project models to multiple scenarios","text":"selected models fit using fit_selected(), projections single multiple scenarios can performed. project_selected() function designed projections multiple scenarios. project using selected models, fitted_models object required. detailed information model fitting, please consult vignette Fit Explore Selected Models. fitted_models object generated vignette included example dataset within package. Let’s load .","code":"#Load packages library(kuenm2) library(terra) #> terra 1.8.60  #Import calib_results_maxnet data(\"fitted_model_maxnet\", package = \"kuenm2\") #Print calibration result fitted_model_maxnet #> fitted_models object summary #> ============================ #> Species: Myrcia hatschbachii  #> Algortihm: maxnet  #> Number of fitted models: 2  #> Models fitted with 4 replicates"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_projections.html","id":"pre-processing-raster-predictors","dir":"Articles","previous_headings":"","what":"Pre-processing raster predictors","title":"Project models to multiple scenarios","text":"predicting models single scenario requires single SpatRaster object containing predictor variables (detailed Predict models single scenario), projecting models multiple scenarios necessitates folder stores predictor variables scenario. folders must organized specific hierarchical manner: root directory contain nested folders representing different scenarios, raster variables stored within. first level inside root folder, subfolders correspond distinct time periods (e.g., future years like “2070” “2100,” past periods “Mid-holocene” “LGM”). Within period folder, applicable, include subfolders emission scenario (e.g., “ssp126”, “ssp585”). Finally, within emission scenario time period folder, include separate folder General Circulation Model (GCM) (e.g., “BCC-CSM2-MR”, “MIROC6”). structured organization enables function automatically access process data according period, emission scenario, GCM.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_projections.html","id":"organize-and-structure-future-climate-variables-from-worldclim","dir":"Articles","previous_headings":"Pre-processing raster predictors","what":"Organize and structure future climate variables from WorldClim","title":"Project models to multiple scenarios","text":"package provides function import future climate variables downloaded WorldClim (version 2.1). function renames files organizes folders categorized period/year, emission scenario (Shared Socioeconomic Pathways; SSPs), General Circulation Model (GCM). simplifies preparation climate data, ensuring required variables properly structured modeling projections. use function, download future raster variables WorldClim 2.1 save within folder. rename files variables, function relies patterns provided original files work properly. package also provides example raw variables downloaded WorldClim 2.1. example includes bioclimatic predictions periods “2041-2060” “2081-2100”, two SSPs (125 585) two GCMs (ACCESS-CM2 MIROC6), 10 arc-minutes resolution. Note variables folder retain original names provided WorldClim. can download variables directly WorldClim using geodata R package: Let’s check variables inside “geodata_dir” folder: Now, can organize structure files using organize_future_worldclim function.","code":"# See raster files with future predictors provided as example # The data is located in the \"inst/extdata\" folder. in_dir <- system.file(\"extdata\", package = \"kuenm2\") list.files(in_dir) #>  [1] \"bias_file.tif\"                                  #>  [2] \"CHELSA_LGM_CCSM4.tif\"                           #>  [3] \"CHELSA_LGM_CNRM-CM5.tif\"                        #>  [4] \"CHELSA_LGM_FGOALS-g2.tif\"                       #>  [5] \"CHELSA_LGM_IPSL-CM5A-LR.tif\"                    #>  [6] \"CHELSA_LGM_MIROC-ESM.tif\"                       #>  [7] \"CHELSA_LGM_MPI-ESM-P.tif\"                       #>  [8] \"CHELSA_LGM_MRI-CGCM3.tif\"                       #>  [9] \"Current_CHELSA.tif\"                             #> [10] \"Current_variables.tif\"                          #> [11] \"m.gpkg\"                                         #> [12] \"wc2.1_10m_bioc_ACCESS-CM2_ssp126_2041-2060.tif\" #> [13] \"wc2.1_10m_bioc_ACCESS-CM2_ssp126_2081-2100.tif\" #> [14] \"wc2.1_10m_bioc_ACCESS-CM2_ssp585_2041-2060.tif\" #> [15] \"wc2.1_10m_bioc_ACCESS-CM2_ssp585_2081-2100.tif\" #> [16] \"wc2.1_10m_bioc_MIROC6_ssp126_2041-2060.tif\"     #> [17] \"wc2.1_10m_bioc_MIROC6_ssp126_2081-2100.tif\"     #> [18] \"wc2.1_10m_bioc_MIROC6_ssp585_2041-2060.tif\"     #> [19] \"wc2.1_10m_bioc_MIROC6_ssp585_2081-2100.tif\" #Install geodata if necessary if(!require(\"geodata\")){   install.packages(\"geodata\") } #Load geodata library(geodata) #Create folder to save the raster files #Here, in a temporary directory geodata_dir <- file.path(tempdir(), \"Future_worldclim\") dir.create(geodata_dir) #Define GCMs, SSPs and time periods gcms <- c(\"ACCESS-CM2\", \"MIROC6\") ssps <- c(\"126\", \"585\") periods <- c(\"2041-2060\", \"2061-2080\") #Create a grid of combination of periods, ssps and gcms g <- expand.grid(\"period\" = periods, \"ssps\" = ssps, \"gcms\" = gcms) g #Each line is a specific scenario for future #Loop to download variables for each scenario lapply(1:nrow(g), function(i){   cmip6_world(model = g$gcms[i],                ssp = g$ssps[i],                time = g$period[i],                var = \"bioc\",                res = 10, path = geodata_dir) }) #It will take a while... list.files(geodata_dir, recursive = TRUE) #> [1] \"climate/wc2.1_10m/wc2.1_10m_bioc_ACCESS-CM2_ssp126_2041-2060.tif\" #> [2] \"climate/wc2.1_10m/wc2.1_10m_bioc_ACCESS-CM2_ssp126_2061-2080.tif\" #> [3] \"climate/wc2.1_10m/wc2.1_10m_bioc_ACCESS-CM2_ssp585_2041-2060.tif\" #> [4] \"climate/wc2.1_10m/wc2.1_10m_bioc_ACCESS-CM2_ssp585_2061-2080.tif\" #> [5] \"climate/wc2.1_10m/wc2.1_10m_bioc_MIROC6_ssp126_2041-2060.tif\"     #> [6] \"climate/wc2.1_10m/wc2.1_10m_bioc_MIROC6_ssp126_2061-2080.tif\"     #> [7] \"climate/wc2.1_10m/wc2.1_10m_bioc_MIROC6_ssp585_2041-2060.tif\"     #> [8] \"climate/wc2.1_10m/wc2.1_10m_bioc_MIROC6_ssp585_2061-2080.tif\"  #>  #> #Set climate as input directory #> in_dir <- file.path(geodata_dir, \"climate\")"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_projections.html","id":"format-for-renaming","dir":"Articles","previous_headings":"Pre-processing raster predictors > Organize and structure future climate variables from WorldClim","what":"Format for renaming","title":"Project models to multiple scenarios","text":"important argument name_format, defines format renaming variables. names variables SpatRaster must precisely match used model calibration running PCA (do_pca = TRUE set prepare_data() function; see Prepare Data Model Calibration details). Therefore, variables used calibrate models named “bio_1”, “bio_2”, etc., variables future raster layers must also named “bio_1”, “bio_2”, etc. However, variables different pattern, starting uppercase letters using zeros single-digit numbers (e.g., “Bio_01”, “Bio_02”, etc.), must named “Bio_01”, “Bio_02”, etc. function provides four options: \"bio_\": Variables renamed bio_1, bio_2, bio_3, bio_10, etc. \"bio_0\": Variables renamed bio_01, bio_02, bio_03, bio_10, etc. \"Bio_\": Variables renamed Bio_1, Bio_2, Bio_3, Bio_10, etc. \"Bio_0\": Variables renamed Bio_01, Bio_02, Bio_03, Bio_10, etc. Let’s check variables named fitted_model: variables follows standards first option (\"bio_\").","code":"fitted_model_maxnet$continuous_variables #> [1] \"bio_1\"  \"bio_7\"  \"bio_12\" \"bio_15\""},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_projections.html","id":"fixed-variables","dir":"Articles","previous_headings":"Pre-processing raster predictors > Organize and structure future climate variables from WorldClim","what":"Fixed variables","title":"Project models to multiple scenarios","text":"predicting times, can assume variables static (.e., remain unchanged projected scenarios). fixed_variables argument allows append static variables alongside bioclimatic variables. , let’s assume soilType remain static future scenarios:","code":"# Import raster layers (same used to calibrate and fit final models) var <- rast(system.file(\"extdata\", \"Current_variables.tif\", package = \"kuenm2\")) #Get soilType soiltype <- var$SoilType"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_projections.html","id":"organize-and-structure-worldclim-files","dir":"Articles","previous_headings":"Pre-processing raster predictors > Organize and structure future climate variables from WorldClim","what":"Organize and structure WorldClim files","title":"Project models to multiple scenarios","text":"Now, let’s organize WorldClim files organize_future_worldclim() function: can check files structured hierarchically nested folders using dir_tree() function fs package: organizing variables, next step create prepared_projection object.","code":"#Create folder to save structured files out_dir_future <- file.path(tempdir(), \"Future_raw\") #Here, in a temporary directory #Organize organize_future_worldclim(input_dir = in_dir, #Path to the raw variables from WorldClim                           output_dir = out_dir_future,                            name_format = \"bio_\", #Name format                           fixed_variables = var$SoilType) #Static variables #>   |                                                                              |                                                                      |   0%  |                                                                              |=========                                                             |  12%  |                                                                              |==================                                                    |  25%  |                                                                              |==========================                                            |  38%  |                                                                              |===================================                                   |  50%  |                                                                              |============================================                          |  62%  |                                                                              |====================================================                  |  75%  |                                                                              |=============================================================         |  88%  |                                                                              |======================================================================| 100% #>  #> Variables successfully organized in directory: #> /tmp/RtmpK65HxK/Future_raw  # Check files organized dir(out_dir_future, recursive = TRUE) #> [1] \"2041-2060/ssp126/ACCESS-CM2/Variables.tif\" #> [2] \"2041-2060/ssp126/MIROC6/Variables.tif\"     #> [3] \"2041-2060/ssp585/ACCESS-CM2/Variables.tif\" #> [4] \"2041-2060/ssp585/MIROC6/Variables.tif\"     #> [5] \"2081-2100/ssp126/ACCESS-CM2/Variables.tif\" #> [6] \"2081-2100/ssp126/MIROC6/Variables.tif\"     #> [7] \"2081-2100/ssp585/ACCESS-CM2/Variables.tif\" #> [8] \"2081-2100/ssp585/MIROC6/Variables.tif\" #Install package if necessary if(!require(\"fs\")){   install.packages(\"fs\") } dir_tree(out_dir_future) #> Temp\\RtmpkhmGWN/Future_raw #> ├── 2041-2060 #> │   ├── ssp126 #> │   │   ├── ACCESS-CM2 #> │   │   │   └── Variables.tif #> │   │   └── MIROC6 #> │   │       └── Variables.tif #> │   └── ssp585 #> │       ├── ACCESS-CM2 #> │       │   └── Variables.tif #> │       └── MIROC6 #> │           └── Variables.tif #> └── 2081-2100 #>     ├── ssp126 #>     │   ├── ACCESS-CM2 #>     │   │   └── Variables.tif #>     │   └── MIROC6 #>     │       └── Variables.tif #>     └── ssp585 #>         ├── ACCESS-CM2 #>         │   └── Variables.tif #>         └── MIROC6 #>             └── Variables.tif"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_projections.html","id":"preparation-of-data-for-model-projections","dir":"Articles","previous_headings":"","what":"Preparation of data for model projections","title":"Project models to multiple scenarios","text":"Now, let’s prepare data model projections across multiple scenarios, storing paths rasters representing scenario. contrast predict_selected(), requires SpatRaster object, need paths folders raster files stored. includes variables present time, used calibrate fit models. Currently, future climate files. present-day predictor variables must reside root directory processed future variables. Let’s copy rasters used model calibration fitting folder: Now, can prepare data projections. addition storing paths variables scenario, function also verifies variables used fit final models available across scenarios. perform check, need provide either fitted_models object intend use projection simply variable names. strongly suggest using fitted_models object minimize projection errors. also need define root directory containing scenarios projection (present, past, /future), along additional information regarding time periods, SSPs, GCMs. print projection_data object, summarizes scenarios predict also shows root directory predictor rasters stored: check structure prepared_projection object, can see ’s list containing: Paths variables representing distinct scenarios subfolders. pattern used identify format raster files within folders (default, *.tif). names predictors. list class prcomp Principal Component Analysis (PCA) performed set variables prepare_data().","code":"# Create a \"Current_raw\" folder in a temporary directory # and copy the rawvariables there. out_dir_current <- file.path(tempdir(), \"Current_raw\") dir.create(out_dir_current, recursive = TRUE)  # Save current variables in temporary directory terra::writeRaster(var, file.path(out_dir_current, \"Variables.tif\"))  #Check folder list.files(out_dir_current) #> [1] \"Variables.tif\" # Prepare projections using fitted models to check variables pr <- prepare_projection(models = fitted_model_maxnet,                          present_dir = out_dir_current, #Directory with present-day variables                          past_dir = NULL, #NULL because we won't project to the past                          past_period = NULL, #NULL because we won't project to the past                          past_gcm = NULL, #NULL because we won't project to the past                          future_dir = out_dir_future, #Directory with future variables                          future_period = c(\"2041-2060\", \"2081-2100\"),                          future_pscen = c(\"ssp126\", \"ssp585\"),                          future_gcm = c(\"ACCESS-CM2\", \"MIROC6\")) pr #> projection_data object summary #> ============================= #> Variables prepared to project models for Present and Future  #> Future projections contain the following periods, scenarios and GCMs: #>   - Periods: 2041-2060 | 2081-2100  #>   - Scenarios: ssp126 | ssp585  #>   - GCMs: ACCESS-CM2 | MIROC6  #> All variables are located in the following root directory: #> /tmp/RtmpK65HxK #Open prepared_projection in a new window View(pr)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_projections.html","id":"project-selected-models-to-multiple-scenarios","dir":"Articles","previous_headings":"","what":"Project selected models to multiple scenarios","title":"Project models to multiple scenarios","text":"preparing data, can use project_selected() function predict selected models across multiple scenarios specified prepare_projections: function returns model_projections object. object similar prepared_data object, storing information predicted scenarios folder resulting projection rasters saved. Note results saved hierarchically nested subfolders, representing distinct scenario. root directory, function also saves file named “Projection_paths.RDS”, model_projections object. object can imported R using readRDS(file.path(out_dir, \"Projection_paths.RDS\")). default, scenario, function computes consensus metrics (mean, median, range, standard deviation) model across replicates (one model selected), well general consensus across models. Note selected model can also replicates. default, function output individual replicates unless write_replicates = TRUE set. important write replicates intend compute variability across using projection_variability(). details, check vignette Explore Variability Uncertainty Projections. function accepts several parameters control predictions predict_selected(), consensus compute, extrapolation type (free extrapolation (E), extrapolation clamping (EC), extrapolation (NE)), variables clamp, format prediction values (raw, cumulative, logistic, default cloglog). details, consult vignette Predict models single scenario.","code":"## Create a folder to save projection results #Here, in a temporary directory out_dir <- file.path(tempdir(), \"Projection_results/maxnet\") dir.create(out_dir, recursive = TRUE)  ## Project selected models to multiple scenarios p <- project_selected(models = fitted_model_maxnet,                        projection_data = pr,                       out_dir = out_dir,                        write_replicates = TRUE,                       progress_bar = FALSE) #Do not print progress bar print(p) #> model_projections object summary #> ================================ #> Models projected for Present and Future  #> Future projections contain the following periods, scenarios and GCMs: #>   - Periods: 2041-2060 | 2081-2100  #>   - Scenarios:   #>   - GCMs: ACCESS-CM2 | MIROC6  #> All raster files containing the projection results are located in the following root directory: #>  /tmp/RtmpK65HxK/Projection_results/maxnet dir_tree(out_dir) #> Temp\\Projection_results/maxnet #> ├── Future #> │   ├── 2041-2060 #> │   │   ├── ssp126 #> │   │   │   ├── ACCESS-CM2 #> │   │   │   │   ├── General_consensus.tif #> │   │   │   │   ├── Model_192_consensus.tif #> │   │   │   │   ├── Model_192_replicates.tif #> │   │   │   │   ├── Model_219_consensus.tif #> │   │   │   │   └── Model_219_replicates.tif #> │   │   │   └── MIROC6 #> │   │   │       ├── General_consensus.tif #> │   │   │       ├── Model_192_consensus.tif #> │   │   │       ├── Model_192_replicates.tif #> │   │   │       ├── Model_219_consensus.tif #> │   │   │       └── Model_219_replicates.tif #> │   │   └── ssp585 #> │   │       ├── ACCESS-CM2 #> │   │       │   ├── General_consensus.tif #> │   │       │   ├── Model_192_consensus.tif #> │   │       │   ├── Model_192_replicates.tif #> │   │       │   ├── Model_219_consensus.tif #> │   │       │   └── Model_219_replicates.tif #> │   │       └── MIROC6 #> │   │           ├── General_consensus.tif #> │   │           ├── Model_192_consensus.tif #> │   │           ├── Model_192_replicates.tif #> │   │           ├── Model_219_consensus.tif #> │   │           └── Model_219_replicates.tif #> │   └── 2081-2100 #> │       ├── ssp126 #> │       │   ├── ACCESS-CM2 #> │       │   │   ├── General_consensus.tif #> │       │   │   ├── Model_192_consensus.tif #> │       │   │   ├── Model_192_replicates.tif #> │       │   │   ├── Model_219_consensus.tif #> │       │   │   └── Model_219_replicates.tif #> │       │   └── MIROC6 #> │       │       ├── General_consensus.tif #> │       │       ├── Model_192_consensus.tif #> │       │       ├── Model_192_replicates.tif #> │       │       ├── Model_219_consensus.tif #> │       │       └── Model_219_replicates.tif #> │       └── ssp585 #> │           ├── ACCESS-CM2 #> │           │   ├── General_consensus.tif #> │           │   ├── Model_192_consensus.tif #> │           │   ├── Model_192_replicates.tif #> │           │   ├── Model_219_consensus.tif #> │           │   └── Model_219_replicates.tif #> │           └── MIROC6 #> │               ├── General_consensus.tif #> │               ├── Model_192_consensus.tif #> │               ├── Model_192_replicates.tif #> │               ├── Model_219_consensus.tif #> │               └── Model_219_replicates.tif #> ├── Present #> │   └── Present #> │       ├── General_consensus.tif #> │       ├── Model_192_consensus.tif #> │       ├── Model_192_replicates.tif #> │       ├── Model_219_consensus.tif #> │       └── Model_219_replicates.tif #> └── Projection_paths.RDS"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_projections.html","id":"import-rasters-resulting-from-projections","dir":"Articles","previous_headings":"","what":"Import rasters resulting from projections","title":"Project models to multiple scenarios","text":"model_projections object stores paths resultant rasters. import results, can use import_projections() function. default, imports consensus metrics (“median”, “range”, “mean”, “stdev”) scenarios (time periods, SSPs, GCMs) available model_projections object. Let’s import mean scenarios:  Alternatively, can import results specific scenarios. example, let’s import results “2041-2060” time period SSP 126:  model_projections object, can compute changes suitable areas scenarios (see projection_changes function), explore variance stemming replicates, model parameterizations, GCMs (see projection_variability), perform analysis extrapolation risks (see projection_mop). details, check vignette Explore Variability Uncertainty Projections.","code":"#Import mean of each projected scenario p_mean <- import_projections(projection = p, consensus = \"mean\") #Plot all scenarios plot(p_mean, cex.main = 0.8) p_2060_ssp126 <- import_projections(projection = p, consensus = \"mean\",                                      present = FALSE, #Do not import present projections                                     future_period = \"2041-2060\",                                     future_pscen = \"ssp126\") #Plot all scenarios plot(p_2060_ssp126, cex.main = 0.8)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/organize_past_chelsa.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Organize projections manually: An example with LGM from CHELSA","text":"“Project models multiple scenarios” vignette, show predict models multiple scenarios . used example future scenarios available WorldClim, specific function (organize_future_worldclim) organize files specific hierarchical manner compatible kuenm2. Specifically, need root directory containing nested folders representing different scenarios, raster variables stored within. first level inside root folder, subfolders correspond distinct time periods (e.g., future years like “2070” “2100,” past periods “Mid-holocene” “LGM”). Within period folder, applicable, include subfolders emission scenario (e.g., “ssp126”, “ssp585”). Finally, within emission scenario time period folder, need include separate folder General Circulation Model (GCM) (e.g., “BCC-CSM2-MR”, “MIROC6”) , show organize files manually, making possible project models using scenarios available different sources. example, use variables CHELSA representing scenarios Last Glacial Maximum (LGM, 21,000 year ago).","code":"#Load packages library(kuenm2) library(terra) #> terra 1.8.60"},{"path":"https://marlonecobos.github.io/kuenm2/articles/organize_past_chelsa.html","id":"standardize-raster-file-names-for-model-projections","dir":"Articles","previous_headings":"","what":"Standardize Raster File Names for Model Projections","title":"Organize projections manually: An example with LGM from CHELSA","text":"using organize_for_projection() function, climate variables must first saved separate .tif files — one file per scenario. Filenames follow consistent pattern clearly indicates time period, GCM, , future scenarios, emission scenario (SSP). example: file representing past conditions “LGM” period using “MIROC6” GCM named: “Past_LGM_MIROC6.tif” file representing future conditions period “2081–2100” emission scenario “ssp585” GCM “ACCESS-CM2” named: “Future_2081-2100_ssp585_ACCESS-CM2.tif” scenario files must contain variable names (e.g., bio1, bio2, etc.) units used model calibration. Let’s see achieve practice.","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/articles/organize_past_chelsa.html","id":"download-variables-from-lgm","dir":"Articles","previous_headings":"Getting variables from CHELSA","what":"Download variables from LGM","title":"Organize projections manually: An example with LGM from CHELSA","text":"first step downloading variables representing LGM condition CHELSA. can download files directly link, follow script . need specify folder save variables, General Circulation Models (GCMs) variables:","code":"# Define variables to download var_to_use <- c(\"BIO_01\", \"BIO_07\", \"BIO_12\", \"BIO_15\") # Define GCMs gcms <- c(\"CCSM4\", \"CNRM-CM5\", \"FGOALS-g2\", \"IPSL-CM5A-LR\", \"MIROC-ESM\",           \"MPI-ESM-P\", \"MRI-CGCM3\") # Create a grid combining variables and GCMs g <- expand.grid(\"gcm\" = gcms, \"var\" = var_to_use)  # Create links to download l <- sapply(1:nrow(g), function(i){   gcm_i <- g$gcm[i]   var_i <- g$var[i]   #Create link to download   l_i <- paste0(\"https://os.zhdk.cloud.switch.ch/chelsav1/pmip3/bioclim/CHELSA_PMIP_\",                 gcm_i, \"_\", var_i, \".tif\") }) # See links head(l) #> [1] \"https://os.zhdk.cloud.switch.ch/chelsav1/pmip3/bioclim/CHELSA_PMIP_CCSM4_BIO_01.tif\"        #> [2] \"https://os.zhdk.cloud.switch.ch/chelsav1/pmip3/bioclim/CHELSA_PMIP_CNRM-CM5_BIO_01.tif\"     #> [3] \"https://os.zhdk.cloud.switch.ch/chelsav1/pmip3/bioclim/CHELSA_PMIP_FGOALS-g2_BIO_01.tif\"    #> [4] \"https://os.zhdk.cloud.switch.ch/chelsav1/pmip3/bioclim/CHELSA_PMIP_IPSL-CM5A-LR_BIO_01.tif\" #> [5] \"https://os.zhdk.cloud.switch.ch/chelsav1/pmip3/bioclim/CHELSA_PMIP_MIROC-ESM_BIO_01.tif\"    #> [6] \"https://os.zhdk.cloud.switch.ch/chelsav1/pmip3/bioclim/CHELSA_PMIP_MPI-ESM-P_BIO_01.tif\"  # Create a directory to save the raw variables raw_past_chelsa <- file.path(tempdir(), \"Raw_past\") #Here, in a temporary directory dir.create(raw_past_chelsa)  # Download files and save in the Raw_past directory options(timeout = 300) #For avoiding errors with timeout sapply(l, function(i){   #Donwload only if the file has not been downloaded yet   if(!file.exists(file.path(raw_past_chelsa, basename(i))))   download.file(url = i,                 destfile = file.path(raw_past_chelsa, basename(i)),                 method = \"curl\") }) #It will take a while  #Check the files in the Raw_past list.files(raw_past_chelsa) #> [1] \"CHELSA_PMIP_CCSM4_BIO_01.tif\"        \"CHELSA_PMIP_CCSM4_BIO_07.tif\" #> [3] \"CHELSA_PMIP_CCSM4_BIO_12.tif\"        \"CHELSA_PMIP_CCSM4_BIO_15.tif\" #> [5] \"CHELSA_PMIP_CNRM-CM5_BIO_01.tif\"     \"CHELSA_PMIP_CNRM-CM5_BIO_07.tif\" #> [7] \"CHELSA_PMIP_CNRM-CM5_BIO_12.tif\"     \"CHELSA_PMIP_CNRM-CM5_BIO_15.tif\" #> [9] \"CHELSA_PMIP_FGOALS-g2_BIO_01.tif\"    \"CHELSA_PMIP_FGOALS-g2_BIO_07.tif\" #> [11] \"CHELSA_PMIP_FGOALS-g2_BIO_12.tif\"    \"CHELSA_PMIP_FGOALS-g2_BIO_15.tif\" #> [13] \"CHELSA_PMIP_IPSL-CM5A-LR_BIO_01.tif\" \"CHELSA_PMIP_IPSL-CM5A-LR_BIO_07.tif\" #> [15] \"CHELSA_PMIP_IPSL-CM5A-LR_BIO_12.tif\" \"CHELSA_PMIP_IPSL-CM5A-LR_BIO_15.tif\" #> [17] \"CHELSA_PMIP_MIROC-ESM_BIO_01.tif\"    \"CHELSA_PMIP_MIROC-ESM_BIO_07.tif\" #> [19] \"CHELSA_PMIP_MIROC-ESM_BIO_12.tif\"    \"CHELSA_PMIP_MIROC-ESM_BIO_15.tif\" #> [21] \"CHELSA_PMIP_MPI-ESM-P_BIO_01.tif\"    \"CHELSA_PMIP_MPI-ESM-P_BIO_07.tif\" #> [23] \"CHELSA_PMIP_MPI-ESM-P_BIO_12.tif\"    \"CHELSA_PMIP_MPI-ESM-P_BIO_15.tif\" #> [25] \"CHELSA_PMIP_MRI-CGCM3_BIO_01.tif\"    \"CHELSA_PMIP_MRI-CGCM3_BIO_07.tif\" #> [27] \"CHELSA_PMIP_MRI-CGCM3_BIO_12.tif\"    \"CHELSA_PMIP_MRI-CGCM3_BIO_15.tif\""},{"path":"https://marlonecobos.github.io/kuenm2/articles/organize_past_chelsa.html","id":"download-variables-from-current-period","dir":"Articles","previous_headings":"Getting variables from CHELSA","what":"Download variables from Current Period","title":"Organize projections manually: An example with LGM from CHELSA","text":"also need variables representing current conditions (1981-2010) CHELSA. can download files directly link, follow script :","code":"#Create directory to save tha variables present_dir <- file.path(tempdir(), \"Present_raw\") dir.create(present_dir)  #Define variables to download var_present <-  c(\"bio1\", \"bio7\", \"bio12\", \"bio15\")  #Create links, download and save in the Present_raw directory l_present <- sapply(var_present, function(i){   #Create link to download   l_present_i <- paste0(\"https://os.zhdk.cloud.switch.ch/chelsav2/GLOBAL/climatologies/1981-2010/bio/CHELSA_\", i, \"_1981-2010_V.2.1.tif\")   #Donwload only if the file has not been downloaded yet   if(!file.exists(file.path(present_dir, basename(l_present_i))))   download.file(url = l_present_i,                 destfile = file.path(present_dir, basename(l_present_i)),                 method = \"curl\") }) #It will take a while  #Check the files in the directory list.files(present_dir) #> [1] \"CHELSA_bio1_1981-2010_V.2.1.tif\"     \"CHELSA_bio12_1981-2010_V.2.1.tif\" #> [3] \"CHELSA_bio15_1981-2010_V.2.1.tif\"    \"CHELSA_bio7_1981-2010_V.2.1.tif\""},{"path":"https://marlonecobos.github.io/kuenm2/articles/organize_past_chelsa.html","id":"merge-and-rename-variables-from-the-same-scenario","dir":"Articles","previous_headings":"Getting variables from CHELSA","what":"Merge and Rename Variables from the Same Scenario","title":"Organize projections manually: An example with LGM from CHELSA","text":"downloading files, need merge variables single SpatRaster scenario. general, past scenarios differs terms GCM, future scenarios characterized different emission scenarios (.e., SSP1-26 SSP5-85) GCMs. First, let’s merge variables present scenarios. speed analysis example, also resample rasters 30 arc-seconds 10 arc-minutes, crop variables using calibration area (M), provided example package defined drawing minimum convex polygons around species’ occurrences added buffer 300km. also rename variables pattern “bio1”, “bio12”, “bio15”, “bio7”. Now, let’s thing variables representing LGM conditions, starting masking variables using m resampling 10arc-min: Note variables names contains gcms used download. trick using patterns grouping variables: One important thing note Technical Specifications CHELSA specifies variables LGM different units current ones. current time, bio_1 values ºC, LGM values K * 10. bio_7 values ºC current time ºC * 10 LGM. current precipitation variables milimeters (mm) percentage variation (bio_15), LGM mm * 10 % * 10. Yes, ’s mess. need convert variables units current variables: Now variables LGM grouped scenarios (GCMs), names units current variables, can write raster disk. Remember filenames must time period (LGM) GCM scenario (variables representing future scenarios, also must SSPs emission scenarios):","code":"#Load package library(terra)  # Import M m <- terra::vect(system.file(\"extdata\", \"m.gpkg\",                          package = \"kuenm2\"))  # Import present variables present_files <- list.files(present_dir, full.names = TRUE) #List files present_var <- rast(present_files)  #Mask variables using the calibration area (m) present_m <- crop(present_var, m, mask = TRUE)  # Check variables names names(present_m) #> [1] \"CHELSA_bio1_1981-2010_V.2.1\"  \"CHELSA_bio12_1981-2010_V.2.1\" #> [3] \"CHELSA_bio15_1981-2010_V.2.1\" \"CHELSA_bio7_1981-2010_V.2.1\"  # Rename variables names(present_m) <- c(\"bio1\", \"bio12\", \"bio15\", \"bio7\") names(present_m) #> [1] \"bio1\"  \"bio12\" \"bio15\" \"bio7\"  # Check current resolution (30arc-sec) res(present_m) #> [1] 0.008333333 0.008333333  #Decrease resolution to 10arc-min present_chelsa <- aggregate(present_m, fact = 20, fun = \"mean\")  #Save processed raster dir_current <- file.path(tempdir(), \"Current_CHELSA\") dir.create(dir_current)  writeRaster(present_chelsa, filename = file.path(dir_current, \"Current_CHELSA.tif\")) # Import LGM variables lgm_files <- list.files(raw_past_chelsa, full.names = TRUE) #List files lgm_var <- rast(lgm_files)  #Mask variables using the calibration area (m) lgm_m <- crop(lgm_var, m, mask = TRUE)  #Decrease resolution to 10arc-min lgm_chelsa <- aggregate(lgm_m, fact = 20, fun = \"mean\")  #Check variables names names(lgm_chelsa) #>  [1] \"CHELSA_PMIP_CCSM4_BIO_01\"        \"CHELSA_PMIP_CCSM4_BIO_07\"        #>  [3] \"CHELSA_PMIP_CCSM4_BIO_12\"        \"CHELSA_PMIP_CCSM4_BIO_15\"        #>  [5] \"CHELSA_PMIP_CNRM-CM5_BIO_01\"     \"CHELSA_PMIP_CNRM-CM5_BIO_07\"     #>  [7] \"CHELSA_PMIP_CNRM-CM5_BIO_12\"     \"CHELSA_PMIP_CNRM-CM5_BIO_15\"     #>  [9] \"CHELSA_PMIP_FGOALS-g2_BIO_01\"    \"CHELSA_PMIP_FGOALS-g2_BIO_07\"    #> [11] \"CHELSA_PMIP_FGOALS-g2_BIO_12\"    \"CHELSA_PMIP_FGOALS-g2_BIO_15\"    #> [13] \"CHELSA_PMIP_IPSL-CM5A-LR_BIO_01\" \"CHELSA_PMIP_IPSL-CM5A-LR_BIO_07\" #> [15] \"CHELSA_PMIP_IPSL-CM5A-LR_BIO_12\" \"CHELSA_PMIP_IPSL-CM5A-LR_BIO_15\" #> [17] \"CHELSA_PMIP_MIROC-ESM_BIO_01\"    \"CHELSA_PMIP_MIROC-ESM_BIO_07\"    #> [19] \"CHELSA_PMIP_MIROC-ESM_BIO_12\"    \"CHELSA_PMIP_MIROC-ESM_BIO_15\"    #> [21] \"CHELSA_PMIP_MPI-ESM-P_BIO_01\"    \"CHELSA_PMIP_MPI-ESM-P_BIO_07\"    #> [23] \"CHELSA_PMIP_MPI-ESM-P_BIO_12\"    \"CHELSA_PMIP_MPI-ESM-P_BIO_15\"    #> [25] \"CHELSA_PMIP_MRI-CGCM3_BIO_01\"    \"CHELSA_PMIP_MRI-CGCM3_BIO_07\"    #> [27] \"CHELSA_PMIP_MRI-CGCM3_BIO_12\"    \"CHELSA_PMIP_MRI-CGCM3_BIO_15\" # In each iteration, 'i' is a GCM lgm_by_gcm <- lapply(gcms, function(i){   #Subset variables that belong to GCM i   lgm_gcm_i <- lgm_chelsa[[grepl(i, names(lgm_chelsa))]]   #Rename variables   names(lgm_gcm_i) <- c(\"bio1\", \"bio7\", \"bio12\", \"bio15\")   return(lgm_gcm_i) }) names(lgm_by_gcm) <- gcms # Check units of variables in present #> minmax(present_chelsa[[c(\"bio1\", \"bio7\", \"bio12\", \"bio15\")]]) #>         bio1     bio7    bio12    bio15 #> min 12.87700 10.11300 1211.605 10.32925 #> max 24.70025 21.06125 3063.049 70.45125  # Check units of variables in LGM (CCSM4) minmax(lgm_by_gcm$CCSM4) #>         bio1     bio7    bio12    bio15 #> min 2822.425 173.5125 10710.62  86.9125 #> max 2946.758 219.7700 23159.20 659.0025 lgm_fixed_units <- lapply(lgm_by_gcm, function(x){   x$bio1 <- (x$bio1/10) - 273 #Divide by 10 and subtracts -273 to convert to ºC   x$bio7 <- x$bio7/10 #Divide by 10   x$bio12 <- x$bio12/10 #Divide by 10   x$bio15 <- x$bio15/10 #Divide by 10   return(x) })  #Check units minmax(lgm_fixed_units$CCSM4) #>         bio1     bio7    bio12    bio15 #> min  9.24250 17.35125 1071.062  8.69125 #> max 21.67575 21.97700 2315.920 65.90025 # Create directory to save processed lgm variables dir_lgm <- file.path(tempdir(), \"LGM_CHELSA\") dir.create(dir_lgm)  # In each iteration, i is one of the GCMs  lapply(names(lgm_fixed_units), function(i){   #Subset spatraster from GCM i   r_i <- lgm_fixed_units[[i]]   #Name the file with the Period (LGM) and GCM (i)   filename_i <- paste0(\"CHELSA_LGM_\", i, \".tif\")    #Write Raster   writeRaster(r_i,                filename = file.path(dir_lgm, filename_i)) })"},{"path":"https://marlonecobos.github.io/kuenm2/articles/organize_past_chelsa.html","id":"organize-and-structure-variables","dir":"Articles","previous_headings":"","what":"Organize and structure variables","title":"Organize projections manually: An example with LGM from CHELSA","text":"points must : variables stored TIF files, one file per scenario. names raster variables containing patterns identify time periods, GCMS SSPs (case future scenarios). variables names (bio1, bio2, etc) scenarios names units current variables used calibrate model. Now, can use function organize_for_projection() organize files specific hierarchical manner compatible kuenm2. function requires: present_file, past_files future_files: character vectors full path variables scenarios interest (present, past future, respectively). listing files, important set full.names = TRUE list.files(). past_period /future_period: character vectors specifing time periods past future, respectively. Examples past periods LGM MID. Examples future periods includes “2061-2080” “2100”. Remember periods needs part filenames. past_gcm /future_gcm: character vectors specifing specific GCMS past /future, respectively. Examples GCMS “CCSM4”, “CNRM-CM5”, “FGOALS-g2”. Remember GCMs needs part filenames. future_pscen: character vectors specifing specific emission scenarios future, respectively. Examples future_pscen “ssp126” “ssp585. Remember scenarios needs part filenames. variables_names models: character variables names fitted_model object (variables used model extracted). function also allows use spatial object (SpatVector, SpatRaster, SpatExtent) mask variables, append static variables (.e., topographic variables) climatic variables. Let’s use organize_for_projection() function organize past variables representing LGM conditions. first step using list.files(path, full.names = TRUE list path variables representing present LGM conditions. Remembering, save processed current variables present_dir directory LGM variables dir_lgm: Let’s check listed files ensure storing full path variables scenario: check files, ready organize : can check files structured hierarchically nested folders using dir_tree() function fs package: organizing variables, next step create prepared_projection object. , let’s import fitted_model provided data example kuenm2 package, calibrated using current variables CHELSA used example. information data preparation, model calibration model exploration, consult respective vignetts.","code":"present_list <- list.files(path = present_dir,                          pattern = \"Current_CHELSA\",                         full.names = TRUE) lgm_list <- list.files(path = dir_lgm,                          pattern = \"LGM\",                         full.names = TRUE) present_list #The paths in your computer will be different #> \"Local\\\\Temp\\\\Current_CHELSA.tif\"  lgm_list #The paths in your computer will be different #> [1] \"Local\\\\Temp\\\\CHELSA_LGM_CCSM4.tif\"        #> [2] \"Local\\\\Temp\\\\CHELSA_LGM_CNRM-CM5.tif\"     #> [3] \"Local\\\\Temp\\\\CHELSA_LGM_FGOALS-g2.tif\"    #> [4] \"Local\\\\Temp\\\\CHELSA_LGM_IPSL-CM5A-LR.tif\" #> [5] \"Local\\\\Temp\\\\CHELSA_LGM_MIROC-ESM.tif\"    #> [6] \"Local\\\\Temp\\\\CHELSA_LGM_MPI-ESM-P.tif\"    #> [7] \"Local\\\\Temp\\\\CHELSA_LGM_MRI-CGCM3.tif\" # Create a directory to save the variables #Here, in a temporary directory. Change to your work directory in your computer out_dir <- file.path(tempdir(), \"Projection_variables\")  organize_for_projection(output_dir = out_dir,                          variable_names = c(\"bio1\", \"bio7\", \"bio12\", \"bio15\"),                          present_file = present_list,                          past_files = lgm_list,                          past_period = \"LGM\",                          past_gcm = c(\"CCSM4\", \"CNRM-CM5\", \"FGOALS-g2\",                                       \"IPSL-CM5A-LR\", \"MIROC-ESM\", \"MPI-ESM-P\",                                      \"MRI-CGCM3\"),                         resample_to_present = TRUE,                         overwrite = TRUE) #>  #> Variables successfully organized in directory: #> /tmp/Rtmphaei7S/Projection_variables #Install package if necessary if(!require(\"fs\")){   install.packages(\"fs\") } dir_tree(out_dir) #> Local\\Temp\\Projection_variables #> ├── Past #> │   └── LGM #> │       ├── CCSM4 #> │       │   └── Variables.tif #> │       ├── CNRM-CM5 #> │       │   └── Variables.tif #> │       ├── FGOALS-g2 #> │       │   └── Variables.tif #> │       ├── IPSL-CM5A-LR #> │       │   └── Variables.tif #> │       ├── MIROC-ESM #> │       │   └── Variables.tif #> │       ├── MPI-ESM-P #> │       │   └── Variables.tif #> │       └── MRI-CGCM3 #> │           └── Variables.tif #> └── Present #>     └── Variables.tif"},{"path":"https://marlonecobos.github.io/kuenm2/articles/organize_past_chelsa.html","id":"preparation-of-data-for-model-projections","dir":"Articles","previous_headings":"","what":"Preparation of data for model projections","title":"Organize projections manually: An example with LGM from CHELSA","text":"Now, let’s prepare data model projections across multiple scenarios, storing paths rasters representing scenario. need paths folders raster files stored. includes variables present time, used calibrate fit models. addition storing paths variables scenario, function also verifies variables used fit final models available across scenarios. perform check, need provide either fitted_models object intend use projection simply variable names. strongly suggest using fitted_models object minimize projection errors. also need define root directory containing scenarios projection (present, past, /future), along additional information regarding time periods, SSPs, GCMs. print projection_data object, summarizes scenarios predict also shows root directory predictor rasters stored:","code":"#Define present_dir and past_dir in_dir_present <- file.path(out_dir, \"Present\") in_dir_past <- file.path(out_dir, \"Past\")  # Prepare projections using fitted models to check variables pr <- prepare_projection(models = fitted_model_chelsa,                          present_dir = in_dir_present, #Directory with present-day variables                          past_dir = in_dir_past,                           past_period = \"LGM\",                           past_gcm = c(\"CCSM4\", \"CNRM-CM5\", \"FGOALS-g2\",                                        \"IPSL-CM5A-LR\", \"MIROC-ESM\", \"MPI-ESM-P\",                                       \"MRI-CGCM3\"),                           future_dir = NULL, #NULL because we won't project to the past                          future_period = NULL, #NULL because we won't project to the past                          future_pscen = NULL, #NULL because we won't project to the past                          future_gcm = NULL) #NULL because we won't project to the past pr #> projection_data object summary #> ============================== #> Variables prepared to project models for Present and Past  #> Past projections contain the following periods and GCMs: #>   - Periods: LGM  #>   - GCMs: CCSM4 | CNRM-CM5 | FGOALS-g2 | IPSL-CM5A-LR | MIROC-ESM | MPI-ESM-P | MRI-CGCM3  #> All variables are located in the following root directory: #> Local/Temp/Projection_variables"},{"path":"https://marlonecobos.github.io/kuenm2/articles/organize_past_chelsa.html","id":"project-selected-models-to-multiple-scenarios","dir":"Articles","previous_headings":"","what":"Project selected models to multiple scenarios","title":"Organize projections manually: An example with LGM from CHELSA","text":"preparing data, can use project_selected() function predict selected models across multiple scenarios specified prepare_projections. comprehensive detail step, please see Project models multiple scenarios vignette.","code":"## Create a folder to save projection results #Here, in a temporary directory out_dir_projections <- file.path(tempdir(), \"Projection_results/chelsa\") dir.create(out_dir_projections, recursive = TRUE)  ## Project selected models to multiple scenarios p <- project_selected(models = fitted_model_chelsa,                        projection_data = pr,                       out_dir = out_dir_projections,                        write_replicates = TRUE,                       progress_bar = FALSE, overwrite = T) #Do not print progress bar  #Import mean of each projected scenario p_mean <- import_projections(projection = p, consensus = \"mean\") #Plot all scenarios plot(p_mean, cex.main = 0.8)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/organize_past_chelsa.html","id":"compute-changes","dir":"Articles","previous_headings":"","what":"Compute changes","title":"Organize projections manually: An example with LGM from CHELSA","text":"projecting niche model different temporal scenarios (past future), species’ areas can classified three categories relative current baseline: gain, loss stability. interpretation categories depends temporal direction projection. projecting future scenarios: Gain: Areas currently unsuitable become suitable future. Loss: Areas currently suitable become unsuitable future. Stability: Areas retain current classification future, whether suitable unsuitable. projecting past scenarios: Gain: Areas unsuitable past now suitable present. Loss: Areas suitable past now unsuitable present. Stability: Areas retain past classification present, whether suitable unsuitable. outcomes may vary across different General Circulation Models (GCMs) within time scenario (e.g., various Shared Socioeconomic Pathways (SSPs) period). projection_changes() function summarizes number GCMs predicting gain, loss, stability time scenario. , present example projected changes LGM conditions current period. example changes present future conditions, along details function, see Exploring Model Uncertainty Variability vignette.  example, can see present-day conditions, GCMs indicate species lost much suitable area LGM.","code":"changes <- projection_changes(model_projections = p, consensus = \"mean\",                               output_dir = out_dir_projections,                                write_bin_models = TRUE, # Write individual binarized results                               return_raster = TRUE, overwrite = TRUE)  #Set colors summary_with_colors <- colors_for_changes(changes_projections = changes)  #Plot plot(summary_with_colors$Summary_changes)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"description","dir":"Articles","previous_headings":"","what":"Description","title":"Prepare Data for Model Calibration","text":"starting ENM process, data must formatted specific structure required functions kuenm2. vignette guides users steps necessary prepare occurrence data environmental predictors using built-tools. covers use prepare_data() prepare_user_data() generate standardized objects, essential model calibration. vignette also demonstrates options applying PCA, incorporating sampling bias, saving prepared data later use.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"getting-ready","dir":"Articles","previous_headings":"","what":"Getting ready","title":"Prepare Data for Model Calibration","text":"kuenm2 installed yet, please . See Main guide installation instructions. See basic data cleaning guide steps cleaning data. Use following lines code load kuenm2 required packages, define working directory (needed). general, setting working directory R considered good practice, provides better control files read saved . users working within R project, recommend setting working directory, since least one file saved later stages guide.","code":"# Load packages library(kuenm2) library(terra)  # Current directory getwd()  # Define new directory #setwd(\"YOUR/DIRECTORY\")  # uncomment this line if setting a new directory"},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"import-data","dir":"Articles","previous_headings":"Prepare data","what":"Import data","title":"Prepare Data for Model Calibration","text":"use occurrence records provided within kuenm2 package. example data package derived Trindade & Marques (2024). occ_data object contains 51 occurrences Myrcia hatschbachii, tree endemic Southern Brazil. Although example data set three columns (species, x, y), users’ input data requires two numeric columns longitude latitude coordinates.  predictor variables, use data included package. data set comprises four bioclimatic variables WorldClim 2.1 10 arc-minute resolution, categorical variable (SoilType) SoilGrids resampled 10 arc-minutes. variables masked using polygon delimits area model calibration, generated drawing minimum convex polygon around records 300 km buffer.   Visualize occurrences records geography:","code":"# Import occurrences data(occ_data, package = \"kuenm2\")  # Check data structure str(occ_data) #> 'data.frame':    51 obs. of  3 variables: #>  $ species: chr  \"Myrcia hatschbachii\" \"Myrcia hatschbachii\" \"Myrcia hatschbachii\" \"Myrcia hatschbachii\" ... #>  $ x      : num  -51.3 -50.6 -49.3 -49.8 -50.2 ... #>  $ y      : num  -29 -27.6 -27.8 -26.9 -28.2 ... # Import raster layers var <- rast(system.file(\"extdata\", \"Current_variables.tif\", package = \"kuenm2\"))  # Check variables plot(var) # Visualize occurrences on one variable plot(var[[\"bio_1\"]], main = \"Bio 1\")  points(occ_data[, c(\"x\", \"y\")], col = \"black\")"},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"first-steps-in-preparing-data","dir":"Articles","previous_headings":"Prepare data","what":"First steps in preparing data","title":"Prepare Data for Model Calibration","text":"function prepare_data() central getting data ready model calibration. handles several key steps: Defining algorithm: Users can choose maxnet glm. Generating background points: Background points area sampled raster layers, unless provided user. points serve reference contrast presence records. Principal component analysis (PCA): optional step can applied set predictors PCA. Preparing calibration data: Presence records background points associate predictor values put together data.frame used ENM. Data partitioning: function divides data prepare training testing sets via cross-validation process. partitioning methods available includes kfolds, subsample, bootstrap. Defining grid model parameters: helps setting combinations feature classes (FCs), regularization multiplier (RM) values (Maxnet), sets predictor variables. explanation roles RMs FCs Maxent models see Merow et al. 2013. function, recommend consulting documentation detailed explanations (e.g., help(prepare_data)). Now, let’s prepare data model calibration using prepare_data(), using 4 k-folds partitioning method:  prepare_data() function returns prepared_data object, list containing various essential pieces information fro model calibration. example object printed summarize components.  parts prepared_data object can explored detail indexing following example.  default, prepare_data() prepares object fitting models using maxnet. However, can changed use GLM instead. using GLM, necessary set regularization multipliers, algorithm utilize . Let’s create object fitting models using glm, time using subsample partitioning method, 10 replicates 70% dataset used training.  default, prepare_data() extracts background points entire extent provided raster variables. species-specific polygon defining calibration area, can use mask raster variables delimit calibration area. example, let’s create 100km buffer around occurrence records:  Now, let’s use new polygon mask variables within prepare_data(). Let’s also increase number background points 1,000: Note warning message: “n_background’ >= initial number points, using points”. occurs , masking variables, total number pixels within buffer less specified n_background (1,000). cases, available points within calibration area used background points. following examples, ’ll use object d_maxnet, prepared maxnet algorithm without mask. However, functions compatible objects prepared GLM well.","code":"# Prepare data for maxnet model d <- prepare_data(algorithm = \"maxnet\",                   occ = occ_data,                   x = \"x\", y = \"y\",                   raster_variables = var,                   species = \"Myrcia hatschbachii\",                   categorical_variables = \"SoilType\",                    partition_method = \"kfolds\",                    n_replicates = 4,                   n_background = 1000,                   features = c(\"l\", \"q\", \"lq\", \"lqp\"),                   r_multiplier = c(0.1, 1, 2)) #> Warning in handle_missing_data(occ_bg, weights): 43 rows were excluded from #> database because NAs were found. print(d) #> prepared_data object summary #> ============================ #> Species: Myrcia hatschbachii  #> Number of Records: 1008  #>   - Presence: 51  #>   - Background: 957  #> Partition Method: kfolds  #>   - Number of kfolds: 4  #> Continuous Variables: #>   - bio_1, bio_7, bio_12, bio_15  #> Categorical Variables: #>   - SoilType  #> PCA Information: PCA not performed #> Weights: No weights provided #> Calibration Parameters: #>   - Algorithm: maxnet  #>   - Number of candidate models: 300  #>   - Features classes (responses): l, q, lq, lqp  #>   - Regularization multipliers: 0.1, 2, 1 # See first rows of calibration data head(d$calibration_data) #>   pr_bg    bio_1    bio_7 bio_12   bio_15 SoilType #> 1     1 16.49046 18.66075   1778 12.96107       19 #> 2     1 15.46644 19.65775   1560 14.14697       19 #> 3     1 15.70560 17.99450   1652 23.27548        6 #> 4     1 17.78899 19.55600   1597 18.91694        1 #> 5     1 15.50116 18.30750   1497 15.39440       19 #> 7     1 17.42421 17.25875   1760 34.17664        6  # See first rows of formula grid head(d$formula_grid) #>   ID           Formulas R_multiplier Features #> 1  1  ~bio_1 + bio_7 -1          0.1        l #> 2  2  ~bio_1 + bio_7 -1          2.0        l #> 3  3  ~bio_1 + bio_7 -1          1.0        l #> 4  4 ~bio_1 + bio_12 -1          2.0        l #> 5  5 ~bio_1 + bio_12 -1          1.0        l #> 6  6 ~bio_1 + bio_12 -1          0.1        l d_glm <- prepare_data(algorithm = \"glm\",                       occ = occ_data,                       x = \"x\", y = \"y\",                       raster_variables = var,                       species = \"Myrcia hatschbachii\",                       categorical_variables = \"SoilType\",                        partition_method = \"subsample\",                        n_replicates = 10,                        train_proportion = 0.7,                       n_background = 300,                       features = c(\"l\", \"q\", \"p\", \"lq\", \"lqp\"),                       r_multiplier = NULL) #Not necessary with glms #> Warning in handle_missing_data(occ_bg, weights): 8 rows were excluded from #> database because NAs were found. #Print object d_glm #> prepared_data object summary #> ============================ #> Species: Myrcia hatschbachii  #> Number of Records: 343  #>   - Presence: 51  #>   - Background: 292  #> Partition Method: subsample  #>   - Number of replicates: 10  #>   - Train proportion: 0.7  #> Continuous Variables: #>   - bio_1, bio_7, bio_12, bio_15  #> Categorical Variables: #>   - SoilType  #> PCA Information: PCA not performed #> Weights: No weights provided #> Calibration Parameters: #>   - Algorithm: glm  #>   - Number of candidate models: 122  #>   - Features classes (responses): l, q, p, lq, lqp #Convert dataframe with occurrences to SpatVector pts <- vect(occ_data, geom = c(x = \"x\", y = \"y\"), crs = \"EPSG:4326\") #Create buffer b <- buffer(x = pts, width = 100000) #Width in meters #Aggregate buffers b <- terra::aggregate(b) #Plot buffer plot(b) points(occ_data[, c(\"x\", \"y\")], col = \"black\") d_buffer <- prepare_data(algorithm = \"maxnet\",                          occ = occ_data,                          x = \"x\", y = \"y\",                          raster_variables = var,                          mask = b, #Polygon to mask variables                          species = \"Myrcia hatschbachii\",                          categorical_variables = \"SoilType\",                          n_background = 1000, partition_method = \"kfolds\",                          features = c(\"l\", \"q\", \"p\", \"lq\", \"lqp\"),                          r_multiplier = c(0.1, 1, 2, 3, 5)) #> 'n_background' >= initial number of points, using all points. #> Warning in handle_missing_data(occ_bg, weights): 25 rows were excluded from #> database because NAs were found."},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"exploring-calibration-data","dir":"Articles","previous_headings":"Prepare data","what":"Exploring calibration data","title":"Prepare Data for Model Calibration","text":"Users can visualize distribution predictor values occurrence records, background points, entire calibration area using histograms. example presented . See full documentation help(explore_calibration_hist) help(plot_explore_calibration).  gray bars represent values across entire calibration area. Blue bars show values background, green bars display values presence points (magnified factor 2 improved visualization). can customize colors magnification factor.  Additionally, users can explore geographic distribution occurrence background points. See full documentation help(explore_calibration_geo).   Note , default, background points selected randomly within calibration area. However, users can influence spatial distribution background, increasing decreasing probability selection certain regions, providing bias file (demonstrated next section).","code":"# Prepare histogram data calib_hist <- explore_calibration_hist(data = d, raster_variables = var,                                        include_m = TRUE)  # Plot histograms plot_explore_calibration(explore_calibration = calib_hist) pbg <- explore_calibration_geo(data = d, raster_variables = var[[1]],                                plot = TRUE)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"using-a-bias-file","dir":"Articles","previous_headings":"Prepare data","what":"Using a bias file","title":"Prepare Data for Model Calibration","text":"bias file SpatRaster object contains values influence selection background points within calibration area. can particularly useful mitigating sampling bias, instance, incorporating density records target group (discussed Ponder et al. 2001, Anderson et al. 2003, Barber et al. 2020). bias file must extent, resolution, number cells raster variables, unless mask supplied. mask used, extent bias file encompass larger mask extent. Let’s illustrate example bias file included package. SpatRaster lower values center higher values towards borders:   now use bias file prepare two new datasets: one bias effect “direct” (higher probability regions higher bias values) another effect “inverse” (higher probability regions lower bias values):  Note bias effect “direct”, majority background points sampled borders calibration area, corresponding higher bias values. Conversely, “inverse” bias effect, background points selected center, bias values lower.","code":"# Import a bias file bias <- rast(system.file(\"extdata\", \"bias_file.tif\", package = \"kuenm2\"))  plot(bias) # Using bias as a direct effect in sampling d_bias_direct <- prepare_data(algorithm = \"maxnet\",                               occ = occ_data,                               x = \"x\", y = \"y\",                               raster_variables = var,                               species = \"Myrcia hatschbachii\",                               categorical_variables = \"SoilType\",                               n_background = 1000,                                partition_method = \"kfolds\",                               bias_file = bias, bias_effect = \"direct\",  # bias parameters                               features = c(\"l\", \"q\", \"p\", \"lq\", \"lqp\"),                               r_multiplier = c(0.1, 1, 2, 3, 5)) #> Warning in handle_missing_data(occ_bg, weights): 57 rows were excluded from #> database because NAs were found.  # Using bias as an indirect effect in sampling d_bias_inverse <- prepare_data(algorithm = \"maxnet\",                                occ = occ_data,                                x = \"x\", y = \"y\",                                raster_variables = var,                                species = \"Myrcia hatschbachii\",                                categorical_variables = \"SoilType\",                                n_background = 1000,                                partition_method = \"kfolds\",                                bias_file = bias, bias_effect = \"inverse\",   # bias parameters                                features = c(\"l\", \"q\", \"p\", \"lq\", \"lqp\"),                                r_multiplier = c(0.1, 1, 2, 3, 5)) #> Warning in handle_missing_data(occ_bg, weights): 45 rows were excluded from #> database because NAs were found.  # Compare background points generated randomly versus with bias effects ## Saving original plotting parameters original_par <- par(no.readonly = TRUE)  ## Adjusting plotting grid par(mfrow = c(2,2))    ## The plots to show sampling bias effects plot(bias, main = \"Bias file\") explore_calibration_geo(d, raster_variables = var[[1]],                         main = \"Random Background\") explore_calibration_geo(d_bias_direct, raster_variables = var[[1]],                         main = \"Direct Bias Effect\") explore_calibration_geo(d_bias_inverse, raster_variables = var[[1]],                         main = \"Inverse Bias Effect\") par(original_par)  # Reset grid"},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"pca-for-variables","dir":"Articles","previous_headings":"Prepare data","what":"PCA for variables","title":"Prepare Data for Model Calibration","text":"common approach ENM involves summarizing information set predictor variables smaller set uncorrelated variables using Principal Component Analysis (PCA) (see Cruz-Cardenaz et al. 2014 example). kuenm2 users can perform PCA internally use variables externally prepared PCs.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"internal-pca","dir":"Articles","previous_headings":"Prepare data > PCA for variables","what":"Internal PCA","title":"Prepare Data for Model Calibration","text":"kuenm2 can perform PCA transformations internally, eliminating need prepare new PC variables scenario projection. particularly advantageous projecting model results across multiple time scenarios (e.g., various Global Climate Models different future periods). performing PCA internally, need store raw environmental variables (e.g., bio_1, bio_2, etc.) directory, functions handle PCA transformation needed. Let’s explore implement :  elements calibration data formula grid now generated considering principal components (PCs). default, continuous variables included PCA, categorical variables (e.g., “SoilType”) excluded. default settings number PCs selected retain axes collectively explain 95% total variance, filter , keeping axes individually explain least 5% variance. parameters can changed using arguments function prepare_data  PCA performed internally, prepared_data object contains necessary information transform raw environmental variables required PCs means predicting projecting models, users provide raw raster variables, PCs obtained internally function.","code":"# Prepare data for maxnet models using PCA parameters d_pca <- prepare_data(algorithm = \"maxnet\",                       occ = occ_data,                       x = \"x\", y = \"y\",                       raster_variables = var,                        do_pca = TRUE, center = TRUE, scale = TRUE,  # PCA parameters                       species = \"Myrcia hatschbachii\",                       categorical_variables = \"SoilType\",                       n_background = 1000,                       partition_method = \"kfolds\",                       features = c(\"l\", \"q\", \"p\", \"lq\", \"lqp\"),                       r_multiplier = c(0.1, 1, 2, 3, 5)) #> Warning in handle_missing_data(occ_bg, weights): 43 rows were excluded from #> database because NAs were found.  print(d_pca) #> prepared_data object summary #> ============================ #> Species: Myrcia hatschbachii  #> Number of Records: 1008  #>   - Presence: 51  #>   - Background: 957  #> Partition Method: kfolds  #>   - Number of kfolds: 4  #> Continuous Variables: #>   - bio_1, bio_7, bio_12, bio_15  #> Categorical Variables: #>   - SoilType  #> PCA Information: #>   - Variables included: bio_1, bio_7, bio_12, bio_15  #>   - Number of PCA components: 4  #> Weights: No weights provided #> Calibration Parameters: #>   - Algorithm: maxnet  #>   - Number of candidate models: 610  #>   - Features classes (responses): l, q, p, lq, lqp  #>   - Regularization multipliers: 0.1, 1, 2, 3, 5 # Check calibration data head(d_pca$calibration_data) #>   pr_bg         PC1        PC2        PC3         PC4 SoilType #> 1     1  1.48690341 1.01252697  0.1180156 -0.09119257       19 #> 2     1  1.46028074 0.17701144  1.1573461 -0.12326796       19 #> 3     1  0.82676494 1.21965795  0.8145129 -0.67588891        6 #> 4     1  0.62680441 0.03967459  0.1525997  0.18784282        1 #> 5     1  0.94584897 0.93302089  1.4382424 -0.03192094       19 #> 7     1 -0.07597437 1.55268331 -0.2007953 -0.98153204        6  # Check formula grid head(d_pca$formula_grid) #>   ID      Formulas R_multiplier Features #> 1  1 ~PC1 + PC2 -1          0.1        l #> 2  2 ~PC1 + PC2 -1          1.0        l #> 3  3 ~PC1 + PC2 -1          2.0        l #> 4  4 ~PC1 + PC2 -1          3.0        l #> 5  5 ~PC1 + PC2 -1          5.0        l #> 6  6 ~PC1 + PC3 -1          5.0        l  # Explore variables distribution calib_hist_pca <- explore_calibration_hist(data = d_pca, raster_variables = var,                                            include_m = TRUE, breaks = 7)  plot_explore_calibration(explore_calibration = calib_hist_pca)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"external-pca","dir":"Articles","previous_headings":"Prepare data > PCA for variables","what":"External PCA","title":"Prepare Data for Model Calibration","text":"Alternatively, users can perform PCA data using perform_pca() function, one preference. See full documentation help(perform_pca). Se example perform_pca() :   Now, let’s use PCs generated perform_pca() prepare data:  Note since PCA performed externally, do_pca = FALSE set within prepare_data function. crucial setting TRUE incorrectly apply PCA variables already PCs. Consequently, prepared_data object scenario store PCA-related information. means users predict project models, must must provide PCs instead raw raster variables.","code":"pca_var <- perform_pca(raster_variables = var, exclude_from_pca = \"SoilType\",                        center = TRUE, scale = TRUE)  # Plot plot(pca_var$env) # Prepare data for maxnet model using PCA variables d_pca_extern <- prepare_data(algorithm = \"maxnet\",                              occ = occ_data,                              x = \"x\", y = \"y\",                              raster_variables = pca_var$env,  # Output of perform_pca()                              do_pca = FALSE,  # Set to FALSE because variables are PCs                              species = \"Myrcia hatschbachii\",                              categorical_variables = \"SoilType\",                               n_background = 1000,                               partition_method = \"kfolds\",                              features = c(\"l\", \"q\", \"p\", \"lq\", \"lqp\"),                              r_multiplier = c(0.1, 1, 2, 3, 5)) #> Warning in handle_missing_data(occ_bg, weights): 43 rows were excluded from #> database because NAs were found.  print(d_pca_extern) #> prepared_data object summary #> ============================ #> Species: Myrcia hatschbachii  #> Number of Records: 1008  #>   - Presence: 51  #>   - Background: 957  #> Partition Method: kfolds  #>   - Number of kfolds: 4  #> Continuous Variables: #>   - PC1, PC2, PC3, PC4  #> Categorical Variables: #>   - SoilType  #> PCA Information: PCA not performed #> Weights: No weights provided #> Calibration Parameters: #>   - Algorithm: maxnet  #>   - Number of candidate models: 610  #>   - Features classes (responses): l, q, p, lq, lqp  #>   - Regularization multipliers: 0.1, 1, 2, 3, 5 # Check calibration data head(d_pca_extern$calibration_data) #>   pr_bg         PC1        PC2        PC3         PC4 SoilType #> 1     1  1.48690341 1.01252697  0.1180156 -0.09119257       19 #> 2     1  1.46028074 0.17701144  1.1573461 -0.12326796       19 #> 3     1  0.82676494 1.21965795  0.8145129 -0.67588891        6 #> 4     1  0.62680441 0.03967459  0.1525997  0.18784282        1 #> 5     1  0.94584897 0.93302089  1.4382424 -0.03192094       19 #> 7     1 -0.07597437 1.55268331 -0.2007953 -0.98153204        6  # Check formula grid head(d_pca_extern$formula_grid) #>   ID      Formulas R_multiplier Features #> 1  1 ~PC1 + PC2 -1          0.1        l #> 2  2 ~PC1 + PC2 -1          1.0        l #> 3  3 ~PC1 + PC2 -1          2.0        l #> 4  4 ~PC1 + PC2 -1          3.0        l #> 5  5 ~PC1 + PC2 -1          5.0        l #> 6  6 ~PC1 + PC3 -1          5.0        l"},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"prepare-user-pre-processed-data","dir":"Articles","previous_headings":"Prepare data","what":"Prepare user pre-processed data","title":"Prepare Data for Model Calibration","text":"users already data prepared calibration, can use prepare_user_data() function create object required model calibration. User-prepared calibration data must data.frame includes column indicating presence (1) background (0) records, along columns values variables. package includes example data.frame reference. See example use :  prepare_user_data() function operates similarly prepare_data(), key difference: instead requiring data.frame occurrence coordinates SpatRaster predictor variables, takes already prepared user data.frame (see ). See full documentation help(prepare_user_data). function also allows provide list index identifying test points cross-validation used model calibration. user_folds NULL, function automatically partition data according specified partition_method, n_replicates, train_proportion.","code":"data(\"user_data\", package = \"kuenm2\")  head(user_data) #>   pr_bg    bio_1    bio_7 bio_12   bio_15 SoilType #> 1     1 16.49046 18.66075   1778 12.96107       19 #> 2     1 15.46644 19.65775   1560 14.14697       19 #> 3     1 15.70560 17.99450   1652 23.27548        6 #> 4     1 17.78899 19.55600   1597 18.91694        1 #> 5     1 15.50116 18.30750   1497 15.39440       19 #> 7     1 17.42421 17.25875   1760 34.17664        6 # Prepare data for maxnet model data_user <- prepare_user_data(algorithm = \"maxnet\",                                user_data = user_data,  # user-prepared data.frame                                pr_bg = \"pr_bg\",                                species = \"Myrcia hatschbachii\",                                categorical_variables = \"SoilType\",                                partition_method = \"bootstrap\",                                features = c(\"l\", \"q\", \"p\", \"lq\", \"lqp\"),                                r_multiplier = c(0.1, 1, 2, 3, 5))  data_user  #> prepared_data object summary #> ============================ #> Species: Myrcia hatschbachii  #> Number of Records: 527  #>   - Presence: 51  #>   - Background: 476  #> Partition Method: bootstrap  #>   - Number of replicates: 4  #>   - Train proportion: 0.7  #> Continuous Variables: #>   - bio_1, bio_7, bio_12, bio_15  #> Categorical Variables: #>   - SoilType  #> PCA Information: PCA not performed #> Weights: No weights provided #> Calibration Parameters: #>   - Algorithm: maxnet  #>   - Number of candidate models: 610  #>   - Features classes (responses): l, q, p, lq, lqp  #>   - Regularization multipliers: 0.1, 1, 2, 3, 5"},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"using-custom-data-partitions","dir":"Articles","previous_headings":"","what":"Using Custom Data Partitions","title":"Prepare Data for Model Calibration","text":"functions prepare_data() prepare_user_data() kuenm2 package include four built-methods randomly partitioning data: “kfolds”: Splits dataset K subsets (folds) approximately equal size. replicate, one fold used test set, remaining folds combined form training set. “bootstrap”: Creates training dataset sampling observations original dataset replacement (.e., observation can selected multiple times). test set consists observations selected specific replicate. “subsample”: Similar bootstrap, training set created sampling without replacement (.e., observation selected ). test set includes observations selected training. methods data partitioning also available, including based spatial rules (Radosavljevic Anderson, 2014). Although kuenm2 currently implement spatial partitioning techniques, possible apply using R packages incorporate resulting partitions prepared_data object model calibration within kuenm2 framework. ENMeval flexsdm R packages offers options. , provide examples split data using packages include partition prepared_data object run kuenm2.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"spatial-partitioning-with-enmeval","dir":"Articles","previous_headings":"Using Custom Data Partitions","what":"Spatial Partitioning with ENMeval","title":"Prepare Data for Model Calibration","text":"ENMeval package (Kass et al. 2021) provides three spatial partitioning methods: Spatial block: Divides occurrence data four groups based latitude longitude lines, aiming groups roughly equal size. Checkerboard 1 (basic): Generates checkerboard grid study area assigns points groups based location grid. Checkerboard 2 (hierarchical): Aggregates input raster two hierarchical spatial scales using specified aggregation factors. Points grouped based position resulting hierarchical checkerboard. example, let’s use spatial block method partition dataset. example, use object d, corresponds prepared_data created previous steps. Note output get.block() list two elements: occs.grp bg.grp. occs.grp vector length number occurrence records, bg.grp length background points. vectors contain numeric values 1 4, indicating spatial group point belongs. example, occurrence labeled 1 belongs first spatial block, 2 second block, . contrast, kuenm2 stores partitioned data different format: expects list vectors, element corresponds replicate contains indices test points replicate. can convert spatial group information stored enmeval_block format compatible kuenm2: now list containing four vectors, storing indices test data defined using get.block() function ENMeval package. final step replace original part_data prepared_data object new_part_data. also update partitioning method reflect change. Alternatively, already prepared calibration data, can define custom data partitioning using prepare_user_data() function. example, can use user_data dataset included package:","code":"# Install ENMeval if not already installed if(!require(\"ENMeval\")){   install.packages(\"ENMeval\") }  # Extract calibration data from the prepared_data object and separate presence and background records calib_occ <- d$calibration_data[d$calibration_data$pr_bg == 1,] #Presences calib_bg <- d$calibration_data[d$calibration_data$pr_bg == 0,] #Background  # Apply spatial block partitioning using ENMeval enmeval_block <- get.block(occs = calib_occ, bg = calib_bg)  # Inspect the structure of the output str(enmeval_block) #> List of 2 #>  $ occs.grp: num [1:51] 2 1 1 3 1 4 1 1 3 3 ... #>   $ bg.grp  : num [1:957] 3 3 3 3 3 3 3 1 3 1 ... str(d$part_data) #> List of 4 #>  $ Rep_1: num [1:253] 1 3 12 13 14 20 24 25 34 43 ... #>  $ Rep_2: num [1:252] 5 7 8 10 15 17 23 27 31 33 ... #>  $ Rep_3: num [1:251] 4 9 19 21 28 29 30 32 35 36 ... #>  $ Rep_4: num [1:252] 2 6 11 16 18 22 26 37 38 40 ... # Identify unique spatial blocks id_blocks <- sort(unique(unlist(enmeval_block)))  # Create a list of test indices for each spatial block new_part_data <- lapply(id_blocks, function(i) {   # Indices of occurrence records in group i   rep_i_presence <- which(enmeval_block$occs.grp == i)      # Indices of background records in group i   rep_i_bg <- which(enmeval_block$bg.grp == i)      # Combine presence and background indices for the test set   c(rep_i_presence, rep_i_bg) })  # Assign names to each replicate names(new_part_data) <- paste0(\"Rep_\", id_blocks)  # Inspect the structure of the new partitioned data str(new_part_data) #> List of 4 #>  $ Rep_1: int [1:252] 2 3 5 7 8 12 13 21 29 36 ... #>  $ Rep_2: int [1:13] 1 15 16 18 19 22 23 25 31 38 ... #>  $ Rep_3: int [1:731] 4 9 10 20 24 28 33 34 35 37 ... #>  $ Rep_4: int [1:12] 6 11 14 17 26 27 30 32 40 44 ... # Replace the original partition data with the new spatial blocks d$part_data <- new_part_data  # Update the partitioning method to reflect the new approach d$partition_method <- \"Spatial block (ENMeval)\"  # You can use any descriptive name  # Print the updated prepared_data object print(d) #> prepared_data object summary #> ============================ #> Species: Myrcia hatschbachii  #> Number of Records: 1008  #>   - Presence: 51  #>   - Background: 957  #> Partition Method: Spatial block (ENMeval)  #> Continuous Variables: #>   - bio_1, bio_7, bio_12, bio_15  #> Categorical Variables: #>   - SoilType  #> PCA Information: PCA not performed #> Weights: No weights provided #> Calibration Parameters: #>   - Algorithm: maxnet  #>   - Number of candidate models: 300  #>   - Features classes (responses): l, q, lq, lqp  #>   - Regularization multipliers: 0.1, 2, 1 # Extract presence and background points from the user_data object user_occ <- user_data[user_data$pr_bg == 1, ]  # Presence records user_bg  <- user_data[user_data$pr_bg == 0, ]  # Background records  # Apply spatial block partitioning using ENMeval enmeval_block_user <- get.block(occs = user_occ, bg = user_bg)  # Identify unique spatial blocks id_blocks <- sort(unique(unlist(enmeval_block_user)))  # Create a list of test indices for each spatial block user_part_data <- lapply(id_blocks, function(i) {   # Indices of presence points in block i   rep_i_presence <- which(enmeval_block_user$occs.grp == i)   # Indices of background points in block i   rep_i_bg <- which(enmeval_block_user$bg.grp == i)   # Combine presence and background indices   c(rep_i_presence, rep_i_bg) })  # Name each partition replicate names(user_part_data) <- paste0(\"Rep_\", id_blocks)  # Prepare user data with custom partitions using prepare_user_data() data_user_block <- prepare_user_data(   algorithm = \"maxnet\",   user_data = user_data,              # User-provided data frame   pr_bg = \"pr_bg\",   species = \"Myrcia hatschbachii\",   categorical_variables = \"SoilType\",   user_part = user_part_data,         # Custom partition list   features = c(\"l\", \"q\", \"p\", \"lq\", \"lqp\"),   r_multiplier = c(0.1, 1, 2, 3, 5) )  # Update partition method description data_user_block$partition_method <- \"Spatial block (ENMeval)\"  # Descriptive name  # Display the prepared data object print(data_user_block) #> prepared_data object summary #> ============================ #> Species: Myrcia hatschbachii  #> Number of Records: 527  #>   - Presence: 51  #>   - Background: 476  #> Partition Method: Spatial block (ENMeval)  #> Continuous Variables: #>   - bio_1, bio_7, bio_12, bio_15  #> Categorical Variables: #>   - SoilType  #> PCA Information: PCA not performed #> Weights: No weights provided #> Calibration Parameters: #>   - Algorithm: maxnet  #>   - Number of candidate models: 610  #>   - Features classes (responses): l, q, p, lq, lqp  #>   - Regularization multipliers: 0.1, 1, 2, 3, 5"},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"spatial-partitioning-with-flexsdm","dir":"Articles","previous_headings":"Using Custom Data Partitions","what":"Spatial Partitioning with flexsdm","title":"Prepare Data for Model Calibration","text":"flexsdm (Velazco et al. 2022) offers similar spatial partitioning methods. However, determines number size bands blocks based spatial autocorrelation, environmental similarity, number presence background records within partition. details, please visit package website. example, use Spatial Block method, evaluates different raster cell sizes identify optimal block configuration dataset. output flexsdm differs ENMeval. Instead returning list vectors spatial group IDs occurrences background points separately, flexsdm appends group assignments new column within part element output. ENMeval example, can transform spatial group information stored flexsdm_block format compatible kuenm2:","code":"# Install flexsdm if not already installed if (!require(\"flexsdm\")) {   install.packages(\"flexsdm\") }  # Combine calibration data with spatial coordinates calib_data <- cbind(d$data_xy, d$calibration_data)  # Split data in test and train using flexsdm flexsdm_block <- part_sblock(env_layer = var, data = calib_data,                               x = \"x\", y = \"y\",                              pr_ab = \"pr_bg\", min_res_mult = 1,                              max_res_mult = 500, num_grids = 30, n_part = 4,                               prop = 0.5) #> The following grid cell sizes will be tested: #> 0.17 | 3.03 | 5.9 | 8.77 | 11.64 | 14.51 | 17.37 | 20.24 | 23.11 | 25.98 |  #> 28.84 | 31.71 | 34.58 | 37.45 | 40.32 | 43.18 | 46.05 | 48.92 | 51.79 |  #> 54.66 | 57.52 | 60.39 | 63.26 | 66.13 | 68.99 | 71.86 | 74.73 | 77.6 |  #> 80.47 | 83.33 #>  #> Creating basic raster mask... #>  #> Searching for the optimal grid size...  # See the structure of the object str(flexsdm_block$part) #> Classes ‘tbl_df’, ‘tbl’ and 'data.frame':    1008 obs. of  4 variables: #>  $ x    : num  -51.3 -50.6 -49.3 -49.8 -50.2 ... #>  $ y    : num  -29 -27.6 -27.8 -26.9 -28.2 ... #>  $ pr_ab: num  1 1 1 1 1 1 1 1 1 1 ... #>  $ .part: int  3 1 3 2 3 3 1 4 4 3 ... # Identify unique spatial blocks from flexsdm output id_blocks <- sort(unique(flexsdm_block$part$.part))  # Create a list of test indices for each spatial block new_part_data_flexsdm <- lapply(id_blocks, function(i) {   # Indices of occurrences and background points in group i   which(flexsdm_block$part$.part == i) })  # Assign names to each partition replicate names(new_part_data_flexsdm) <- paste0(\"Rep_\", id_blocks)  # Inspect the structure of the new partitioned data str(new_part_data_flexsdm) #> List of 4 #>  $ Rep_1: int [1:248] 2 7 13 21 24 25 26 27 36 40 ... #>  $ Rep_2: int [1:266] 4 12 15 17 18 19 29 31 32 33 ... #>  $ Rep_3: int [1:247] 1 3 5 6 10 16 20 28 34 35 ... #>  $ Rep_4: int [1:247] 8 9 11 14 22 23 30 37 38 41 ...  # Replace the partition data in the prepared_data object d$part_data <- new_part_data_flexsdm  # Update the partition method description d$partition_method <- \"Spatial block (flexsdm)\"  # You can choose any descriptive name  # Display the updated prepared_data object print(d) #> prepared_data object summary #> ============================ #> Species: Myrcia hatschbachii  #> Number of Records: 1008  #>   - Presence: 51  #>   - Background: 957  #> Partition Method: Spatial block (flexsdm)  #> Continuous Variables: #>   - bio_1, bio_7, bio_12, bio_15  #> Categorical Variables: #>   - SoilType  #> PCA Information: PCA not performed #> Weights: No weights provided #> Calibration Parameters: #>   - Algorithm: maxnet  #>   - Number of candidate models: 300  #>   - Features classes (responses): l, q, lq, lqp  #>   - Regularization multipliers: 0.1, 2, 1"},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"saving-a-prepared_data-object","dir":"Articles","previous_headings":"","what":"Saving a prepared_data object","title":"Prepare Data for Model Calibration","text":"prepared_data object crucial next step ENM workflow kuenm2, model calibration. object essentially list, users can save local directory using saveRDS(). Saving object facilitates loading back R session later using readRDS(). See example :","code":"# Set directory to save (here, in a temporary directory) dir_to_save <- file.path(tempdir())  # Save the data saveRDS(d, file.path(dir_to_save, \"Data.rds\"))  # Import data d <- readRDS(file.path(dir_to_save, \"Data.rds\"))"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Exploring Model Uncertainty and Variability","text":"projecting models across multiple scenarios using project_selected() function, several types analyses can performed: Compute changes scenarios using projection_changes() function. Explore variability arising replicates, model parameterizations, General Circulation Models (GCMs) projection_variability(). Assess extrapolation risks analysis projection_mop(). analyses require model_projections object, output project_selected(). object contains predicted scenario data well directory paths projection rasters saved. create object, follow steps outlined described “Project models multiple scenarios” vignette.","code":"#Load packages library(kuenm2) library(terra) #> terra 1.8.60  #Import calib_results_maxnet data(\"fitted_model_maxnet\", package = \"kuenm2\")  # Import path to raster files with future predictors provided as example # The data is located in the \"inst/extdata\" folder. in_dir <- system.file(\"extdata\", package = \"kuenm2\")  # Import raster layers (same used to calibrate and fit final models) var <- rast(system.file(\"extdata\", \"Current_variables.tif\", package = \"kuenm2\")) #Get soilType soiltype <- var$SoilType  # Organize and structure WorldClim files #Create folder to save structured files out_dir_future <- file.path(tempdir(), \"Future_raw\") #Here, in a temporary directory #Organize organize_future_worldclim(input_dir = in_dir, #Path to the raw variables from WorldClim                           output_dir = out_dir_future,                            name_format = \"bio_\", #Name format                           fixed_variables = var$SoilType, #Static variables                           progress_bar = FALSE, overwrite = TRUE) #>  #> Variables successfully organized in directory: #> /tmp/RtmpRqDWCW/Future_raw  # Create a \"Current_raw\" folder in a temporary directory #and copy the rawvariables there. out_dir_current <- file.path(tempdir(), \"Current_raw\") dir.create(out_dir_current, recursive = TRUE)  # Save current variables in temporary directory terra::writeRaster(var, file.path(out_dir_current, \"Variables.tif\"),                     overwrite = TRUE)  # Prepare projections using fitted models to check variables pr <- prepare_projection(models = fitted_model_maxnet,                          present_dir = out_dir_current, #Directory with present-day variables                          future_dir = out_dir_future, #Directory with future variables                          future_period = c(\"2041-2060\", \"2081-2100\"),                          future_pscen = c(\"ssp126\", \"ssp585\"),                          future_gcm = c(\"ACCESS-CM2\", \"MIROC6\"))  # Project selected models to multiple scenarios ## Create a folder to save projection results #Here, in a temporary directory out_dir <- file.path(tempdir(), \"Projection_results/maxnet\") dir.create(out_dir, recursive = TRUE)  ## Project selected models to multiple scenarios p <- project_selected(models = fitted_model_maxnet,                        projection_data = pr,                       out_dir = out_dir,                       write_replicates = TRUE,                       progress_bar = FALSE, #Do not print progress bar                       overwrite = TRUE)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"compute-changes-between-scenarios","dir":"Articles","previous_headings":"","what":"Compute Changes Between Scenarios","title":"Exploring Model Uncertainty and Variability","text":"projecting niche model different temporal scenarios (past future), species’ areas can classified three categories relative current baseline: gain, loss stability. interpretation categories depends temporal direction projection. projecting future scenarios: Gain: Areas currently unsuitable become suitable future. Loss: Areas currently suitable become unsuitable future. Stability: Areas retain current classification future, whether suitable unsuitable. projecting past scenarios: Gain: Areas unsuitable past now suitable present. Loss: Areas suitable past now unsuitable present. Stability: Areas retain past classification present, whether suitable unsuitable. outcomes may vary across different General Circulation Models (GCMs) within time scenario (e.g., various Shared Socioeconomic Pathways (SSPs) period). projection_changes() function summarizes number GCMs predicting gain, loss, stability time scenario. default, function writes summary results disk (unless write_results set FALSE), save binarized results individual GCMs. example , demonstrate configure function return resulting rasters write binarized results disk.","code":"changes <- projection_changes(model_projections = p,                                output_dir = out_dir,                                write_bin_models = TRUE, # Write individual binarized results                               return_raster = TRUE)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"set-colors-for-change-maps","dir":"Articles","previous_headings":"Compute Changes Between Scenarios","what":"Set Colors for Change Maps","title":"Exploring Model Uncertainty and Variability","text":"plotting results, can use colors_for_changes() function assign custom colors areas gain, loss, stability. default, function uses ‘teal green’ gains, ‘orange-red’ losses, ‘Oxford blue’ areas remain suitable, ‘grey’ areas remain unsuitable. However, can customize colors needed. opacity color automatically adjusted based number GCMs: highest (defined max_alpha) GCMs agree prediction, decreases progressively (min_alpha) fewer GCMs support outcome. function returns changes_projections object, color tables embedded SpatRasters. colors automatically applied visualizing data using plot().","code":"#Set colors for change maps changes_col <- colors_for_changes(changes)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"types-os-results","dir":"Articles","previous_headings":"Compute Changes Between Scenarios","what":"Types os results","title":"Exploring Model Uncertainty and Variability","text":"projection_changes() function returns four main types results: binarized models, results GCM, results change, general summary considering GCMs: Binarized models GCM: suitable/unsuitable maps binarized individual GCM. default, binarization applies omission error threshold used selecting best models (e.g., 10%). can specify different threshold using user_threshold argument.  Results gcm: provides computed changes (gain, loss, stability) GCM individually.  Results change: list SpatRaster represents specific type change (e.g., gain, loss, stability) across GCMs given scenario.  Summary changes: provides general summary indicating many GCMs project gain, loss, stability scenario.","code":"plot(changes_col$Binarized, cex.main = 0.8) plot(changes_col$Results_by_gcm, cex.main = 0.8) # Results by change for the scenario of 2041-2060 (ssp126) plot(changes_col$Results_by_change$`Future_2041-2060_ssp126`) plot(changes_col$Summary_changes,       plg=list(cex=0.75)) #Decrease size of legend text"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"importing-results","dir":"Articles","previous_headings":"Compute Changes Between Scenarios","what":"Importing Results","title":"Exploring Model Uncertainty and Variability","text":"return_raster = TRUE set, resulting SpatRaster objects returned within changes object. default, however, return_raster = FALSE object contains directory path results saved. case, saved results can imported using import_projections() function. can specify type computed changes import, along target period emission scenario. changes_projections object imported using import_projections() can also used input colors_for_changes() customize colors used plotting. example, import general summary 2041–2060 period SSP5-8.5 scenario:","code":"general_changes <- import_projections(projection = changes,                                        future_period = \"2041-2060\",                                        future_pscen = \"ssp585\",                                       change_types = \"summary\") #Set colors general_changes <- colors_for_changes(general_changes)  #Plot plot(general_changes$Summary, main = names(general_changes$Summary),      plg=list(cex=0.75)) #Decrease size of legend text"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"save-changes_projections","dir":"Articles","previous_headings":"Compute Changes Between Scenarios","what":"Save changes_projections","title":"Exploring Model Uncertainty and Variability","text":"changes_projections object list contains resulting SpatRaster objects (return_raster = TRUE) directory path results saved (write_results = TRUE). results written disk initial run, can save SpatRaster objects afterward using writeRaster() function. example, save general summary raster: results saved disk, changes_projections object automatically stored folder named Projection_changes inside specified output_dir. can load back R using readRDS(): loading, object can used import specific results import_projections() function.","code":"writeRaster(changes$Summary_changes,             file.path(out_dir, \"Summary_changes.tif\")) changes <- readRDS(file.path(out_dir, \"Projection_changes/changes_projections.rds\"))"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"explore-variance","dir":"Articles","previous_headings":"","what":"Explore Variance","title":"Exploring Model Uncertainty and Variability","text":"projecting niche models, predictions can vary across different replicates, selected models, GCMs. projection_variability() function quantifies spatializes sources variability, offering valuable insights prediction uncertainty. function requires model_projections object, generated project_selected() function. default, projection_variability() uses median consensus summarize variance across selected models GCMs. Alternatively, users can specify summary statistics mean, range, sd (standard deviation). analyze variability originating replicates, ensure write_replicates = TRUE set running project_selected(). , demonstrate calculate variance different sources (replicates, models, GCMs) save results designated out_dir directory. output variability_projections object, list containing SpatRaster layers represent variance attributed replicates, models, GCMs scenario, including present time. results highlight areas prediction uncertainty higher. example, present time scenario, variance mainly arises differences among replicates.  pessimistic scenario (SSP5-8.5) 2081–2100 period, slight variance observed, primarily arising replicates different GCMs used.","code":"# Create a directory to save results v <- projection_variability(model_projections = p, write_files = TRUE,                             output_dir = out_dir,                             verbose = FALSE, overwrite = T) # Variance for the present time plot(v$Present, range = c(0, 0.15)) plot(v$`Future_2081-2100_ssp585`, range = c(0, 0.1))"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"importing-results-1","dir":"Articles","previous_headings":"Explore Variance","what":"Importing Results","title":"Exploring Model Uncertainty and Variability","text":"write_files = TRUE set, variability_projections object includes file path results saved. can use path import_projections() function load results whenever needed. example, import results 2041–2060 period SSP1-2.6 scenario. scenario, variability mainly originates differences among selected models.","code":"# See the folder where the results were saved v$root_directory #> [1] \"Temp/Projection_results/maxnet/variance\" v_2041_2060_ssp126 <- import_projections(projection = v,                                           present = FALSE, #Do not import results from the present time                                          future_period = \"2041-2060\",                                           future_pscen = \"ssp126\") # Plot plot(v_2041_2060_ssp126, range = c(0, 0.1))"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"saving-the-variability_projections-object","dir":"Articles","previous_headings":"Explore Variance","what":"Saving the variability_projections Object","title":"Exploring Model Uncertainty and Variability","text":"variability_projections object list contains resulting SpatRaster layers (return_raster = TRUE) directory path results saved (write_files = TRUE). results saved disk initial run, can save SpatRaster layers afterward using writeRaster() function. example, save variability map one future scenarios: results saved disk, variability_projections object automatically stored folder named variance within specified output_dir. can reload R using readRDS() function: object can used import results import_projections() function.","code":"writeRaster(v$`Future_2081-2100_ssp585`,              file.path(out_dir, \"Future_2081-2100_ssp585.tif\")) v <- readRDS(file.path(out_dir, \"variance/variability_projections.rds\"))"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"analyze-extrapolation-risks-using-the-mop-metric","dir":"Articles","previous_headings":"","what":"Analyze Extrapolation Risks Using the MOP Metric","title":"Exploring Model Uncertainty and Variability","text":"projecting model predictions new regions time periods, common encounter non-analogous conditions, environmental conditions present calibration area. example, maximum temperature (bio_1) model’s calibration area 22.7∘C22.7^\\circ C: However, future scenarios, conditions projected become warmer, temperatures may reach higher values. illustrate , let’s import environmental variables one GCMs (ACCESS-CM2) future scenario SSP5-8.5 examine maximum temperature:  Note projected area, temperatures expected exceed current maximum temperature. projection_mop() function performs Mobility-Oriented Parity (MOP) analysis (Owens et al. 2013), identifies areas non-analogous environmental conditions pose extrapolation risks. also quantifies dissimilar conditions projection area relative calibration data. MOP analysis requires following inputs: wish perform MOP variables used selected models, set subset_variables = TRUE provide fitted_models object. object class projection_data returned prepare_projection() function, contains paths raster layers representing environmental conditions projected scenarios. default, projection_mop() performs basic MOP, highlights regions non-analogous conditions relative calibration data. Alternatively, can set: - type = \"simple\" compute number variables non-analogous conditions per location. - type = \"detailed\" identify exactly variables exhibit non-analogous conditions. , perform detailed MOP identify areas extrapolation risk future scenarios predictions made: function returns mop_projections object, contains paths directories results saved. object can used import_projections() function load results.","code":"max(fitted_model_maxnet$calibration_data$bio_1) #> [1] 22.6858 #Import variables from the 2081-2100 period (SSP585, GCM MIROC6) future_ACCESS_CM2 <- rast(file.path(out_dir_future,                                 \"2081-2100/ssp585/ACCESS-CM2/Variables.tif\")) minmax(future_ACCESS_CM2$bio_1) #>     bio_1 #> min  17.8 #> max  29.6  #Plot plot(future_ACCESS_CM2$bio_1,       breaks = c(-Inf, 22.7, Inf)) #Highlight regions with temperature above 22.7ºC # Create a folder to save MOP results out_dir_mop <- file.path(tempdir(), \"MOPresults\") dir.create(out_dir_mop, recursive = TRUE)  # Run MOP kmop <- projection_mop(data = fitted_model_maxnet, projection_data = pr,                         subset_variables = TRUE,                        calculate_distance = TRUE,                        out_dir = out_dir_mop,                         type = \"detailed\",                         overwrite = TRUE, progress_bar = FALSE)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"mop-types","dir":"Articles","previous_headings":"Analyze Extrapolation Risks Using the MOP Metric","what":"MOP types","title":"Exploring Model Uncertainty and Variability","text":"results MOP analysis provide multiple perspectives extrapolation risks. component mop_projections object captures different aspect dissimilarity environmental conditions calibration projection areas. importing results, can specify scenarios (e.g., “2081-2100” “ssp585”) well type MOP results load. default, available MOP types imported, including: basic simple towards_high_combined towards_low_combined towards_high_end towards_low_end , examine MOP results SSP5-8.5 scenario 2081–2100 period:","code":"mop_ssp585_2100 <- import_projections(projection = kmop,                                       future_period = \"2081-2100\",                                        future_pscen = \"ssp585\") #See types of results names(mop_ssp585_2100) #> [1] \"distances\"             \"simple\"                \"basic\"                 #> [4] \"towards_high_combined\" \"towards_low_combined\"  \"towards_high_end\"      #> [7] \"towards_low_end\""},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"distances","dir":"Articles","previous_headings":"Analyze Extrapolation Risks Using the MOP Metric > MOP types","what":"Distances","title":"Exploring Model Uncertainty and Variability","text":"distances result represents Euclidean Mahalanobis distance projected environmental conditions (G) calibration dataset (M). Higher distance values indicate greater dissimilarity calibration conditions, highlighting areas increased extrapolation risk.","code":"plot(mop_ssp585_2100$distances)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"basic","dir":"Articles","previous_headings":"Analyze Extrapolation Risks Using the MOP Metric > MOP types","what":"Basic","title":"Exploring Model Uncertainty and Variability","text":"basic result identifies areas least one environmental variable differs reference (calibration) conditions. value ‘1’ indicates presence non-analogous conditions specific area scenario.","code":"plot(mop_ssp585_2100$basic)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"simple","dir":"Articles","previous_headings":"Analyze Extrapolation Risks Using the MOP Metric > MOP types","what":"Simple","title":"Exploring Model Uncertainty and Variability","text":"simple result quantifies number environmental variables projected area non-analogous calibration data.","code":"plot(mop_ssp585_2100$simple)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"combined-towards-highlow","dir":"Articles","previous_headings":"Analyze Extrapolation Risks Using the MOP Metric > MOP types","what":"Combined Towards High/Low","title":"Exploring Model Uncertainty and Variability","text":"towards_high_combined towards_low_combined results identify variables exhibit non-analogous conditions. Specifically, towards_high_combined highlights variables values exceeding observed calibration data, towards_low_combined highlights variables values calibration range.","code":"# Non-analogous conditions towards high values plot(mop_ssp585_2100$towards_high_combined) # Non-analogous conditions towards low values plot(mop_ssp585_2100$towards_low_combined)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"towards-highlow-end","dir":"Articles","previous_headings":"Analyze Extrapolation Risks Using the MOP Metric > MOP types","what":"Towards High/Low End","title":"Exploring Model Uncertainty and Variability","text":"towards_high_end towards_low_end results similar “combined” counterparts provide individual SpatRaster layers variable.","code":"# Non-analogous conditions towards high values in the ACCESS-CM2 scenario plot(mop_ssp585_2100$towards_high_end$`Future_2081-2100_ssp585_ACCESS-CM2`) # Non-analogous conditions towards low values in the MIROC6 scenario plot(mop_ssp585_2100$towards_low_end$`Future_2081-2100_ssp585_ACCESS-CM2`,      main = names(mop_ssp585_2100$towards_low_end$`Future_2081-2100_ssp585_ACCESS-CM2`))"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"handling-in-range-values-na-or-zero","dir":"Articles","previous_headings":"Analyze Extrapolation Risks Using the MOP Metric","what":"Handling In-Range Values: NA or Zero","title":"Exploring Model Uncertainty and Variability","text":"default, projection_mop() function assigns NA cells whose values fall within range calibration data, indicating considered analogous. Alternatively, can assign value 0 cells setting na_in_range = FALSE running projection_mop(). Let’s explore setting affects simple detailed MOP outputs:","code":"# Create a folder to save MOP results, now assigning 0 to cells within the range out_dir_mop_zero <- file.path(tempdir(), \"MOPresults_zero\") dir.create(out_dir_mop_zero, recursive = TRUE)  # Run MOP kmop_zero <- projection_mop(data = fitted_model_maxnet, projection_data = pr,                              subset_variables = TRUE,                              na_in_range = FALSE, #Assign 0 to cells within range                             calculate_distance = TRUE,                             out_dir = out_dir_mop_zero,                              type = \"detailed\",                              overwrite = TRUE, progress_bar = FALSE) # Import MOP for the scenario ssp585 in 2081-2100 mop_ssp585_2100_zero <- import_projections(projection = kmop_zero,                                            future_period = \"2081-2100\",                                             future_pscen = \"ssp585\")  # Compare with the MOP that assigns NA to cells within the calibration range # Simple MOP plot(c(mop_ssp585_2100$simple$`Future_2081-2100_ssp585_ACCESS-CM2`,         mop_ssp585_2100_zero$simple$`Future_2081-2100_ssp585_ACCESS-CM2`),      main = c(\"Within range as NA\", \"Within range as 0\")) # Detailed MOP plot(c(mop_ssp585_2100$towards_high_combined$`Future_2081-2100_ssp585_ACCESS-CM2`,         mop_ssp585_2100_zero$towards_high_combined$`Future_2081-2100_ssp585_ACCESS-CM2`),      main = c(\"Within range as NA\", \"Within range as 0\"),      plg=list(cex=0.6))"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"comparing-mop-results-with-response-curves","dir":"Articles","previous_headings":"Analyze Extrapolation Risks Using the MOP Metric","what":"Comparing MOP Results with Response Curves","title":"Exploring Model Uncertainty and Variability","text":"projection_mop() function identifies areas non-analogous environmental conditions, actual risk extrapolation depends strongly additional factors, especially response curves environmental variables. example, consider future scenario predicted GCM (e.g., ACCESS-CM2 SSP5-8.5 2081–2100), values bio_1 (Annual Mean Temperature), bio_12 (Annual Precipitation) bio_15 (Precipitation Seasonality) exceed upper limits observed calibration data.  Now, let’s examine response curves variables. better visualize model responds range values projected future scenario, can set plotting limits using scenario’s variable values new_data:  response curves bio_1, bio_12, bio_15, higher values correspond lower suitability, reaching zero near upper limit calibration data (indicated dashed line). Beyond upper limit, suitability remains close zero. Given pattern, unlikely suitability increase suddenly even higher values, provides greater confidence model’s extrapolation variables. Next, let’s investigate variables values lower limit calibration data:  regions projected scenario, bio_7 (Temperature Annual Range) exhibits values lower limit calibration data. response curve indicates , extrapolating lower values, suitability continues increase. situation represents higher risk extrapolation substantial uncertainty, don’t know whether suitability truly continues rise low bio_7 values () might eventually decline. example highlights strongly recommend interpreting MOP results alongside response curves.","code":"# Non-analogous conditions towards high values in the MIROC6 scenario plot(mop_ssp585_2100$towards_high_combined$`Future_2081-2100_ssp585_ACCESS-CM2`) par(mfrow = c(1,3)) #Set plot grid response_curve(models = fitted_model_maxnet, variable = \"bio_1\",                 new_data = future_ACCESS_CM2) response_curve(models = fitted_model_maxnet, variable = \"bio_12\",                 new_data = future_ACCESS_CM2) response_curve(models = fitted_model_maxnet, variable = \"bio_15\",                 new_data = future_ACCESS_CM2) #Reinitiate grids on.exit() # Non-analogous conditions towards low values in the MIROC6 scenario par(mfrow = c(1,2)) #Set grid plot(mop_ssp585_2100$towards_low_combined$`Future_2081-2100_ssp585_ACCESS-CM2`) ## It's bio 7. Plot response curve: response_curve(models = fitted_model_maxnet, variable = \"bio_7\",                 new_data = future_ACCESS_CM2) #Reinitiate grid on.exit()"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"saving-and-importing-mop-results","dir":"Articles","previous_headings":"Analyze Extrapolation Risks Using the MOP Metric","what":"Saving and Importing MOP Results","title":"Exploring Model Uncertainty and Variability","text":"projection_mop() run, automatically saves mop_projections object RDS file specified out_dir. can reload object R time using readRDS() function:","code":"# Check for RDS files in the directory where we saved the MOP results list.files(path = out_dir_mop, pattern = \"rds\") #> [1] \"mop_projections.rds\"  # Import the mop_projections file kmop <- readRDS(file.path(out_dir_mop, \"mop_projections.rds\"))"},{"path":"https://marlonecobos.github.io/kuenm2/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Weverton C. F. Trindade. Author, maintainer. Luis F. Arias-Giraldo. Author. Luis Osorio-Olvera. Author. . Townsend Peterson. Author. Marlon E. Cobos. Author.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Trindade W, Arias-Giraldo L, Osorio-Olvera L, Peterson , Cobos M (2025). kuenm2: Detailed Development Ecological Niche Models. R package version 0.0.10, https://marlonecobos.github.io/kuenm2/.","code":"@Manual{,   title = {kuenm2: Detailed Development of Ecological Niche Models},   author = {Weverton C. F. Trindade and Luis F. Arias-Giraldo and Luis Osorio-Olvera and A. Townsend Peterson and Marlon E. Cobos},   year = {2025},   note = {R package version 0.0.10},   url = {https://marlonecobos.github.io/kuenm2/}, }"},{"path":"https://marlonecobos.github.io/kuenm2/index.html","id":"kuenm2-detailed-development-of-ecological-niche-models","dir":"","previous_headings":"","what":"Detailed Development of Ecological Niche Models","title":"Detailed Development of Ecological Niche Models","text":"Weverton C. F. Trindade, Luis F. Arias-Giraldo, Luis Osorio-Olvera, . Townsend Peterson, Marlon E. Cobos Package description Installing package Basic data cleaning Data preparation Model calibration Model explorations Model projections Projection comparisons Variability uncertainty Checking vignettes","code":""},{"path":"https://marlonecobos.github.io/kuenm2/index.html","id":"package-description","dir":"","previous_headings":"","what":"Package description","title":"Detailed Development of Ecological Niche Models","text":"kuenm2 new version kuenm Cobos et al. 2019, R package designed make process ecological niche modeling (ENM) easier, faster, reproducible, time robust. aim package facilitate crucial steps ENM process: data preparation, model calibration, selected model exploration, model projections, analyses uncertainty variability. new version package reduces dependency strictly organized working directory (required projections multiple scenarios needed). Instead, kuenm2 functions generate specific R objects store necessary information subsequent steps. ENM workflow kuenm2 begins data preparation, requires minimum data.frame containing occurrence record coordinates (longitude latitude) SpatRaster object predictor variables. kuenm2 fits maximum entropy (Maxnet) models logistic generalized linear models (GLMs). Maxnet models created described Phillips et al. (2017), GLMs constructed Cobos Peterson (2023).","code":""},{"path":"https://marlonecobos.github.io/kuenm2/index.html","id":"installing-the-package","dir":"","previous_headings":"","what":"Installing the package","title":"Detailed Development of Ecological Niche Models","text":"Note: Internet connection required install package. install latest release kuenm2 use following line code:  development version kuenm2 can installed using code .  problems? problems installation development version GitHub, restart R session, close RStudio sessions may open, try . installation asked update packages, don’t need specific version one packages installed. packages gives error updating, please install alone using install.packages(), try installing kuenm2 .  load package use:","code":"# Installing from CRAN  #install.packages(\"kuenm2\")  # in progress # Installing and loading packages if(!require(remotes)){   install.packages(\"remotes\") }  # To install the package use remotes::install_github(\"marlonecobos/kuenm2\")  # To install the package and its vignettes use (if needed use: force = TRUE)   remotes::install_github(\"marlonecobos/kuenm2\", build_vignettes = TRUE)  # in progress # Load kuenm2 library(kuenm2)"},{"path":"https://marlonecobos.github.io/kuenm2/index.html","id":"workflow-in-kuenm2","dir":"","previous_headings":"","what":"Workflow in kuenm2","title":"Detailed Development of Ecological Niche Models","text":"kuenm2 package facilitates following steps ENM process: basic data cleaning, data preparation, model calibration, model exploration, model projections, projection comparisons, exploration variability uncertainty. figure shows schematic view package works. brief description steps can performed package presented . complete description demonstration steps, see package vignettes listed section Checking vignettes. Figure 1. Schematic view workflow use kuenm2.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/index.html","id":"basic-data-cleaning","dir":"","previous_headings":"Workflow in kuenm2","what":"Basic data cleaning","title":"Detailed Development of Ecological Niche Models","text":"Data cleaning tools kuenm2 help automate following basic steps: columns sorting, missing-data cleaning, duplicate removal, exclusion coordinates longitude latitude values 0, filtering based coordinate decimal precision (see function initial_cleaning()). addition, users can erase duplicates based pixels raster layer, move records barely outside valid pixels raster layer (see function advanced_cleaning()).","code":""},{"path":"https://marlonecobos.github.io/kuenm2/index.html","id":"data-preparation","dir":"","previous_headings":"Workflow in kuenm2","what":"Data preparation","title":"Detailed Development of Ecological Niche Models","text":"kuenm2 two main functions help users prepare data ENM process. functions take initial data, guide users make decisions algorithm used models combination parameters (feature classes, regularization multiplier, sets variables) explored later model calibration. Users can input occurrence records raster layers, data.frame prepared hand. main functions step prepare_data() prepare_user_data(). Users can also explore detail environmental values look like data model calibration using results preparing data functions explore_calibration_geo(), explore_calibration_hist(), plot_explore_calibration().","code":""},{"path":"https://marlonecobos.github.io/kuenm2/index.html","id":"model-calibration","dir":"","previous_headings":"Workflow in kuenm2","what":"Model calibration","title":"Detailed Development of Ecological Niche Models","text":"Model calibration computationally challenging process automated kuenm2. step, candidate models trained tested using k-fold cross-validation approach. , models selected based multiple criteria warranty models used later steps robust among candidates. main function used step calibration().","code":""},{"path":"https://marlonecobos.github.io/kuenm2/index.html","id":"model-explorations","dir":"","previous_headings":"Workflow in kuenm2","what":"Model explorations","title":"Detailed Development of Ecological Niche Models","text":"best performing models selected, users need fit models (fit_selected()) order explore characteristics continue next steps. Fitted models can used assess variable importance models, well explore variable response curves. See functions variable_importance() response_curves().","code":""},{"path":"https://marlonecobos.github.io/kuenm2/index.html","id":"model-projections","dir":"","previous_headings":"Workflow in kuenm2","what":"Model projections","title":"Detailed Development of Ecological Niche Models","text":"selected models fit explored, projections single multiple scenarios can done. facilitate projections simple complex combinations scenarios, multiple functions available. function predict_selected() performs projections single scenarios, function project_selected() helps multiple scenarios. Notice, however, projecting multiple scenarios, steps need done. example projections future scenarios, WorldClim variables, see functions: organize_future_worldclim(), prepare_projection().","code":""},{"path":"https://marlonecobos.github.io/kuenm2/index.html","id":"projection-comparisons","dir":"","previous_headings":"Workflow in kuenm2","what":"Projection comparisons","title":"Detailed Development of Ecological Niche Models","text":"projections multiple scenarios involve transfer another time can compared current scenario, kuenm2 provides way quantify characterize changes. can done using function projection_changes() results help describe changes scenarios, distinct levels agreement changes among scenarios.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/index.html","id":"variability-and-uncertainty","dir":"","previous_headings":"Workflow in kuenm2","what":"Variability and uncertainty","title":"Detailed Development of Ecological Niche Models","text":"results model projections multiple scenarios, tools analyses variability uncertainty included kuenm2. Variability model projections (projection_variability()) represented geographically exploring variance comes replicates, distinct model parameterizations, general circulation models (projections times). represent uncertainty, Mobility Oriented-Parity (MOP) metric used compare projection scenarios conditions model training (projection_mop()).","code":""},{"path":"https://marlonecobos.github.io/kuenm2/index.html","id":"checking-the-vignettes","dir":"","previous_headings":"","what":"Checking the vignettes","title":"Detailed Development of Ecological Niche Models","text":"Users can check kuenm2 vignettes full explanation package functionality. installing development version form GitHub, make sure use argument build_vignettes = TRUE. Check vignettes code :","code":"# Guide to basic data cleaning before the ENM process vignette(\"basic_data_cleaning\")  # Guide to prepare data for the ENM process vignette(\"prepare_data\")  # Guide to train and evaluate candidate models, and select based on performance vignette(\"model_calibration\")   # Guide to explore selected models, variable importance, response curves vignette(\"model_exploration\")  # Guide to predict models in geographic space (single scenarios) vignette(\"model_predictions\")  # Guide to project models in geographic space (multiple scenarios) vignette(\"model_projections\")   # Guide to explore variability and uncertainty in projections (multiple scenarios) vignette(\"variability_and_uncertainty\")  # Guide to organize projections manually: An example with LGM from CHELSA vignette(\"organize_past_chelsa\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/advanced_cleaning.html","id":null,"dir":"Reference","previous_headings":"","what":"Advanced occurrence data cleaning — advanced_cleaning","title":"Advanced occurrence data cleaning — advanced_cleaning","text":"Advanced processes data cleaning involving duplicate removal movement records.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/advanced_cleaning.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Advanced occurrence data cleaning — advanced_cleaning","text":"","code":"advanced_cleaning(data, x, y, raster_layer, cell_duplicates = TRUE,                   move_points_inside = FALSE, move_limit_distance = NULL,                   verbose = TRUE)  remove_cell_duplicates(data, x, y,                        raster_layer)  move_2closest_cell(data, x, y, raster_layer,                    move_limit_distance, verbose = TRUE)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/advanced_cleaning.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Advanced occurrence data cleaning — advanced_cleaning","text":"data data.frame occurrence records. Rows NA values omitted. x (character) name column data containing longitude values. y (character) name column data containing latitude values. raster_layer raster layer (object class SpatRaster). cell_duplicates (logical) whether remove duplicate coordinates considering raster cells. Default = TRUE. move_points_inside (logical) whether move records outside raster cells valid values closest cell values. Default = FALSE. move_limit_distance maximum distance move records outside cells valid values. Default = NULL. Must defined move_points_inside = TRUE. verbose (logical) whether print messages progress. Default = TRUE.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/advanced_cleaning.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Advanced occurrence data cleaning — advanced_cleaning","text":"data.frame occurrence records resulting advanced cleaning procedures. columns added describe changes made original data.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/advanced_cleaning.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Advanced occurrence data cleaning — advanced_cleaning","text":"Data used functions gone initial processes cleaning filtering.","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/bias.html","id":null,"dir":"Reference","previous_headings":"","what":"Example Bias File — bias","title":"Example Bias File — bias","text":"SpatRaster object representing bias layer used extracting background points prepare_data() function.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/bias.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Example Bias File — bias","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/bias.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Example Bias File — bias","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/bias.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Example Bias File — bias","text":"","code":"bias <- terra::rast(system.file(\"extdata\", \"bias_file.tif\",                                 package = \"kuenm2\"))  terra::plot(bias)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/bivariate_response.html","id":null,"dir":"Reference","previous_headings":"","what":"Bivariate response plot for fitted models — bivariate_response","title":"Bivariate response plot for fitted models — bivariate_response","text":"plot suitability prediction two-dimensional environmental space.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/bivariate_response.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bivariate response plot for fitted models — bivariate_response","text":"","code":"bivariate_response(models, variable1 , variable2, modelID = NULL, n = 500,                    new_data = NULL, extrapolate = TRUE, add_bar = TRUE ,                    add_limits = TRUE, color_palette  = NULL,                    xlab = NULL, ylab = NULL, ...)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/bivariate_response.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bivariate response plot for fitted models — bivariate_response","text":"models object class fitted_models returned fit_selected() function. variable1 (character) name variable plotted x axis. variable2 (character) name variable plotted y axis. modelID (character) name ModelID presents fitted object. Default = NULL. n (numeric) number breaks plotting grid. Default = 500 new_data SpatRaster, data.frame,  matrix variables representing area interest. Default = NULL. extrapolate (logical) whether allow extrapolation study behavior response outside calibration limits. Ignored new_data defined. Default = TRUE. add_bar (logical) whether add bar legend. Default = TRUE. add_limits (logical) whether add calibration limits extrapolate = TRUE. Default = TRUE. color_palette (function) color palette function used assign colors plot. default, NULL uses rev(hcl.colors(n, \"terrain\")). xlab (character) label x axis. default, NULL, uses name defined variable1. ylab (character) label y axis. default, NULL, uses name defined variable2. ... additional arguments passed image.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/bivariate_response.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bivariate response plot for fitted models — bivariate_response","text":"bivariate plot considering variable1 variable2.","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/bivariate_response.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bivariate response plot for fitted models — bivariate_response","text":"","code":"# Example with glmnet # Import example of fitted_models (output of fit_selected()) data(fitted_model_maxnet, package = \"kuenm2\")  # Response curve (notice response affected by covariance) bivariate_response(models = fitted_model_maxnet, modelID = \"Model_219\",                    variable1 = \"bio_1\", variable2 = \"bio_12\")   # Example with glm # Import example of fitted_models (output of fit_selected()) data(fitted_model_glm, package = \"kuenm2\")  # Response curve bivariate_response(models = fitted_model_glm, modelID = \"Model_85\",                    variable1 = \"bio_1\", variable2 = \"bio_7\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/calib_results_glm.html","id":null,"dir":"Reference","previous_headings":"","what":"Calibration Results (glm) — calib_results_glm","title":"Calibration Results (glm) — calib_results_glm","text":"calibration_results object resulted calibration() using maxnet algorithm","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/calib_results_glm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calibration Results (glm) — calib_results_glm","text":"","code":"data(\"calib_results_glm\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/calib_results_glm.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Calibration Results (glm) — calib_results_glm","text":"calibration_results following elements: species Species names calibration_data data.frame variables extracted presence background points formula_grid data.frame ID, formulas, regularization multipliers candidate model part_data list partition data, element corresponds replicate contains indices test points replicate partition_method character indicating partition method n_replicates numeric value indicating number replicates k-folds train_proportion numeric value indicating proportion occurrences used train points partition method 'subsample' 'boostrap' data_xy data.frame coordinates occurrence bakground points continuous_variables character indicating names continuous variables categorical_variables character indicating names categorical variables weights numeric value specifying weights occurrence records. NULL, meaning set weights. pca prcomp object storing PCA information. NULL, meaning PCA performed algorithm character indicanting algorithm (glm) calibration_results list containing evaluation metrics candidate model omission_rate numeric value indicating omission rate used evaluate models (10%) addsamplestobackground logical value indicating whether add background presence sample already . selected_models data.frame formulas evaluation metrics selected model summary list number ID models removed selected selection procedure","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/calib_results_maxnet.html","id":null,"dir":"Reference","previous_headings":"","what":"Calibration Results (Maxnet) — calib_results_maxnet","title":"Calibration Results (Maxnet) — calib_results_maxnet","text":"calibration_results object resulted calibration() using maxnet algorithm","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/calib_results_maxnet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calibration Results (Maxnet) — calib_results_maxnet","text":"","code":"data(\"calib_results_maxnet\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/calib_results_maxnet.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Calibration Results (Maxnet) — calib_results_maxnet","text":"calibration_results following elements: species Species names calibration_data data.frame variables extracted presence background points formula_grid data.frame ID, formulas, regularization multipliers candidate model part_data list partition data, element corresponds replicate contains indices test points replicate partition_method character indicating partition method n_replicates numeric value indicating number replicates k-folds train_proportion numeric value indicating proportion occurrences used train points partition method 'subsample' 'boostrap' data_xy data.frame coordinates occurrence bakground points continuous_variables character indicating names continuous variables categorical_variables character indicating names categorical variables weights numeric value specifying weights occurrence records. NULL, meaning set weights. pca prcomp object storing PCA information. NULL, meaning PCA performed algorithm character indicanting algorithm (maxnet) calibration_results list containing evaluation metrics candidate model omission_rate numeric value indicating omission rate used evaluate models (10%) addsamplestobackground logical value indicating whether add background presence sample already . selected_models data.frame formulas evaluation metrics selected model summary list number ID models removed selected selection procedure","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/calibration.html","id":null,"dir":"Reference","previous_headings":"","what":"Fitting and evaluation of models, and selection of the best ones — calibration","title":"Fitting and evaluation of models, and selection of the best ones — calibration","text":"function fits evaluates candidate models using data grid formulas prepared prepare_data(). supports algorithms glm maxnet. function selects best models based unimodality (optional), partial ROC, omission rate, AIC values.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/calibration.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fitting and evaluation of models, and selection of the best ones — calibration","text":"","code":"calibration(data, error_considered, remove_concave = FALSE,             proc_for_all = FALSE, omission_rate = NULL, delta_aic = 2,             allow_tolerance = TRUE, tolerance = 0.01,             addsamplestobackground = TRUE, use_weights = NULL,             write_summary = FALSE, output_directory = NULL,             skip_existing_models = FALSE, return_all_results = TRUE,             parallel = FALSE, ncores = NULL, progress_bar = TRUE,             verbose = TRUE)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/calibration.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fitting and evaluation of models, and selection of the best ones — calibration","text":"data object class prepared_data returned prepare_data() function. contains calibration data, formulas grid, kfolds, model type. error_considered (numeric) values 0 100 representing percentage potential error due source uncertainty data. value used calculate omission rates partial ROC. See details. remove_concave (logical) whether remove candidate models presenting concave curves. Default FALSE. proc_for_all (logical) whether apply partial ROC tests candidate models selected models. Default FALSE, meaning tests applied selected models. omission_rate (numeric) values 0 - 100, maximum omission rate candidate model can considered potentially selected model. default, NULL, uses value error_considered. one value used error_considered, omission_rate must defined. delta_aic (numeric) value delta AIC used threshold select models. Default 2. allow_tolerance (logical) whether allow selection models minimum values omission rates even omission rate surpasses omission_rate. applicable candidate models omission rates higher omission_rate. Default TRUE. tolerance (numeric) value added minimum omission rate exceeds omission_rate. allow_tolerance = TRUE, selected models omission rate equal less minimum rate plus tolerance. Default 0.01. addsamplestobackground (logical) whether add background presence sample already . Default TRUE. use_weights (logical) whether apply weights present data. default, NULL, uses weights provided data. present data, NULL weights 1 presences 100 background. turned FALSE, uses NULL weights even present data. write_summary (logical) whether save evaluation results candidate model disk. Default FALSE. output_directory (character) file name, without path, saving evaluation results candidate model. applicable write_summary = TRUE. skip_existing_models (logical) whether check skip candidate models already fitted saved output_directory. applicable write_summary = TRUE. Default FALSE. return_all_results (logical) whether return evaluation results replicate. Default TRUE, meaning evaluation results replicate returned. parallel (logical) whether fit candidate models parallel. Default FALSE. ncores (numeric) number cores use parallel processing. Default NULL uses available cores - 1. applicable parallel = TRUE. progress_bar (logical) whether display progress bar processing. Default TRUE. verbose (logical) whether display messages processing. Default TRUE.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/calibration.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fitting and evaluation of models, and selection of the best ones — calibration","text":"object class 'calibration_results' containing following elements: species: character string name species. calibration data: data.frame containing column (pr_bg) identifies occurrence points (1) background points (0), along corresponding values predictor variables point. formula_grid: data frame containing calibration grid possible formulas parameters. kfolds: list vectors row indices corresponding fold. data_xy: data.frame occurrence background coordinates. continuous_variables: character indicating continuous variables. categorical_variables: character, categorical variable names (used). weights: numeric vector specifying weights data_xy (used). pca: principal component analysis performed variables, list class \"prcomp\". See prcomp() details. algorithm: model type (glm maxnet) calibration_results: list containing data frame evaluation metrics replicates (return_all_results = TRUE) summary evaluation metrics candidate model. omission_rate: omission rate used select models. addsamplestobackground: logical value indicating whether presence sample already background added. selected_models: data frame ID summary evaluation metrics selected models. summary: list containing delta AIC values model selection, ID values models failed fit, concave curves, non-significant pROC values, omission rates threshold, delta AIC values threshold, selected models.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/calibration.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fitting and evaluation of models, and selection of the best ones — calibration","text":"Partial ROC calculated using values defined error_considered following Peterson et al. (2008). Omission rates calculated using separate testing data subsets. Users can specify multiple values error_considered calculate metric (e.g., c(5, 10)), one can used omission rate model selection. Model fitting complexity (AICc) assessed using models generated complete set occurrences. AICc values computed proposed Warren Seifert (2011).","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/calibration.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Fitting and evaluation of models, and selection of the best ones — calibration","text":"Ninomiya, Yoshiyuki, Shuichi Kawano. \"AIC Lasso generalized linear models.\" (2016): 2537-2560. Warren, D. L., & Seifert, S. N. (2011). Ecological niche modeling Maxent: importance model complexity performance model selection criteria. Ecological applications, 21(2), 335-342.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/calibration.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fitting and evaluation of models, and selection of the best ones — calibration","text":"","code":"# Import occurrences data(occ_data, package = \"kuenm2\")  # Import raster layers var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\")) # Use only continuous variables var <- var[[c(\"bio_1\", \"bio_7\", \"bio_12\", \"bio_15\")]]  # Prepare data for maxnet model sp_swd <- prepare_data(algorithm = \"maxnet\", occ = occ_data,                        x = \"x\", y = \"y\",                        raster_variables = var,                        species = occ_data[1, 1],                        n_background = 100, ,                        features = c(\"l\", \"lq\"),                        r_multiplier = 1,                        partition_method = \"kfolds\") # Model calibration (maxnet) m <- calibration(data = sp_swd, error_considered = 10) #> Task 1/1: fitting and evaluating models: #>    |                                                                               |                                                                      |   0%   |                                                                               |===                                                                   |   5%   |                                                                               |======                                                                |   9%   |                                                                               |==========                                                            |  14%   |                                                                               |=============                                                         |  18%   |                                                                               |================                                                      |  23%   |                                                                               |===================                                                   |  27%   |                                                                               |======================                                                |  32%   |                                                                               |=========================                                             |  36%   |                                                                               |=============================                                         |  41%   |                                                                               |================================                                      |  45%   |                                                                               |===================================                                   |  50%   |                                                                               |======================================                                |  55%   |                                                                               |=========================================                             |  59%   |                                                                               |=============================================                         |  64%   |                                                                               |================================================                      |  68%   |                                                                               |===================================================                   |  73%   |                                                                               |======================================================                |  77%   |                                                                               |=========================================================             |  82%   |                                                                               |============================================================          |  86%   |                                                                               |================================================================      |  91%   |                                                                               |===================================================================   |  95%   |                                                                               |======================================================================| 100% #>  #>  #> Model selection step: #> Selecting best among 22 models. #> Calculating pROC... #>  #> Filtering 22 models. #> Removing 0 model(s) because they failed to fit. #> 7 models were selected with omission rate below 10%. #> Selecting 1 final model(s) with delta AIC <2. #> Validating pROC of selected models... #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #>  #> All selected models have significant pROC values. m #> calibration_results object summary (maxnet) #> ============================================================= #> Species: Myrcia hatschbachii  #> Number of candidate models: 22  #>   - Models removed because they failed to fit: 0  #>   - Models identified with concave curves: 0  #>   - Model with concave curves not removed  #>   - Models removed with non-significant values of pROC: 0  #>   - Models removed with omission error > 10%: 15  #>   - Models removed with delta AIC > 2: 6  #> Selected models: 1  #>   - Up to 5 printed here: #>   ID                   Formulas Features R_multiplier pval_pROC_at_10.mean #> 8  8 ~bio_1 + bio_7 + bio_15 -1        l            1                    0 #>   Omission_rate_at_10.mean dAIC Parameters #> 8                   0.0978    0          3  # Prepare data for glm model sp_swd_glm <- prepare_data(algorithm = \"glm\", occ = occ_data,                            x = \"x\", y = \"y\",                            raster_variables = var,                            species = occ_data[1, 1],                            n_background = 100,                            features = c(\"l\", \"lq\"),                            partition_method = \"kfolds\") m_glm <- calibration(data = sp_swd_glm, error_considered = 10) #> Task 1/1: fitting and evaluating models: #>    |                                                                               |                                                                      |   0%   |                                                                               |===                                                                   |   5%   |                                                                               |======                                                                |   9%   |                                                                               |==========                                                            |  14%   |                                                                               |=============                                                         |  18%   |                                                                               |================                                                      |  23%   |                                                                               |===================                                                   |  27%   |                                                                               |======================                                                |  32%   |                                                                               |=========================                                             |  36%   |                                                                               |=============================                                         |  41%   |                                                                               |================================                                      |  45%   |                                                                               |===================================                                   |  50%   |                                                                               |======================================                                |  55%   |                                                                               |=========================================                             |  59%   |                                                                               |=============================================                         |  64%   |                                                                               |================================================                      |  68%   |                                                                               |===================================================                   |  73%   |                                                                               |======================================================                |  77%   |                                                                               |=========================================================             |  82%   |                                                                               |============================================================          |  86%   |                                                                               |================================================================      |  91%   |                                                                               |===================================================================   |  95%   |                                                                               |======================================================================| 100% #>  #>  #> Model selection step: #> Selecting best among 22 models. #> Calculating pROC... #>  #> Filtering 22 models. #> Removing 0 model(s) because they failed to fit. #> 13 models were selected with omission rate below 10%. #> Selecting 2 final model(s) with delta AIC <2. #> Validating pROC of selected models... #>    |                                                                               |                                                                      |   0%   |                                                                               |===================================                                   |  50%   |                                                                               |======================================================================| 100% #>  #> All selected models have significant pROC values. m_glm #> calibration_results object summary (glm) #> ============================================================= #> Species: Myrcia hatschbachii  #> Number of candidate models: 22  #>   - Models removed because they failed to fit: 0  #>   - Models identified with concave curves: 1  #>   - Model with concave curves not removed  #>   - Models removed with non-significant values of pROC: 0  #>   - Models removed with omission error > 10%: 9  #>   - Models removed with delta AIC > 2: 11  #> Selected models: 2  #>   - Up to 5 printed here: #>    ID                                                        Formulas Features #> 12 12                        ~bio_1 + bio_7 + I(bio_1^2) + I(bio_7^2)       lq #> 18 18 ~bio_1 + bio_7 + bio_12 + I(bio_1^2) + I(bio_7^2) + I(bio_12^2)       lq #>    pval_pROC_at_10.mean Omission_rate_at_10.mean       dAIC Parameters #> 12                    0                   0.0962 0.02361168          4 #> 18                    0                   0.0769 0.00000000          6"},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_current.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing present-day Conditions (CHELSA) — chelsa_current","title":"SpatRaster Representing present-day Conditions (CHELSA) — chelsa_current","text":"Raster layer containing bioclimatic variables representing present-day climatic conditions. variables resampled 10 arc-minute resolution masked using m region provided package. Data sourced CHELSA: https://chelsa-climate.org/","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_current.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing present-day Conditions (CHELSA) — chelsa_current","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_current.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing present-day Conditions (CHELSA) — chelsa_current","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_current.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing present-day Conditions (CHELSA) — chelsa_current","text":"","code":"chelsa_current <- terra::rast(system.file(\"extdata\",                                            \"Current_CHELSA.tif\",                                            package = \"kuenm2\")) terra::plot(chelsa_current)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_ccsm4.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing LGM Conditions (GCM: CCSM4) — chelsa_lgm_ccsm4","title":"SpatRaster Representing LGM Conditions (GCM: CCSM4) — chelsa_lgm_ccsm4","text":"Raster layer containing bioclimatic variables representing Last Glacial Maximum (LGM) climatic conditions based CCSM4 General Circulation Model (GCM). variables resampled 10arc-minutes masked using m provided package. Data sourced CHELSA: https://chelsa-climate.org/last-glacial-maximum-climate/","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_ccsm4.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing LGM Conditions (GCM: CCSM4) — chelsa_lgm_ccsm4","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_ccsm4.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing LGM Conditions (GCM: CCSM4) — chelsa_lgm_ccsm4","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_ccsm4.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing LGM Conditions (GCM: CCSM4) — chelsa_lgm_ccsm4","text":"","code":"chelsa_lgm_ccsm4 <- terra::rast(system.file(\"extdata\",                                            \"CHELSA_LGM_CCSM4.tif\",                                             package = \"kuenm2\"))  terra::plot(chelsa_lgm_ccsm4)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_cnrm_cm5.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing LGM Conditions (GCM: CNRM-CM5) — chelsa_lgm_cnrm_cm5","title":"SpatRaster Representing LGM Conditions (GCM: CNRM-CM5) — chelsa_lgm_cnrm_cm5","text":"Raster layer containing bioclimatic variables representing Last Glacial Maximum (LGM) climatic conditions based CNRM-CM5 General Circulation Model. variables resampled 10arc-minutes masked using m provided package. Data sourced CHELSA: https://chelsa-climate.org/last-glacial-maximum-climate/","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_cnrm_cm5.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing LGM Conditions (GCM: CNRM-CM5) — chelsa_lgm_cnrm_cm5","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_cnrm_cm5.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing LGM Conditions (GCM: CNRM-CM5) — chelsa_lgm_cnrm_cm5","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_cnrm_cm5.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing LGM Conditions (GCM: CNRM-CM5) — chelsa_lgm_cnrm_cm5","text":"","code":"chelsa_lgm_cnrm_cm5 <- terra::rast(system.file(\"extdata\",                                                \"CHELSA_LGM_CNRM-CM5.tif\",                                                package = \"kuenm2\")) terra::plot(chelsa_lgm_cnrm_cm5)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_fgoals_g2.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing LGM Conditions (GCM: FGOALS-g2) — chelsa_lgm_fgoals_g2","title":"SpatRaster Representing LGM Conditions (GCM: FGOALS-g2) — chelsa_lgm_fgoals_g2","text":"Raster layer containing bioclimatic variables representing Last Glacial Maximum (LGM) climatic conditions based FGOALS-g2 General Circulation Model. variables resampled 10arc-minutes masked using m provided package. Data sourced CHELSA: https://chelsa-climate.org/last-glacial-maximum-climate/","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_fgoals_g2.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing LGM Conditions (GCM: FGOALS-g2) — chelsa_lgm_fgoals_g2","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_fgoals_g2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing LGM Conditions (GCM: FGOALS-g2) — chelsa_lgm_fgoals_g2","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_fgoals_g2.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing LGM Conditions (GCM: FGOALS-g2) — chelsa_lgm_fgoals_g2","text":"","code":"chelsa_lgm_fgoals_g2 <- terra::rast(system.file(\"extdata\",                                                \"CHELSA_LGM_FGOALS-g2.tif\",                                                package = \"kuenm2\")) terra::plot(chelsa_lgm_fgoals_g2)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_ipsl.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing LGM Conditions (GCM: IPSL-CM5A-LR) — chelsa_lgm_ipsl","title":"SpatRaster Representing LGM Conditions (GCM: IPSL-CM5A-LR) — chelsa_lgm_ipsl","text":"Raster layer containing bioclimatic variables representing Last Glacial Maximum (LGM) climatic conditions based IPSL-CM5A-LR General Circulation Model. variables resampled 10arc-minutes masked using m provided package. Data sourced CHELSA: https://chelsa-climate.org/last-glacial-maximum-climate/","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_ipsl.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing LGM Conditions (GCM: IPSL-CM5A-LR) — chelsa_lgm_ipsl","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_ipsl.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing LGM Conditions (GCM: IPSL-CM5A-LR) — chelsa_lgm_ipsl","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_ipsl.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing LGM Conditions (GCM: IPSL-CM5A-LR) — chelsa_lgm_ipsl","text":"","code":"chelsa_lgm_ipsl <- terra::rast(system.file(\"extdata\",                                            \"CHELSA_LGM_IPSL-CM5A-LR.tif\",                                            package = \"kuenm2\")) terra::plot(chelsa_lgm_ipsl)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_miroc.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing LGM Conditions (GCM: MIROC-ESM) — chelsa_lgm_miroc","title":"SpatRaster Representing LGM Conditions (GCM: MIROC-ESM) — chelsa_lgm_miroc","text":"Raster layer containing bioclimatic variables representing Last Glacial Maximum (LGM) climatic conditions based MIROC-ESM General Circulation Model. variables resampled 10arc-minutes masked using m provided package. Data sourced CHELSA: https://chelsa-climate.org/last-glacial-maximum-climate/","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_miroc.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing LGM Conditions (GCM: MIROC-ESM) — chelsa_lgm_miroc","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_miroc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing LGM Conditions (GCM: MIROC-ESM) — chelsa_lgm_miroc","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_miroc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing LGM Conditions (GCM: MIROC-ESM) — chelsa_lgm_miroc","text":"","code":"chelsa_lgm_miroc <- terra::rast(system.file(\"extdata\",                                            \"CHELSA_LGM_MIROC-ESM.tif\",                                            package = \"kuenm2\")) terra::plot(chelsa_lgm_miroc)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_mpi.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing LGM Conditions (GCM: MPI-ESM-P) — chelsa_lgm_mpi","title":"SpatRaster Representing LGM Conditions (GCM: MPI-ESM-P) — chelsa_lgm_mpi","text":"Raster layer containing bioclimatic variables representing Last Glacial Maximum (LGM) climatic conditions based MPI-ESM-P General Circulation Model. variables resampled 10arc-minutes masked using m provided package. Data sourced CHELSA: https://chelsa-climate.org/last-glacial-maximum-climate/","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_mpi.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing LGM Conditions (GCM: MPI-ESM-P) — chelsa_lgm_mpi","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_mpi.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing LGM Conditions (GCM: MPI-ESM-P) — chelsa_lgm_mpi","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_mpi.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing LGM Conditions (GCM: MPI-ESM-P) — chelsa_lgm_mpi","text":"","code":"chelsa_lgm_mpi <- terra::rast(system.file(\"extdata\",                                            \"CHELSA_LGM_MPI-ESM-P.tif\",                                            package = \"kuenm2\")) terra::plot(chelsa_lgm_mpi)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_mri.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing LGM Conditions (GCM: MRI-CGCM3) — chelsa_lgm_mri","title":"SpatRaster Representing LGM Conditions (GCM: MRI-CGCM3) — chelsa_lgm_mri","text":"Raster layer containing bioclimatic variables representing Last Glacial Maximum (LGM) climatic conditions based MRI-CGCM3 General Circulation Model. variables resampled 10arc-minutes masked using m provided package. Data sourced CHELSA: https://chelsa-climate.org/last-glacial-maximum-climate/","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_mri.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing LGM Conditions (GCM: MRI-CGCM3) — chelsa_lgm_mri","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_mri.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing LGM Conditions (GCM: MRI-CGCM3) — chelsa_lgm_mri","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_mri.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing LGM Conditions (GCM: MRI-CGCM3) — chelsa_lgm_mri","text":"","code":"chelsa_lgm_mri <- terra::rast(system.file(\"extdata\",                                            \"CHELSA_LGM_MRI-CGCM3.tif\",                                            package = \"kuenm2\")) terra::plot(chelsa_lgm_mri)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/colors_for_changes.html","id":null,"dir":"Reference","previous_headings":"","what":"Set Colors for Change Maps — colors_for_changes","title":"Set Colors for Change Maps — colors_for_changes","text":"functions sets color tables associated SpatRaster object resulting projection_changes(). Color tables used associate specific colors raster values using plot(). function defines custom colors areas gain, loss, stability across scenarios.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/colors_for_changes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set Colors for Change Maps — colors_for_changes","text":"","code":"colors_for_changes(   changes_projections,   gain_color = \"#009E73\",   loss_color = \"#D55E00\",   stable_suitable = \"#0072B2\",   stable_unsuitable = \"grey\",   max_alpha = 1,   min_alpha = 0.25 )"},{"path":"https://marlonecobos.github.io/kuenm2/reference/colors_for_changes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set Colors for Change Maps — colors_for_changes","text":"changes_projections object class changes_projections, generated projection_changes() imported using import_projections(), containing $Summary_changes element. gain_color (character) color used define pallete representing gains. Default \"#009E73\" (teal green). loss_color (character) color used define pallete representing losses. Default \"#D55E00\" (orange-red). stable_suitable (character) color used representing areas remain suitable across scenarios. Default \"#0072B2\" (oxford blue). stable_unsuitable (character) color used representing areas remain unsuitable across scenarios. Default \"grey\". max_alpha (numeric) opacity value (0 1) areas GCMs agree change (gain, loss, stability). Default 1. min_alpha (numeric) opacity value (0 1) areas one GCM predicts given change.  Default 0.25","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/colors_for_changes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set Colors for Change Maps — colors_for_changes","text":"object class changes_projections structure SpatRasters input changes_projections, color tables embedded SpatRasters. colors used automatically visualizing data plot().","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/colors_for_changes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set Colors for Change Maps — colors_for_changes","text":"","code":"# Step 1: Organize variables for current projection ## Import current variables (used to fit models) var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  ## Create a folder in a temporary directory to copy the variables out_dir_current <- file.path(tempdir(), \"Current_raw_color_example\") dir.create(out_dir_current, recursive = TRUE)  ## Save current variables in temporary directory terra::writeRaster(var, file.path(out_dir_current, \"Variables.tif\"))   # Step 2: Organize future climate variables (example with WorldClim) ## Directory containing the downloaded future climate variables (example) in_dir <- system.file(\"extdata\", package = \"kuenm2\")  ## Create a folder in a temporary directory to copy the future variables out_dir_future <- file.path(tempdir(), \"Future_raw_color_example\")  ## Organize and rename the future climate data (structured by year and GCM) ### 'SoilType' will be appended as a static variable in each scenario organize_future_worldclim(input_dir = in_dir, output_dir = out_dir_future,                           name_format = \"bio_\",                           fixed_variables = var$SoilType) #>    |                                                                               |                                                                      |   0%   |                                                                               |=========                                                             |  12%   |                                                                               |==================                                                    |  25%   |                                                                               |==========================                                            |  38%   |                                                                               |===================================                                   |  50%   |                                                                               |============================================                          |  62%   |                                                                               |====================================================                  |  75%   |                                                                               |=============================================================         |  88%   |                                                                               |======================================================================| 100% #>  #> Variables successfully organized in directory: #> /tmp/RtmpAuudrv/Future_raw_color_example  # Step 3: Prepare data to run multiple projections ## An example with maxnet models ## Import example of fitted_models (output of fit_selected()) data(fitted_model_maxnet, package = \"kuenm2\")  ## Prepare projection data using fitted models to check variables pr <- prepare_projection(models = fitted_model_maxnet,                          present_dir = out_dir_current,                          future_dir = out_dir_future,                          future_period = c(\"2081-2100\"),                          future_pscen = c(\"ssp126\", \"ssp585\"),                          future_gcm = c(\"ACCESS-CM2\", \"MIROC6\"),                          raster_pattern = \".tif*\")  # Step 4: Run multiple model projections ## A folder to save projection results out_dir <- file.path(tempdir(), \"Projection_results/maxnet_color_example\") dir.create(out_dir, recursive = TRUE)  ## Project selected models to multiple scenarios p <- project_selected(models = fitted_model_maxnet, projection_data = pr,                       out_dir = out_dir) #>    |                                                                               |                                                                      |   0%   |                                                                               |==============                                                        |  20%   |                                                                               |============================                                          |  40%   |                                                                               |==========================================                            |  60%   |                                                                               |========================================================              |  80%   |                                                                               |======================================================================| 100%  # Step 5: Identify areas of change in projections ## Contraction, expansion and stability changes <- projection_changes(model_projections = p, by_gcm = TRUE,                               by_change = TRUE, write_results = FALSE,                               return_raster = TRUE)  #Step 6: Set Colors for Change Maps changes_with_colors <- colors_for_changes(changes_projections = changes) terra::plot(changes_with_colors$Summary_changes)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/detect_concave.html","id":null,"dir":"Reference","previous_headings":"","what":"Detect concave curves in GLM and GLMNET models — detect_concave","title":"Detect concave curves in GLM and GLMNET models — detect_concave","text":"Identifies presence concave response curves within calibration range GLM GLMNET models.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/detect_concave.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Detect concave curves in GLM and GLMNET models — detect_concave","text":"","code":"detect_concave(model, calib_data, extrapolation_factor = 0.1,                averages_from = \"pr\", var_limits = NULL, plot = FALSE,                mfrow = NULL, legend = FALSE)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/detect_concave.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Detect concave curves in GLM and GLMNET models — detect_concave","text":"model object class glmnet_mx glm. calib_data data.frame matrix data used model calibration. extrapolation_factor (numeric) multiplier used calculate extrapolation range. Larger values allow broader extrapolation beyond observed data range. Default 0.1. averages_from (character) specifies averages modes variables calculated. Available options \"pr\" (calculate averages presence localities) \"pr_bg\" (use combined set presence background localities). Default \"pr\". See details. var_limits (list) named list specifying lower /upper limits variables. first value represents lower limit, second value represents upper limit. Default NULL, meaning specific limits applied, range calculated using extrapolation_factor. See details. plot (logical) whether plot response curve variables. Default FALSE. mfrow (numeric) vector form c(number rows, number columns) specifying layout plots. Default c(1, 1), meaning one plot per window. legend (logical) whether include legend plot. legend indicates whether response curve convex, concave outside range limits, concave within range limits. Default FALSE.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/detect_concave.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Detect concave curves in GLM and GLMNET models — detect_concave","text":"list following elements variable: is_concave (logical): indicates whether response curve variable concave within limit range. occurs quadratic term's coefficient positive vertex lies x_min x_max, vertex (numeric): vertex parabola, representing point curve changes direction. b2 (numeric): coefficient quadratic term variable. Positive values indicate concave curve. x_min x_max (numeric): range limits identify concave curves, calculated observed data range multiplied extrapolation factor. real_x_min real_x_max (numeric) actual range data, excluding extrapolation factor.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/detect_concave.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Detect concave curves in GLM and GLMNET models — detect_concave","text":"Concave curves identified analyzing beta coefficients quadratic terms within variable's range. range extrapolation calculated difference variable's maximum minimum values model, multiplied extrapolation factor. concave curve detected beta coefficient positive, vertex — curve changes direction — lies lower upper limits variable. Users can specify lower upper limits certain variables using var_limits. example, var_limits = list(\"bio12\" = c(0, NA), \"bio15\" = c(0, 100)), lower limit bio12 0, upper limit calculated using extrapolation factor. Similarly, lower upper limits bio15 0 100, respectively. calculating vertex position, response curve given variable generated variables set mean values (mode categorical variables). values calculated either presence localities (averages_from = \"pr\") combined set presence background localities (averages_from = \"pr_bg\").","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/detect_concave.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Detect concave curves in GLM and GLMNET models — detect_concave","text":"","code":"# Import example of a fitted_model (output of fit_selected()) that have # concave curves data(\"fitted_model_concave\", package = \"kuenm2\")  #Response curves ccurves <- detect_concave(model = fitted_model_concave$Models$Model_798$Full_model,                           calib_data = fitted_model_concave$calibration_data,                           extrapolation_factor = 0.2,                           var_limits = list(\"bio_2\" = c(0, NA),                                             \"sand\" = c(0, NA),                                             \"clay\" = c(0, NA)),                           plot = TRUE, mfrow = c(2, 3), legend = TRUE)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/enmeval_block.html","id":null,"dir":"Reference","previous_headings":"","what":"Spatial Blocks from ENMeval — enmeval_block","title":"Spatial Blocks from ENMeval — enmeval_block","text":"list resulting ENMeval::get.block() partition occurrence background localities bins training validation (, evaluation calibration). object used \"Prepare Data Model Calibration\" vignette demonstrate implement custom data partitions generated ENMeval kuenm2.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/enmeval_block.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Spatial Blocks from ENMeval — enmeval_block","text":"","code":"data(\"enmeval_block\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/enmeval_block.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Spatial Blocks from ENMeval — enmeval_block","text":"list following elements: occs.grp numeric vector indicating spatial group occurrence belongs bg.grp numeric vector indicating spatial group background point belongs","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_calibration_geo.html","id":null,"dir":"Reference","previous_headings":"","what":"Explore the spatial distribution of occurrence and background points — explore_calibration_geo","title":"Explore the spatial distribution of occurrence and background points — explore_calibration_geo","text":"Explore spatial distribution occurrence background points","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_calibration_geo.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Explore the spatial distribution of occurrence and background points — explore_calibration_geo","text":"","code":"explore_calibration_geo(data, raster_variables, plot = TRUE, ...)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_calibration_geo.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Explore the spatial distribution of occurrence and background points — explore_calibration_geo","text":"data object class prepared_data returned prepare_data() function. raster_variables (SpatRaster) predictor variables used model calibration. plot (logical) wheter plot SpatRaster. Default TRUE. ... additional arguments passed terra::plot().","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_calibration_geo.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Explore the spatial distribution of occurrence and background points — explore_calibration_geo","text":"categorical SpatRaster four factor values representing: 1 - Background cells  2 - Presence cells  3 - Cells presence background  4 - Non-used cells","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_calibration_geo.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Explore the spatial distribution of occurrence and background points — explore_calibration_geo","text":"","code":"# Import raster layers var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  # Import occurrences data(sp_swd, package = \"kuenm2\")  # Explore calibration data pbg <- explore_calibration_geo(data = sp_swd, raster_variables = var[[1]])"},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_calibration_hist.html","id":null,"dir":"Reference","previous_headings":"","what":"Explore variable distribution for occurrence and background points — explore_calibration_hist","title":"Explore variable distribution for occurrence and background points — explore_calibration_hist","text":"function prepares data generate overlaid histograms visualize distribution predictor variables occurrence (presence) background points.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_calibration_hist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Explore variable distribution for occurrence and background points — explore_calibration_hist","text":"","code":"explore_calibration_hist(data, include_m = FALSE, raster_variables = NULL,                          magnify_occurrences = 2, breaks = 15)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_calibration_hist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Explore variable distribution for occurrence and background points — explore_calibration_hist","text":"data object class prepared_data returned prepare_data() function. include_m (logical) whether include data plotting histogram entire area background points sampled. Default FALSE, meaning background presence information plotted. raster_variables (SpatRaster) predictor variables used prepared data prepared_data. applicable include_m TRUE. magnify_occurrences (numeric) factor frequency occurrences magnified better visualization. Default 2, meaning occurrence frequencies plot doubled. breaks (numeric) single number giving desired number intervals histogram.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_calibration_hist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Explore variable distribution for occurrence and background points — explore_calibration_hist","text":"list information plot informative histograms explore data used modeling process. Histogram plots can plotted function plot_explore_calibration().","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_calibration_hist.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Explore variable distribution for occurrence and background points — explore_calibration_hist","text":"","code":"# Import raster layers var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  # Import occurrences data(sp_swd, package = \"kuenm2\")  # Explore calibration data calib_hist <- explore_calibration_hist(data = sp_swd,                                        raster_variables = var,                                        include_m = TRUE)  # To visualize results use the function plot_explore_calibration()"},{"path":"https://marlonecobos.github.io/kuenm2/reference/extract_occurrence_variables.html","id":null,"dir":"Reference","previous_headings":"","what":"Extracts Environmental Variables for Occurrences — extract_occurrence_variables","title":"Extracts Environmental Variables for Occurrences — extract_occurrence_variables","text":"function extracts values environmental predictor variables (SpatRaster) georeferenced occurrence points. also adds column indicating presence points(pr_bg = 1).","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/extract_occurrence_variables.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extracts Environmental Variables for Occurrences — extract_occurrence_variables","text":"","code":"extract_occurrence_variables(occ, x, y, raster_variables)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/extract_occurrence_variables.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extracts Environmental Variables for Occurrences — extract_occurrence_variables","text":"occ data.frame containing occurrence data. must include columns longitude (x) latitude (y) coordinates. x (character) string specifying name column occ contains longitude values. y (character) string specifying name column occ contains latitude values. raster_variables (SpatRaster) predictor variables used calibrate models.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/extract_occurrence_variables.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extracts Environmental Variables for Occurrences — extract_occurrence_variables","text":"data.frame containing original x y coordinates occurrence points (x y), values variables extracted raster_variables, new column pr_bg value 1 occurrences.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/extract_occurrence_variables.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extracts Environmental Variables for Occurrences — extract_occurrence_variables","text":"","code":"# Import occurrences data(occ_data, package = \"kuenm2\")  # Import raster layers var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  # Extracts environmental variables for occurrences occ_var <- extract_occurrence_variables(occ = occ_data, x = \"x\", y = \"y\",                                         raster_variables = var)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/extract_var_from_formulas.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract predictor names from formulas — extract_var_from_formulas","title":"Extract predictor names from formulas — extract_var_from_formulas","text":"Extract predictor names formulas","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/extract_var_from_formulas.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract predictor names from formulas — extract_var_from_formulas","text":"","code":"extract_var_from_formulas(formulas, ...)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/extract_var_from_formulas.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract predictor names from formulas — extract_var_from_formulas","text":"formulas (character formula) model formulas. ... Arguments pass .vars()","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/extract_var_from_formulas.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract predictor names from formulas — extract_var_from_formulas","text":"character vector list length formulas, containing names predictors formula.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/extract_var_from_formulas.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract predictor names from formulas — extract_var_from_formulas","text":"","code":"# Import an example of calibration results data(calib_results_maxnet, package = \"kuenm2\")  # Extract predictor names vars <- extract_var_from_formulas(calib_results_maxnet$formula_grid$Formulas)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/fit_selected.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit models selected after calibration — fit_selected","title":"Fit models selected after calibration — fit_selected","text":"function fits models selected candidate model training testing using function calibration().","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/fit_selected.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit models selected after calibration — fit_selected","text":"","code":"fit_selected(calibration_results, partition_method = \"kfolds\",              n_replicates = 1, train_proportion = 0.7, type = \"cloglog\",              write_models = FALSE,              file_name = NULL, parallel = FALSE, ncores = NULL,              progress_bar = TRUE, verbose = TRUE, seed = 1)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/fit_selected.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit models selected after calibration — fit_selected","text":"calibration_results object class calibration_results returned calibration() function. partition_method (character) method used data partitioning. Available options \"kfolds\", \"subsample\", \"bootstrap\". See Details information. n_replicates (numeric) number replicates generate. partition_method \"subsample\" \"bootstrap\", defines number partitions. \"kfolds\", specifies number folds. Default 4. train_proportion (numeric) proportion occurrence background points used model training replicate. applicable partition_method \"subsample\" \"bootstrap\". Default 0.7 (.e., 70% training 30% testing). type (character) format prediction values computing thresholds. maxnet models, valid options \"raw\", \"cumulative\", \"logistic\", \"cloglog\". glm models, valid options \"cloglog\", \"response\" \"raw\". Default \"cloglog\". write_models (logical) whether save final fitted models disk. Default FALSE. file_name (character) file name, without path, saving final models. applicable write_models = TRUE. parallel (logical) whether fit final models parallel. Default FALSE. ncores (numeric) number cores use parallel processing. Default NULL uses available cores - 1. applicable parallel = TRUE. progress_bar (logical) whether display progress bar processing. Default TRUE. verbose (logical) whether display detailed messages processing. Default TRUE. seed (numeric) integer value used specify initial seed split data. Default 1.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/fit_selected.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit models selected after calibration — fit_selected","text":"object class 'fitted_models' containing following elements: species character string name species. Models list fitted models, including replicates (trained parts data) full models (trained available records). calibration_data data.frame containing column (pr_bg) identifies occurrence points (1) background points (0), along corresponding values predictor variables point. selected_models data frame ID summary evaluation metrics selected models. weights numeric vector specifying weights predictor variables (used). pca list class prcomp representing result principal component analysis (performed). addsamplestobackground logical value indicating whether presence sample already background added. omission_rate omission rate determined calibration step. thresholds thresholds binarize replicate consensus (mean median), calculated based omission rate set calibration().","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/fit_selected.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fit models selected after calibration — fit_selected","text":"function also computes model consensus (mean median), thresholds binarize model predictions based omission rate set model calibration select models.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/fit_selected.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit models selected after calibration — fit_selected","text":"","code":"# An example with maxnet models data(calib_results_maxnet, package = \"kuenm2\")  # Fit models using calibration results fm <- fit_selected(calibration_results = calib_results_maxnet,                    n_replicates = 4) #> Fitting replicates... #>    |                                                                               |                                                                      |   0%   |                                                                               |=========                                                             |  12%   |                                                                               |==================                                                    |  25%   |                                                                               |==========================                                            |  38%   |                                                                               |===================================                                   |  50%   |                                                                               |============================================                          |  62%   |                                                                               |====================================================                  |  75%   |                                                                               |=============================================================         |  88%   |                                                                               |======================================================================| 100% #>  #> Fitting full models... #>    |                                                                               |                                                                      |   0%   |                                                                               |===================================                                   |  50%   |                                                                               |======================================================================| 100%  # Output the fitted models fm #> fitted_models object summary #> ============================ #> Species: Myrcia hatschbachii  #> Algortihm: maxnet  #> Number of fitted models: 2  #> Models fitted with 4 replicates  # An example with GLMs data(calib_results_glm, package = \"kuenm2\")  # Fit models using calibration results fm_glm <- fit_selected(calibration_results = calib_results_glm,                        partition_method = \"subsample\",                        n_replicates = 5) #> Fitting replicates... #>    |                                                                               |                                                                      |   0%   |                                                                               |==============                                                        |  20%   |                                                                               |============================                                          |  40%   |                                                                               |==========================================                            |  60%   |                                                                               |========================================================              |  80%   |                                                                               |======================================================================| 100% #>  #> Fitting full models... #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100%  # Output the fitted models fm_glm #> fitted_models object summary #> ============================ #> Species: Myrcia hatschbachii  #> Algortihm: glm  #> Number of fitted models: 1  #> Models fitted with 5 replicates"},{"path":"https://marlonecobos.github.io/kuenm2/reference/fitted_model_chelsa.html","id":null,"dir":"Reference","previous_headings":"","what":"Fitted model with CHELSA variables — fitted_model_chelsa","title":"Fitted model with CHELSA variables — fitted_model_chelsa","text":"fitted_models object resulting fit_selected() using calibration data based CHELSA variables.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/fitted_model_chelsa.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fitted model with CHELSA variables — fitted_model_chelsa","text":"","code":"data(\"fitted_model_chelsa\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/fitted_model_chelsa.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Fitted model with CHELSA variables — fitted_model_chelsa","text":"fitted_models following elements: species Species names Models list fitted maxnet models (replicates full models) calibration_data data.frame containing variables extracted presence background points continuous_variables character indicating names continuous variables categorical_variables character indicating names categorical variables selected_models data.frame formulas evaluation metrics selected model weights numeric vector specifying weights occurrence records. NULL weights set. pca prcomp object containing PCA results. NULL PCA performed. addsamplestobackground logical value indicating whether add presence point already included background. omission_rate numeric value indicating omission rate used evaluate models. thresholds numeric vector thresholds used binarize replicate consensus (mean median), calculated based omission rate defined calibration(). algorithm character string indicating algorithm used (maxnet). partition_method character string indicating partitioning method used. n_replicates numeric value indicating number replicates folds. train_proportion numeric value indicating proportion occurrences used training partition method 'subsample' 'bootstrap'.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/fitted_model_concave.html","id":null,"dir":"Reference","previous_headings":"","what":"Fitted model with concave curves — fitted_model_concave","title":"Fitted model with concave curves — fitted_model_concave","text":"maxnet fitted_models object resulting fit_selected() model presenting concave curves.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/fitted_model_concave.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fitted model with concave curves — fitted_model_concave","text":"","code":"data(\"fitted_model_concave\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/fitted_model_concave.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Fitted model with concave curves — fitted_model_concave","text":"fitted_models following elements: species Species names Models list fitted maxnet models (replicates full models) calibration_data data.frame containing variables extracted presence background points continuous_variables character indicating names continuous variables categorical_variables character indicating names categorical variables selected_models data.frame formulas evaluation metrics selected model weights numeric vector specifying weights occurrence records. NULL weights set. pca prcomp object containing PCA results. NULL PCA performed. addsamplestobackground logical value indicating whether add presence point already included background. omission_rate numeric value indicating omission rate used evaluate models. thresholds numeric vector thresholds used binarize replicate consensus (mean median), calculated based omission rate defined calibration(). algorithm character string indicating algorithm used (maxnet). partition_method character string indicating partitioning method used. n_replicates numeric value indicating number replicates folds.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/fitted_model_glm.html","id":null,"dir":"Reference","previous_headings":"","what":"Fitted model with glm algorithm — fitted_model_glm","title":"Fitted model with glm algorithm — fitted_model_glm","text":"glm fitted_models object resulting fit_selected() using calibration data based WorldCLim variables.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/fitted_model_glm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fitted model with glm algorithm — fitted_model_glm","text":"","code":"data(\"fitted_model_glm\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/fitted_model_glm.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Fitted model with glm algorithm — fitted_model_glm","text":"fitted_models following elements: species Species names Models list fitted maxnet models (replicates full models) calibration_data data.frame containing variables extracted presence background points continuous_variables character indicating names continuous variables categorical_variables character indicating names categorical variables selected_models data.frame formulas evaluation metrics selected model weights numeric vector specifying weights occurrence records. NULL weights set. pca prcomp object containing PCA results. NULL PCA performed. addsamplestobackground logical value indicating whether add presence point already included background. omission_rate numeric value indicating omission rate used evaluate models. thresholds numeric vector thresholds used binarize replicate consensus (mean median), calculated based omission rate defined calibration(). algorithm character string indicating algorithm used (glm). partition_method character string indicating partitioning method used. n_replicates numeric value indicating number replicates folds. train_proportion numeric value indicating proportion occurrences used training partition method 'subsample' 'bootstrap'.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/fitted_model_maxnet.html","id":null,"dir":"Reference","previous_headings":"","what":"Fitted model with maxnet algorithm — fitted_model_maxnet","title":"Fitted model with maxnet algorithm — fitted_model_maxnet","text":"maxnet fitted_models object resulting fit_selected() using calibration data based WorldCLim variables.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/fitted_model_maxnet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fitted model with maxnet algorithm — fitted_model_maxnet","text":"","code":"data(\"fitted_model_maxnet\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/fitted_model_maxnet.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Fitted model with maxnet algorithm — fitted_model_maxnet","text":"fitted_models following elements: species Species names Models list fitted maxnet models (replicates full models) calibration_data data.frame containing variables extracted presence background points continuous_variables character indicating names continuous variables categorical_variables character indicating names categorical variables selected_models data.frame formulas evaluation metrics selected model weights numeric vector specifying weights occurrence records. NULL weights set. pca prcomp object containing PCA results. NULL PCA performed. addsamplestobackground logical value indicating whether add presence point already included background. omission_rate numeric value indicating omission rate used evaluate models. thresholds numeric vector thresholds used binarize replicate consensus (mean median), calculated based omission rate defined calibration(). algorithm character string indicating algorithm used (maxnet). partition_method character string indicating partitioning method used. n_replicates numeric value indicating number replicates folds. train_proportion numeric value indicating proportion occurrences used training partition method 'subsample' 'bootstrap'.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/flexsdm_block.html","id":null,"dir":"Reference","previous_headings":"","what":"Spatial Blocks from flexsdm — flexsdm_block","title":"Spatial Blocks from flexsdm — flexsdm_block","text":"list resulting flexsdm::part_sblock(), used partition occurrence background localities bins training evaluation. object used \"Prepare Data Model Calibration\" vignette demonstrate implement custom data partitions generated flexsdm kuenm2","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/flexsdm_block.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Spatial Blocks from flexsdm — flexsdm_block","text":"","code":"data(\"enmeval_block\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/flexsdm_block.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Spatial Blocks from flexsdm — flexsdm_block","text":"list following elements: part tibble object information used 'data' arguments additional column .part partition group. best_part_info tibble information best partition.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp126_access.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing Future Conditions (2041-2060, SSP126, GCM: ACCESS-CM2) — future_2050_ssp126_access","title":"SpatRaster Representing Future Conditions (2041-2060, SSP126, GCM: ACCESS-CM2) — future_2050_ssp126_access","text":"raster layer containing bioclimatic variables representing future climatic conditions (2041-2060) based ACCESS-CM2 General Circulation Model SSP126 scenario. variables obtained 10 arc-minute resolution masked using m region provided package. Data sourced WorldClim: https://worldclim.org/data/cmip6/cmip6climate.html","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp126_access.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing Future Conditions (2041-2060, SSP126, GCM: ACCESS-CM2) — future_2050_ssp126_access","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp126_access.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing Future Conditions (2041-2060, SSP126, GCM: ACCESS-CM2) — future_2050_ssp126_access","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp126_access.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing Future Conditions (2041-2060, SSP126, GCM: ACCESS-CM2) — future_2050_ssp126_access","text":"","code":"future_2050_ssp126_access <- terra::rast(system.file(\"extdata\",                                     \"wc2.1_10m_bioc_ACCESS-CM2_ssp126_2041-2060.tif\",                                      package = \"kuenm2\")) terra::plot(future_2050_ssp126_access)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp126_miroc.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing Future Conditions (2041-2060, SSP126, GCM: MIROC6) — future_2050_ssp126_miroc","title":"SpatRaster Representing Future Conditions (2041-2060, SSP126, GCM: MIROC6) — future_2050_ssp126_miroc","text":"raster layer containing bioclimatic variables representing future climatic conditions (2041-2060) based MIROC6 General Circulation Model SSP126 scenario. variables obtained 10 arc-minute resolution masked using m region provided package. Data sourced WorldClim: https://worldclim.org/data/cmip6/cmip6climate.html","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp126_miroc.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing Future Conditions (2041-2060, SSP126, GCM: MIROC6) — future_2050_ssp126_miroc","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp126_miroc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing Future Conditions (2041-2060, SSP126, GCM: MIROC6) — future_2050_ssp126_miroc","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp126_miroc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing Future Conditions (2041-2060, SSP126, GCM: MIROC6) — future_2050_ssp126_miroc","text":"","code":"future_2050_ssp126_miroc <- terra::rast(system.file(\"extdata\",                                     \"wc2.1_10m_bioc_MIROC6_ssp126_2041-2060.tif\",                                      package = \"kuenm2\")) terra::plot(future_2050_ssp126_miroc)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp585_access.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing Future Conditions (2041-2060, SSP585, GCM: ACCESS-CM2) — future_2050_ssp585_access","title":"SpatRaster Representing Future Conditions (2041-2060, SSP585, GCM: ACCESS-CM2) — future_2050_ssp585_access","text":"raster layer containing bioclimatic variables representing future climatic conditions (2041-2060) based ACCESS-CM2 General Circulation Model SSP585 scenario. variables obtained 10 arc-minute resolution masked using m region provided package. Data sourced WorldClim: https://worldclim.org/data/cmip6/cmip6climate.html","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp585_access.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing Future Conditions (2041-2060, SSP585, GCM: ACCESS-CM2) — future_2050_ssp585_access","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp585_access.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing Future Conditions (2041-2060, SSP585, GCM: ACCESS-CM2) — future_2050_ssp585_access","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp585_access.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing Future Conditions (2041-2060, SSP585, GCM: ACCESS-CM2) — future_2050_ssp585_access","text":"","code":"future_2050_ssp585_access <- terra::rast(system.file(\"extdata\",                                     \"wc2.1_10m_bioc_ACCESS-CM2_ssp585_2041-2060.tif\",                                      package = \"kuenm2\")) terra::plot(future_2050_ssp585_access)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp585_miroc.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing Future Conditions (2041-2060, SSP585, GCM: MIROC6) — future_2050_ssp585_miroc","title":"SpatRaster Representing Future Conditions (2041-2060, SSP585, GCM: MIROC6) — future_2050_ssp585_miroc","text":"raster layer containing bioclimatic variables representing future climatic conditions (2041-2060) based MIROC6 General Circulation Model SSP585 scenario. variables obtained 10 arc-minute resolution masked using m region provided package. Data sourced WorldClim: https://worldclim.org/data/cmip6/cmip6climate.html","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp585_miroc.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing Future Conditions (2041-2060, SSP585, GCM: MIROC6) — future_2050_ssp585_miroc","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp585_miroc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing Future Conditions (2041-2060, SSP585, GCM: MIROC6) — future_2050_ssp585_miroc","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp585_miroc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing Future Conditions (2041-2060, SSP585, GCM: MIROC6) — future_2050_ssp585_miroc","text":"","code":"future_2050_ssp585_miroc <- terra::rast(system.file(\"extdata\",                                     \"wc2.1_10m_bioc_MIROC6_ssp585_2041-2060.tif\",                                      package = \"kuenm2\")) terra::plot(future_2050_ssp585_miroc)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp126_access.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing Future Conditions (2081-2100, SSP126, GCM: ACCESS-CM2) — future_2100_ssp126_access","title":"SpatRaster Representing Future Conditions (2081-2100, SSP126, GCM: ACCESS-CM2) — future_2100_ssp126_access","text":"raster layer containing bioclimatic variables representing future climatic conditions (2081-2100) based ACCESS-CM2 General Circulation Model SSP126 scenario. variables obtained 10 arc-minute resolution masked using m region provided package. Data sourced WorldClim: https://worldclim.org/data/cmip6/cmip6climate.html","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp126_access.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing Future Conditions (2081-2100, SSP126, GCM: ACCESS-CM2) — future_2100_ssp126_access","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp126_access.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing Future Conditions (2081-2100, SSP126, GCM: ACCESS-CM2) — future_2100_ssp126_access","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp126_access.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing Future Conditions (2081-2100, SSP126, GCM: ACCESS-CM2) — future_2100_ssp126_access","text":"","code":"future_2100_ssp126_access <- terra::rast(system.file(\"extdata\",                                     \"wc2.1_10m_bioc_ACCESS-CM2_ssp126_2081-2100.tif\",                                      package = \"kuenm2\")) terra::plot(future_2100_ssp126_access)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp126_miroc.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing Future Conditions (2081-2100, SSP126, GCM: MIROC6) — future_2100_ssp126_miroc","title":"SpatRaster Representing Future Conditions (2081-2100, SSP126, GCM: MIROC6) — future_2100_ssp126_miroc","text":"raster layer containing bioclimatic variables representing future climatic conditions (2081-2100) based MIROC6 General Circulation Model SSP126 scenario. variables obtained 10 arc-minute resolution masked using m region provided package. Data sourced WorldClim: https://worldclim.org/data/cmip6/cmip6climate.html","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp126_miroc.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing Future Conditions (2081-2100, SSP126, GCM: MIROC6) — future_2100_ssp126_miroc","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp126_miroc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing Future Conditions (2081-2100, SSP126, GCM: MIROC6) — future_2100_ssp126_miroc","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp126_miroc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing Future Conditions (2081-2100, SSP126, GCM: MIROC6) — future_2100_ssp126_miroc","text":"","code":"future_2100_ssp126_miroc <- terra::rast(system.file(\"extdata\",                                     \"wc2.1_10m_bioc_MIROC6_ssp126_2081-2100.tif\",                                      package = \"kuenm2\")) terra::plot(future_2100_ssp126_miroc)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp585_access.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing Future Conditions (2081-2100, SSP585, GCM: ACCESS-CM2) — future_2100_ssp585_access","title":"SpatRaster Representing Future Conditions (2081-2100, SSP585, GCM: ACCESS-CM2) — future_2100_ssp585_access","text":"raster layer containing bioclimatic variables representing future climatic conditions (2081-2100) based ACCESS-CM2 General Circulation Model SSP585 scenario. variables obtained 10 arc-minute resolution masked using m region provided package. Data sourced WorldClim: https://worldclim.org/data/cmip6/cmip6climate.html","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp585_access.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing Future Conditions (2081-2100, SSP585, GCM: ACCESS-CM2) — future_2100_ssp585_access","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp585_access.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing Future Conditions (2081-2100, SSP585, GCM: ACCESS-CM2) — future_2100_ssp585_access","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp585_access.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing Future Conditions (2081-2100, SSP585, GCM: ACCESS-CM2) — future_2100_ssp585_access","text":"","code":"future_2100_ssp585_access <- terra::rast(system.file(\"extdata\",                                     \"wc2.1_10m_bioc_ACCESS-CM2_ssp585_2081-2100.tif\",                                      package = \"kuenm2\")) terra::plot(future_2100_ssp585_access)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp585_miroc.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing Future Conditions (2081-2100, SSP585, GCM: MIROC6) — future_2100_ssp585_miroc","title":"SpatRaster Representing Future Conditions (2081-2100, SSP585, GCM: MIROC6) — future_2100_ssp585_miroc","text":"raster layer containing bioclimatic variables representing future climatic conditions (2081-2100) based MIROC6 General Circulation Model SSP585 scenario. variables obtained 10 arc-minute resolution masked using m region provided package. Data sourced WorldClim: https://worldclim.org/data/cmip6/cmip6climate.html","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp585_miroc.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing Future Conditions (2081-2100, SSP585, GCM: MIROC6) — future_2100_ssp585_miroc","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp585_miroc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing Future Conditions (2081-2100, SSP585, GCM: MIROC6) — future_2100_ssp585_miroc","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp585_miroc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing Future Conditions (2081-2100, SSP585, GCM: MIROC6) — future_2100_ssp585_miroc","text":"","code":"future_2100_ssp585_miroc <- terra::rast(system.file(\"extdata\",                                     \"wc2.1_10m_bioc_MIROC6_ssp585_2081-2100.tif\",                                      package = \"kuenm2\")) terra::plot(future_2100_ssp585_miroc)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/glm_mx.html","id":null,"dir":"Reference","previous_headings":"","what":"Maxent-like Generalized Linear Models (GLM) — glm_mx","title":"Maxent-like Generalized Linear Models (GLM) — glm_mx","text":"function fits Generalized Linear Model (GLM) binary presence-background data. allows specification custom weights, default presences weight 1 background 100.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/glm_mx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Maxent-like Generalized Linear Models (GLM) — glm_mx","text":"","code":"glm_mx(formula, family = binomial(link = \"cloglog\"), data,        weights = NULL, ...)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/glm_mx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Maxent-like Generalized Linear Models (GLM) — glm_mx","text":"formula formula specifying model fitted, format used glm. family description error distribution link function used model. Defaults binomial(link = \"cloglog\"), commonly used presence-background data. data data.frame containing variables model. Must include column named pr_bg indicates whether record presence (1) background (0), least another column independent variable (predictor). weights Optional. numeric vector weights observation. provided, default weights 1 presences 100 background used. ... Additional arguments passed glm.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/glm_mx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Maxent-like Generalized Linear Models (GLM) — glm_mx","text":"fitted glm object. model object includes minimum maximum values non-factor variables dataset, stored model$varmin model$varmax.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/glm_mx.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Maxent-like Generalized Linear Models (GLM) — glm_mx","text":"details glms using presence background emulating Maxent , see Fithian Hastie (2013) doi:10.1214/13-AOAS667.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/glmnet_mx.html","id":null,"dir":"Reference","previous_headings":"","what":"Maxent-like glmnet models — glmnet_mx","title":"Maxent-like glmnet models — glmnet_mx","text":"function fits Maxent-like models using glmnet package, designed presence-background data.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/glmnet_mx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Maxent-like glmnet models — glmnet_mx","text":"","code":"glmnet_mx(p, data, f, regmult = 1.0, regfun = maxnet.default.regularization,           addsamplestobackground = TRUE, weights = NULL, ...)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/glmnet_mx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Maxent-like glmnet models — glmnet_mx","text":"p vector binary presence-background labels, 1 indicates presence 0 indicates background. data data.frame containing predictor variables model. must include number rows length p. f formula specifying model fitted, format used model.matrix. regmult (numeric) Regularization multiplier, default 1.0. regfun function calculates regularization penalties. Default maxnet.default.regularization. addsamplestobackground (logical) Whether add presence points background background data. Default TRUE. weights (numeric) numeric vector weights observation. Default NULL, sets weights 1 presence points 100 background points. ... Additional arguments pass glmnet.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/glmnet_mx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Maxent-like glmnet models — glmnet_mx","text":"fitted Maxent-like model object class glmnet_mx, includes model coefficients, AIC (requested), elements feature mins maxes, sample means, entropy.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/glmnet_mx.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Maxent-like glmnet models — glmnet_mx","text":"function modified package maxnet fits Maxent-like model using regularization avoid -fitting. Regularization weights computed using provided function (can changed) can multiplied regularization multiplier (regmult). function also includes option calculate AIC.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/import_projections.html","id":null,"dir":"Reference","previous_headings":"","what":"Import rasters resulting from projection functions — import_projections","title":"Import rasters resulting from projection functions — import_projections","text":"function facilitates import rasters generated written disk project_selected(), projection_changes(), variability_projections(), projection_mop() functions. Users can select specific periods (past/future), emission scenarios, General Circulation Models (GCMs), result types import.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/import_projections.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Import rasters resulting from projection functions — import_projections","text":"","code":"import_projections(   projection,   consensus = c(\"median\", \"range\", \"mean\", \"stdev\"),   present = TRUE,   past_period = NULL,   past_gcm = NULL,   future_period = NULL,   future_pscen = NULL,   future_gcm = NULL,   change_types = c(\"summary\", \"by_gcm\", \"by_change\"),   mop_types = c(\"distances\", \"simple\", \"basic\", \"towards_high_combined\",     \"towards_low_combined\", \"towards_high_end\", \"towards_low_end\") )"},{"path":"https://marlonecobos.github.io/kuenm2/reference/import_projections.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Import rasters resulting from projection functions — import_projections","text":"projection object class model_projections, changes_projections, variability_projections, mop_projections. object direct output one projection functions listed description. consensus (character) consensus measures import. Available options : 'median', 'range', 'mean' 'stdev' (standard deviation). Default c(\"median\", \"range\", \"mean\", \"stdev\"), imports options. applicable projection model_projections object. present (logical) wheter import present-day projections. Default TRUE. applicable projection  changes_projections object. past_period (character) names specific past periods (e.g., 'LGM' 'MID') import. Default NULL, meaning available past periods imported. past_gcm (character) names specific General Circulation Models (GCMs) past import. Default NULL, meaning available past GCMs imported. future_period (character) names specific future periods (e.g., '2041-2060' '2081-2100') import. Default NULL, meaning available future periods imported. future_pscen (character) names specific future emission scenarios (e.g., 'ssp126' 'ssp585') import. Default NULL, meaning available future scenarios imported. future_gcm (character) names specific General Circulation Models (GCMs) future import. Default NULL, meaning available future GCMs imported. change_types (character) names type computed changes import. Available options : 'summary', 'by_gcm', 'by_change' 'binarized'. Default c(\"summary\", \"by_gcm\", \"by_change\"), importing types. applicable projection changes_projections object. mop_types (character) type(s) MOP import. Available options : 'basic', 'simple', 'towards_high_combined', 'towards_low_combined', towards_high_end', 'towards_low_end'. Default NULL, meaning available MOPs imported. applicable projection mop_projections object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/import_projections.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Import rasters resulting from projection functions — import_projections","text":"SpatRaster list SpatRasters, structured according input projection class: projection model_projections: stacked SpatRaster containing selected projections. projection changes_projections: list SpatRasters, organized selected change_types (e.g., 'summary', 'by_gcm', /'by_change'). projection mop_projections: list SpatRasters, organized selected mop_types (e.g., 'simple' 'basic'). projection variability_projections: list SpatRasters, containing computed variability.","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/import_projections.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Import rasters resulting from projection functions — import_projections","text":"","code":"# Load packages library(terra) #> terra 1.8.60 # Step 1: Organize variables for current projection ## Import current variables (used to fit models) var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  ## Create a folder in a temporary directory to copy the variables out_dir_current <- file.path(tempdir(), \"Current_raw2\") dir.create(out_dir_current, recursive = TRUE)  ## Save current variables in temporary directory terra::writeRaster(var, file.path(out_dir_current, \"Variables.tif\"))   # Step 2: Organize future climate variables (example with WorldClim) ## Directory containing the downloaded future climate variables (example) in_dir <- system.file(\"extdata\", package = \"kuenm2\")  ## Create a folder in a temporary directory to copy the future variables out_dir_future <- file.path(tempdir(), \"Future_raw2\")  ## Organize and rename the future climate data (structured by year and GCM) ### 'SoilType' will be appended as a static variable in each scenario organize_future_worldclim(input_dir = in_dir, output_dir = out_dir_future,                           name_format = \"bio_\", fixed_variables = var$SoilType) #>    |                                                                               |                                                                      |   0%   |                                                                               |=========                                                             |  12%   |                                                                               |==================                                                    |  25%   |                                                                               |==========================                                            |  38%   |                                                                               |===================================                                   |  50%   |                                                                               |============================================                          |  62%   |                                                                               |====================================================                  |  75%   |                                                                               |=============================================================         |  88%   |                                                                               |======================================================================| 100% #>  #> Variables successfully organized in directory: #> /tmp/RtmpAuudrv/Future_raw2  # Step 3: Prepare data to run multiple projections ## An example with maxnet models ## Import example of fitted_models (output of fit_selected()) data(fitted_model_maxnet, package = \"kuenm2\")  ## Prepare projection data using fitted models to check variables pr <- prepare_projection(models = fitted_model_maxnet,                          present_dir = out_dir_current,                          future_dir = out_dir_future,                          future_period = \"2041-2060\",                          future_pscen = c(\"ssp126\", \"ssp585\"),                          future_gcm = c(\"ACCESS-CM2\", \"MIROC6\"),                          raster_pattern = \".tif*\")  # Step 4: Run multiple model projections ## A folder to save projection results out_dir <- file.path(tempdir(), \"Projection_results/maxnet\") dir.create(out_dir, recursive = TRUE)  ## Project selected models to multiple scenarios p <- project_selected(models = fitted_model_maxnet, projection_data = pr,                       out_dir = out_dir) #>    |                                                                               |                                                                      |   0%   |                                                                               |==============                                                        |  20%   |                                                                               |============================                                          |  40%   |                                                                               |==========================================                            |  60%   |                                                                               |========================================================              |  80%   |                                                                               |======================================================================| 100%  # Use import_projections to import results: raster_p <- import_projections(projection = p, consensus = \"mean\") plot(raster_p)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/independent_eval.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluate models with independent data — independent_eval","title":"Evaluate models with independent data — independent_eval","text":"function evaluates selected models using independent data (.e., data used model calibration). function computes omission rate pROC, optionally assesses whether environmental conditions independent data analogous (.e., within range) calibration data.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/independent_eval.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluate models with independent data — independent_eval","text":"","code":"independent_eval(fitted_models, new_data,                         consensus = c(\"mean\", \"median\"), type = \"cloglog\",                         extrapolation_type = \"E\", var_to_clamp = NULL,                         perform_mop = TRUE, mop_type = \"detailed\",                         calculate_distance = TRUE, where_distance = \"all\",                         return_predictions = TRUE, return_binary = TRUE,                         progress_bar = FALSE, ...)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/independent_eval.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluate models with independent data — independent_eval","text":"fitted_models object class fitted_models returned fit_selected() function. new_data data.frame containing environmental variables independent test records. column names must correspond exactly environmental variables used fit selected models, row individual test record. consensus (character) vector specifying types consensus use. Available options \"median\" \"mean\". Default c(\"median\", \"mean\"). type (character) format prediction values. maxnet models, valid options \"raw\", \"cumulative\", \"logistic\", \"cloglog\". glm models, valid options \"response\" \"cloglog\". Default \"cloglog\". extrapolation_type (character) extrapolation type model. Models can transferred three options: free extrapolation ('E'), extrapolation clamping ('EC'), extrapolation ('NE'). Default = 'E'. See details. var_to_clamp (character) vector specifying variables clamp extrapolate. applicable extrapolation_type \"EC\" \"NE\". Default NULL, meaning variables clamped extrapolated. perform_mop (logical) whether execute Mobility-Oriented Parity (MOP) analysis. analysis assesses environmental conditions new_data analogous (within ranges) calibration data. Defaults TRUE. mop_type (character) type MOP analysis performed. Options available \"basic\", \"simple\" \"detailed\". Default 'simples'. See projection_mop() details. calculate_distance (logical) whether calculate distances (dissimilarities) new_data calibration data. Default TRUE. where_distance (character) specifies values new_data used calculate distances. Options : \"in_range\" (conditions within calibration range), \"out_range\" (conditions outside calibration range), \"\" (conditions). Default \"\". return_predictions (logical) whether return continuous predictions locations independent records new_data. Default TRUE. return_binary (logical) whether return binary predictions locations independent records new_data. predictions binarized using respective thresholds stores fitted_models. Default TRUE. progress_bar (logical) whether display progress bar mop processing. Default FALSE. ... additional arguments passed mop().","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/independent_eval.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evaluate models with independent data — independent_eval","text":"list containing following elements: evaluation: data.frame omission rate pROC values selected model consensus. mop_results: (perform_mop = TRUE) object class mop_results, metrics environmental similarity calibration independent data. predictions: (return_predictions = TRUE) list data.frames containing continuous binary predictions independent record locations, along MOP distances indicator whether environmental conditions location fall within calibration range.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/independent_eval.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Evaluate models with independent data — independent_eval","text":"","code":"# Example with maxnet # Import example of fitted_models (output of fit_selected()) data(\"fitted_model_maxnet\", package = \"kuenm2\")  # Import independent records to evaluate the models data(\"new_occ\", package = \"kuenm2\")  # Import raster layers var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  #Extract variables to occurrences new_data <- extract_occurrence_variables(occ = new_occ, x = \"x\", y = \"y\",                                          raster_variables = var)  #Add some fake data beyond the limits of calibration ranges fake_data <- data.frame(\"pr_bg\" = c(1, 1, 1),                         \"x\" = c(NA, NA, NA),                         \"y\" = c(NA, NA, NA),                         \"bio_1\" = c(10, 15, 23),                         \"bio_7\" = c(12, 16, 20),                         \"bio_12\" = c(2300, 2000, 1000),                         \"bio_15\" = c(30, 40, 50),                         \"SoilType\" = c(1, 1, 1)) new_data <- rbind(new_data, fake_data)   # Evaluate models with independent data res_ind <- independent_eval(fitted_models = fitted_model_maxnet,                             new_data = new_data)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/initial_cleaning.html","id":null,"dir":"Reference","previous_headings":"","what":"Initial occurrence data cleaning steps — initial_cleaning","title":"Initial occurrence data cleaning steps — initial_cleaning","text":"Simple occurrence data cleaning procedures.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/initial_cleaning.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Initial occurrence data cleaning steps — initial_cleaning","text":"","code":"initial_cleaning(data, species, x, y,                  other_columns = NULL, keep_all_columns = TRUE,                  sort_columns = TRUE, remove_na = TRUE, remove_empty = TRUE,                  remove_duplicates = TRUE, by_decimal_precision = FALSE,                  decimal_precision = 0, longitude_precision = NULL,                  latitude_precision = NULL)  sort_columns(data, species, x, y, keep_all_columns = FALSE)  remove_missing(data, columns = NULL, remove_na = TRUE,                remove_empty = TRUE, keep_all_columns = TRUE)  remove_duplicates(data, columns = NULL, keep_all_columns = TRUE)  remove_corrdinates_00(data, x, y)  filter_decimal_precision(data, x,                          y, decimal_precision = 0,                          longitude_precision = NULL,                          latitude_precision = NULL)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/initial_cleaning.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Initial occurrence data cleaning steps — initial_cleaning","text":"data data.frame occurrence records. species (character) name column data containing species name. x (character) name column data containing longitude values. y (character) name column data containing latitude values. other_columns (character) vector column name(s) data considered performing cleaning steps, default = NULL. keep_all_columns (logical) whether keep columns data. Default = TRUE. sort_columns (logical) whether sort species, longitude, latitude columns data. Default = TRUE. remove_na (logical) whether remove NA values columns considered. Default = TRUE. remove_empty (logical) whether remove empty (missing) values columns considered. Default = TRUE. remove_duplicates (logical) whether remove duplicates columns considered. Default = TRUE. by_decimal_precision (logical) whether remove certain records coordinate precision lower following three parameters. Default = FALSE decimal_precision (numeric) decimal precision threshold coordinates. Default = 0. Ignored following two parameters defined. longitude_precision (numeric) decimal precision threshold longitude. Default = NULL. latitude_precision (numeric) decimal precision threshold latitude. Default = NULL. columns (character) vector additional column name(s) data considered removing missing duplicate records, default = NULL.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/initial_cleaning.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Initial occurrence data cleaning steps — initial_cleaning","text":"data.frame resulting occurrence records.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/initial_cleaning.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Initial occurrence data cleaning steps — initial_cleaning","text":"Function initial_cleaning helps perform simple steps data cleaning.","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/kuenm2-package.html","id":null,"dir":"Reference","previous_headings":"","what":"kuenm2: Detailed Development of Ecological Niche Models — kuenm2-package","title":"kuenm2: Detailed Development of Ecological Niche Models — kuenm2-package","text":"kuenm2 new set tools help development detailed ecological niche models using multiple algorithms. Pre-modeling analyses explorations can done prepare data. Model calibration (model selection) can done creating testing several candidate models. Handy options producing final models transfers included. tools assess extrapolation risks variability model transfers also available.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/kuenm2-package.html","id":"functions-by-stage-in-the-enm-process","dir":"Reference","previous_headings":"","what":"Functions by stage in the ENM process","title":"kuenm2: Detailed Development of Ecological Niche Models — kuenm2-package","text":"Data preparation: initial_cleaning(), advanced_cleaning(), prepare_data(), prepare_user_data() Model calibration: calibration(), select_models() Model exploration: fit_selected(), variable_importance(), plot_importance(), response_curve(), bivariate_response() Model projection: predict_selected(), project_selected() Post-modeling: projection_variability(), projection_changes(), projection_mop()","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/kuenm2-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"kuenm2: Detailed Development of Ecological Niche Models — kuenm2-package","text":"Maintainer: Weverton C. F. Trindade wevertonf1993@gmail.com (ORCID) Authors: Luis F. Arias-Giraldo lfarias.giraldo@gmail.com (ORCID) Luis Osorio-Olvera luismurao@gmail.com (ORCID) . Townsend Peterson town@ku.edu (ORCID) Marlon E. Cobos manubio13@gmail.com (ORCID)","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/m.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatVector Representing Calibration Area for Myrcia hatschbachii — m","title":"SpatVector Representing Calibration Area for Myrcia hatschbachii — m","text":"spatial vector defining calibration area used extract background points fitting models Myrcia hatschbachii. area generated creating minimum convex polygon around presence records (occ_data), applying 300 km buffer.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/m.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatVector Representing Calibration Area for Myrcia hatschbachii — m","text":"Spatvector object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/m.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatVector Representing Calibration Area for Myrcia hatschbachii — m","text":"return value. Used function vect bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/m.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatVector Representing Calibration Area for Myrcia hatschbachii — m","text":"","code":"m <- terra::vect(system.file(\"extdata\",                              \"m.gpkg\",                               package = \"kuenm2\")) terra::plot(m)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/new_occ.html","id":null,"dir":"Reference","previous_headings":"","what":"Independent Species Occurrence — new_occ","title":"Independent Species Occurrence — new_occ","text":"data.frame containing coordinates 82 occurrences Myrcia hatschbachii (tree endemic southern Brazil). valid occurrences sourced NeotropicTree (Oliveira-Filho, 2017) used independent data test models fitted occ_data.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/new_occ.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Independent Species Occurrence — new_occ","text":"","code":"data(\"new_occ\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/new_occ.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Independent Species Occurrence — new_occ","text":"data.frame following columns: species species name. x Longitude. y Latitude.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/new_occ.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Independent Species Occurrence — new_occ","text":"Oliveira_Filho, .T. 2017. NeoTropTree, Flora arbórea da Região Neotropical: Um banco de dados envolvendo biogeografia, diversidade e conservação. Universidade Federal de Minas Gerais. (http://www.neotroptree,info).","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/occ_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Species Occurrence — occ_data","title":"Species Occurrence — occ_data","text":"data.frame containing coordinates 51 valid occurrences Myrcia hatschbachii (tree endemic southern Brazil). valid occurrences sourced Trindade & Marques (2024) contains records retrieved GBIF SpeciesLink.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/occ_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Species Occurrence — occ_data","text":"","code":"data(\"occ_data\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/occ_data.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Species Occurrence — occ_data","text":"data.frame following columns: species species name. x Longitude. y Latitude.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/occ_data.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Species Occurrence — occ_data","text":"Trindade, W.C.F., Marques, M.C.M., 2023. Invisible Species: Big Data Unveil Coverage Gaps Atlantic Forest Hotspot. Diversity Distributions 30, e13931. https://doi.org/10.1111/ddi.13931","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/occ_data_noclean.html","id":null,"dir":"Reference","previous_headings":"","what":"Species Occurrence with Erroneous Records — occ_data_noclean","title":"Species Occurrence with Erroneous Records — occ_data_noclean","text":"data.frame containing coordinates 51 valid occurrences Myrcia hatschbachii (tree endemic southern Brazil), along set erroneous records used demonstrate data cleaning procedures. valid occurrences sourced Trindade & Marques (2024).","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/occ_data_noclean.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Species Occurrence with Erroneous Records — occ_data_noclean","text":"","code":"data(\"occ_data_noclean\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/occ_data_noclean.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Species Occurrence with Erroneous Records — occ_data_noclean","text":"data.frame following columns: species species name. x Longitude. y Latitude.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/occ_data_noclean.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Species Occurrence with Erroneous Records — occ_data_noclean","text":"Trindade, W.C.F., Marques, M.C.M., 2023. Invisible Species: Big Data Unveil Coverage Gaps Atlantic Forest Hotspot. Diversity Distributions 30, e13931. https://doi.org/10.1111/ddi.13931","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/organize_for_projection.html","id":null,"dir":"Reference","previous_headings":"","what":"Organize and structure variables for past and future projections — organize_for_projection","title":"Organize and structure variables for past and future projections — organize_for_projection","text":"function helpts organize climate variable files past future scenarios folders categorized time period (\"Past\" \"Future\"), specific period (e.g., \"LGM\" \"2081–2100\"), emission scenario (e.g., \"ssp585\"), GCMs. structure simplifies preparation climate data ensures compatibility prepare_projection() function, making variables properly organized modeling projections. See Details information.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/organize_for_projection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Organize and structure variables for past and future projections — organize_for_projection","text":"","code":"organize_for_projection(output_dir, models = NULL,                                variable_names = NULL, present_file = NULL,                                past_files = NULL, past_period = NULL,                                past_gcm = NULL, future_files = NULL,                                future_period = NULL, future_pscen = NULL,                                future_gcm = NULL, fixed_variables = NULL,                                check_extent = TRUE,                                resample_to_present = TRUE, mask = NULL,                                overwrite = FALSE)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/organize_for_projection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Organize and structure variables for past and future projections — organize_for_projection","text":"output_dir (character) path folder organized data saved. models object class fitted_models returned fit_selected() function. Default NULL. variable_names (character) names variables used fit model PCA prepare_data() function. applicable 'models' argument provided. Default NULL. present_file (character) full paths variables present scenario. Default NULL. past_files (character) full paths variables past scenario(s). Default NULL. past_period (character) names subfolders within 'past_files', representing specific time periods (e.g., 'LGM' 'MID'). applicable 'past_files' provided. Default NULL. past_gcm (character) names subfolders within 'past_files', representing specific General Circulation Models (GCMs). applicable 'past_files' provided. Default NULL. future_files (character) full paths variables future scenario(s). Default NULL. future_period (character) names subfolders within 'future_files', representing specific time periods (e.g., '2041-2060' '2081-2100'). applicable 'future_files' provided. Default NULL. future_pscen (character) names subfolders within 'future_files', representing specific emission scenarios (e.g., 'ssp126' 'ssp585'). applicable 'future_files' provided. Default NULL. future_gcm (character) names subfolders within 'future_files', representing specific General Circulation Models (GCMs). applicable 'future_files' provided. Default NULL. fixed_variables (SpatRaster) optional static variables (.e., soil type) used model, remain unchanged past future scenarios. variable included scenario. Default NULL. check_extent (logical) whether ensure 'fixed_variables' spatial extent bioclimatic variables. Applicable 'fixed_variables' provided. Default TRUE. resample_to_present (logical) whether resample past future variables match extent present variables. used 'present_file' provided. Default TRUE. mask (SpatRaster, SpatVector, SpatExtent) spatial object used mask variables (optional). Default NULL. overwrite whether overwrite existing files output directory. Default FALSE.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/organize_for_projection.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Organize and structure variables for past and future projections — organize_for_projection","text":"message indicating variables successfully organized 'output_dir' directory.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/organize_for_projection.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Organize and structure variables for past and future projections — organize_for_projection","text":"listed input rasters must stored .tif files, one file per scenario. Filenames include identifiable patterns time period, GCM, (future scenarios) emission scenario (SSP). example: file representing \"Past\" conditions \"LGM\" period using \"MIROC6\" GCM named: \"Past_LGM_MIROC6.tif\" file representing \"Future\" conditions period \"2081–2100\" emission scenario \"ssp585\" GCM \"ACCESS-CM2\" named: \"Future_2081-2100_ssp585_ACCESS-CM2.tif\" scenario files must contain variable names (e.g., bio1, bio2, etc.) units used model calibration present-day data. Tip: listing files, use list.files(path, full.names = TRUE) obtain full file paths required function.","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/organize_for_projection.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Organize and structure variables for past and future projections — organize_for_projection","text":"","code":"# Set the input directory containing the climate variables. # In this example, we use present and LGM variables from CHELSA # located in the \"inst/extdata\" folder of the package. present_lgm_dir <- system.file(\"extdata\", package = \"kuenm2\")  # Define an output directory (here, using a temporary folder) # Replace with your own working directory if needed. out_dir <- file.path(tempdir(), \"Projection_variables\")  # List files for present-day conditions present_list <- list.files(path = present_lgm_dir,                            pattern = \"Current_CHELSA\", # Select only CHELSA present-day files                            full.names = TRUE)  # List files for LGM conditions lgm_list <- list.files(path = present_lgm_dir,                        pattern = \"LGM\", # Select only LGM files                        full.names = TRUE)  # Organize variables for projection organize_for_projection(output_dir = out_dir,                         variable_names = c(\"bio1\", \"bio7\", \"bio12\", \"bio15\"),                         present_file = present_list,                         past_files = lgm_list,                         past_period = \"LGM\",                         past_gcm = c(\"CCSM4\", \"CNRM-CM5\", \"FGOALS-g2\",                                      \"IPSL-CM5A-LR\", \"MIROC-ESM\", \"MPI-ESM-P\",                                      \"MRI-CGCM3\"),                         resample_to_present = TRUE,                         overwrite = TRUE) #>  #> Variables successfully organized in directory: #> /tmp/RtmpAuudrv/Projection_variables"},{"path":"https://marlonecobos.github.io/kuenm2/reference/organize_future_worldclim.html","id":null,"dir":"Reference","previous_headings":"","what":"Organize and structure future climate variables from WorldClim — organize_future_worldclim","title":"Organize and structure future climate variables from WorldClim — organize_future_worldclim","text":"function imports future climate variables downloaded WorldClim, renames files, organizes folders categorized year, emission scenario (SSP) General Circulation Model (GCM). simplifies preparation climate data, making compatible prepare_projection() function, ensuring required variables properly structured modeling projections.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/organize_future_worldclim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Organize and structure future climate variables from WorldClim — organize_future_worldclim","text":"","code":"organize_future_worldclim(input_dir, output_dir, name_format = \"bio_\",                           variables = NULL, fixed_variables = NULL,                           check_extent = TRUE, mask = NULL,                           progress_bar = TRUE, overwrite = FALSE)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/organize_future_worldclim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Organize and structure future climate variables from WorldClim — organize_future_worldclim","text":"input_dir (character) path folder containing future climate variables downloaded WorldClim. output_dir (character) path folder organized data saved. name_format (character) format renaming variable. Options \"bio_\", \"Bio_\", \"bio_0\", \"Bio_0\". See details information. Default \"bio_\". variables (character) names variables retain. Default NULL, meaning variables kept. fixed_variables (SpatRaster) optional static variables (.e., soil type) used model, remain unchanged future scenarios. variable included future scenario. Default NULL. check_extent (logical) whether ensure fixed_variables spatial extent bioclimatic variables. Applicable fixed_variables provided. Default TRUE. mask (SpatRaster, SpatVector, SpatExtent) spatial object used mask variables (optional). Default NULL. progress_bar (logical) whether display progress bar processing. Default TRUE. overwrite whether overwrite existing files output directory. Default FALSE.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/organize_future_worldclim.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Organize and structure future climate variables from WorldClim — organize_future_worldclim","text":"list paths folders organized climate data saved.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/organize_future_worldclim.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Organize and structure future climate variables from WorldClim — organize_future_worldclim","text":"raw variables downloaded WorldClim named \"Bio01\", \"Bio02\", \"Bio03\", \"Bio10\", etc. name_format parameter controls variables renamed: \"bio_\": variables renamed bio_1, bio_2, bio_3, bio_10, etc. \"bio_0\": variables renamed bio_01, bio_02, bio_03, bio_10, etc \"Bio_\": variables renamed Bio_1, Bio_2, Bio_3, Bio_10, etc. \"Bio_0\": variables renamed Bio_01, Bio_02, Bio_03, Bio_10, etc.","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/organize_future_worldclim.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Organize and structure future climate variables from WorldClim — organize_future_worldclim","text":"","code":"# Import the current variables used to fit the model. # In this case, SoilType will be treated as a static variable (constant # across future scenarios). var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  # Set the input directory containing the raw future climate variables. # For this example, the data is located in the \"inst/extdata\" folder. in_dir <- system.file(\"extdata\", package = \"kuenm2\")  # Create a \"Future_raw\" folder in a temporary directory and copy the raw # variables there. out_dir <- file.path(tempdir(), \"Future_raw\")  # Organize and rename the future climate data, structuring it by year and GCM. # The 'SoilType' variable will be appended as a static variable in each scenario. # The files will be renamed following the \"bio_\" format organize_future_worldclim(input_dir = in_dir, output_dir = out_dir,                           name_format = \"bio_\",                           fixed_variables = var$SoilType) #>    |                                                                               |                                                                      |   0%   |                                                                               |=========                                                             |  12%   |                                                                               |==================                                                    |  25%   |                                                                               |==========================                                            |  38%   |                                                                               |===================================                                   |  50%   |                                                                               |============================================                          |  62%   |                                                                               |====================================================                  |  75%   |                                                                               |=============================================================         |  88%   |                                                                               |======================================================================| 100% #>  #> Variables successfully organized in directory: #> /tmp/RtmpAuudrv/Future_raw  # Check files organized dir(out_dir, recursive = TRUE) #> [1] \"2041-2060/ssp126/ACCESS-CM2/Variables.tif\" #> [2] \"2041-2060/ssp126/MIROC6/Variables.tif\"     #> [3] \"2041-2060/ssp585/ACCESS-CM2/Variables.tif\" #> [4] \"2041-2060/ssp585/MIROC6/Variables.tif\"     #> [5] \"2081-2100/ssp126/ACCESS-CM2/Variables.tif\" #> [6] \"2081-2100/ssp126/MIROC6/Variables.tif\"     #> [7] \"2081-2100/ssp585/ACCESS-CM2/Variables.tif\" #> [8] \"2081-2100/ssp585/MIROC6/Variables.tif\""},{"path":"https://marlonecobos.github.io/kuenm2/reference/partial_roc.html","id":null,"dir":"Reference","previous_headings":"","what":"Partial ROC calculation for multiple candidate models — partial_roc","title":"Partial ROC calculation for multiple candidate models — partial_roc","text":"Computes partial ROC tests multiple candidate models.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/partial_roc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Partial ROC calculation for multiple candidate models — partial_roc","text":"","code":"partial_roc(formula_grid, data, omission_rate = 10,             addsamplestobackground = TRUE, weights = NULL,             algorithm = \"maxnet\", parallel = FALSE, ncores = NULL,             progress_bar = TRUE)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/partial_roc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Partial ROC calculation for multiple candidate models — partial_roc","text":"formula_grid data.frame grid formulas defining candidate models test. data object class prepared_data returned prepare_data() function object class calibration_results returned calibration() function. contains calibration data k-folds. omission_rate (numeric) values 0 100 representing percentage potential error due source uncertainty. value used calculate omission rate. Default 10. See details. addsamplestobackground (logical) whether add background presence sample already . Default TRUE. weights (numeric) numeric vector specifying weights occurrence records. Default NULL. algorithm (character) type algorithm, either \"glm\" \"maxnet\". Default \"maxnet\". parallel (logical) whether fit candidate models parallel. Default FALSE. ncores (numeric) number cores use parallel processing. Default NULL uses available cores - 1. applicable parallel = TRUE. progress_bar (logical) whether display progress bar processing. Default TRUE.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/partial_roc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Partial ROC calculation for multiple candidate models — partial_roc","text":"data frame summary statistics AUC ratios significance calculated replicates candidate model. Specifically, includes mean standard deviation metrics model.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/partial_roc.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Partial ROC calculation for multiple candidate models — partial_roc","text":"Partial ROC calculated following Peterson et al. (2008) doi:10.1016/j.ecolmodel.2007.11.008.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/partial_roc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Partial ROC calculation for multiple candidate models — partial_roc","text":"","code":"# Import prepared data to get model formulas data(sp_swd, package = \"kuenm2\")  # Calculate proc for the first 5 candidate models res_proc <- partial_roc(formula_grid = sp_swd$formula_grid[1:2,],                         data = sp_swd, omission_rate = 10,                         algorithm = \"maxnet\") #>    |                                                                               |                                                                      |   0%   |                                                                               |===================================                                   |  50%   |                                                                               |======================================================================| 100%"},{"path":"https://marlonecobos.github.io/kuenm2/reference/perform_pca.html","id":null,"dir":"Reference","previous_headings":"","what":"Principal Component Analysis for raster layers — perform_pca","title":"Principal Component Analysis for raster layers — perform_pca","text":"function performs principal component analysis (PCA) set raster variables.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/perform_pca.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Principal Component Analysis for raster layers — perform_pca","text":"","code":"perform_pca(raster_variables, exclude_from_pca = NULL, project = FALSE,             projection_data = NULL, out_dir = NULL, overwrite = FALSE,             progress_bar = FALSE, center = TRUE, scale = FALSE,             variance_explained = 95, min_explained = 5)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/perform_pca.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Principal Component Analysis for raster layers — perform_pca","text":"raster_variables (SpatRaster) set predictor variables function summarize set orthogonal, uncorrelated components based PCA. exclude_from_pca (character) variable names within raster_variables included PCA transformation. Instead, variables added directly final set output variables without modified. default NULL, meaning variables used unless specified otherwise. project (logical) whether function project new data different scenarios (e.g. future variables) onto PCA coordinates generated initial analysis. TRUE, argument projection_data needs defined. Default FALSE. projection_data object class prepared_projection returned prepare_projection() function. file contains paths raster files representing scenario. applicable project = TRUE. Default NULL. out_dir (character) path root directory saving raster files projection. Default = NULL. overwrite (logical) whether overwrite SpatRaster already exists projecting. applicable write_files set TRUE. Default FALSE. progress_bar (logical) whether display progress bar processing projections. applicable project = TRUE. Default FALSE center (logical) whether variables zero-centered. Default TRUE. scale (logical) whether variables scaled unit variance analysis takes place. Default FALSE. variance_explained (numeric) cumulative percentage total variance must explained selected principal components. Default 95. min_explained (numeric) minimum percentage total variance principal component must explain retained. Default 5.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/perform_pca.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Principal Component Analysis for raster layers — perform_pca","text":"list containing following elements: env: SpatRaster object contains orthogonal components derived PCA. PCs correspond variables used perform analysis. pca: object class prcomp, containing details PCA analysis. See prcomp(). variance_explained_cum_sum: cumulative percentage total variance explained selected principal components. value indicates much data’s original variability captured PCA transformation. projection_directory: root directory projection files saved. NULL project set TRUE. directory contains projected raster files scenario.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/perform_pca.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Principal Component Analysis for raster layers — perform_pca","text":"","code":"# PCA with current variables # Import raster layers var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  # PCA pca_var <- perform_pca(raster_variables = var, exclude_from_pca = \"SoilType\",                        center = TRUE, scale = TRUE)  pca_var #> $env #> class       : SpatRaster  #> size        : 52, 40, 5  (nrow, ncol, nlyr) #> resolution  : 0.1666667, 0.1666667  (x, y) #> extent      : -53.5, -46.83333, -30.83333, -22.16667  (xmin, xmax, ymin, ymax) #> coord. ref. : lon/lat WGS 84 (EPSG:4326)  #> sources     : memory  (4 layers)  #>               Current_variables.tif   #> varnames    : Current_variables  #>               Current_variables  #> names       :       PC1,       PC2,       PC3,       PC4, SoilType  #> min values  : -3.621362, -2.041276, -3.923471, -1.730859,        1  #> max values  :  2.929786,  3.029667,  1.752452,  1.601162,       23  #>  #> $pca #> Standard deviations (1, .., p=4): #> [1] 1.4574175 0.9290330 0.8020786 0.6078666 #>  #> Rotation (n x k) = (4 x 4): #>               PC1        PC2         PC3         PC4 #> bio_1  -0.5327531 -0.1500983 -0.66580466  0.50034864 #> bio_7   0.3338164 -0.9359166 -0.09775690 -0.05541088 #> bio_12  0.5032916  0.2775767 -0.73525490 -0.35923383 #> bio_15 -0.5928223 -0.1564666 -0.08091963 -0.78583200 #>  #> $variance_explained_cumsum #>       PC1       PC2       PC3       PC4  #>  53.10165  74.67920  90.76246 100.00000  #>  #> $projection_directory #> NULL #>   # Project PCA for new scenarios (future) # First, organize and prepare future variables # Set the input directory containing the raw future climate variables # For this example, the data is located in the \"inst/extdata\" folder. in_dir <- system.file(\"extdata\", package = \"kuenm2\")  # Create a \"Future_raw\" folder in a temporary directory and copy the variables. out_dir_future <- file.path(tempdir(), \"Future_raw1\")  # Organize and rename the future climate data, structuring it by year and GCM. # The 'SoilType' variable will be appended as a static variable in each scenario. # The files will be renamed following the \"bio_\" format organize_future_worldclim(input_dir = in_dir, output_dir = out_dir_future,                           name_format = \"bio_\", fixed_variables = var$SoilType) #>    |                                                                               |                                                                      |   0%   |                                                                               |=========                                                             |  12%   |                                                                               |==================                                                    |  25%   |                                                                               |==========================                                            |  38%   |                                                                               |===================================                                   |  50%   |                                                                               |============================================                          |  62%   |                                                                               |====================================================                  |  75%   |                                                                               |=============================================================         |  88%   |                                                                               |======================================================================| 100% #>  #> Variables successfully organized in directory: #> /tmp/RtmpAuudrv/Future_raw1  # Prepare projections pr <- prepare_projection(variable_names = c(\"bio_1\", \"bio_7\", \"bio_12\",                                             \"bio_15\", \"SoilType\"),                          future_dir = out_dir_future,                          future_period = c(\"2041-2060\", \"2081-2100\"),                          future_pscen = c(\"ssp126\", \"ssp585\"),                          future_gcm = c(\"ACCESS-CM2\", \"MIROC6\"),                          raster_pattern = \".tif*\")  # Create folder to save projection results out_dir <- file.path(tempdir(), \"PCA_projections\") dir.create(out_dir, recursive = TRUE)  # Perform and project PCA for new scenarios (future) proj_pca <- perform_pca(raster_variables = var, exclude_from_pca = \"SoilType\",                         project = TRUE, projection_data = pr,                         out_dir = out_dir, center = TRUE, scale = TRUE)  proj_pca$projection_directory  # Directory with projected PCA-variables #> [1] \"/tmp/RtmpAuudrv/PCA_projections\""},{"path":"https://marlonecobos.github.io/kuenm2/reference/plot_explore_calibration.html","id":null,"dir":"Reference","previous_headings":"","what":"Histograms to visualize data from explore_calibration objects — plot_explore_calibration","title":"Histograms to visualize data from explore_calibration objects — plot_explore_calibration","text":"Plots histograms visualize data explore_calibration object generated explore_calibration_hist function.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/plot_explore_calibration.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Histograms to visualize data from explore_calibration objects — plot_explore_calibration","text":"","code":"plot_explore_calibration(explore_calibration, color_m = \"grey\",                          color_background = \"#56B4E9\",                          color_presence = \"#009E73\", alpha = 0.4,                          lines = FALSE, which_lines = c(\"cl\", \"mean\"),                          lty_range = 1, lty_cl = 2, lty_mean = 3,                          lwd_range = 3, lwd_cl = 2, lwd_mean = 2,                          xlab = NULL, ylab = NULL, mfrow = NULL)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/plot_explore_calibration.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Histograms to visualize data from explore_calibration objects — plot_explore_calibration","text":"explore_calibration object class explore_calibration generated explore_calibration_hist function. color_m (character) color used fill histogram bars entire area (M). Default \"grey\". color_background (character) color used fill histogram bars background data. Default \"#56B4E9\". color_presence (character) color used fill histogram bars presence data. Default \"#009E73\". alpha (numeric) opacity factor fill bars, typically range 0-1. Default 0.4. lines (logical) whether add vertical lines plot representing range, confidence interval, mean variables. Default = FALSE. which_lines (character) vector indicating lines plot. Available options \"range\", \"cl\" (confidence interval), \"mean\". Default c(\"range\", \"cl\", \"mean\"). lty_range (numeric) line type plotting ranges variables. Default 1, meaning solid line. lty_cl (numeric) line type plotting confidence interval variables. Default 2, meaning dashed line. lty_mean (numeric) line type plotting mean variables. Default 3, meaning dotted line. lwd_range (numeric) line width line representing range. Default 3. lwd_cl (numeric) line width line representing confidence interval. Default 2. lwd_mean (numeric) line width line representing mean. Default 2. xlab (character) vector names labeling x-axis. must length number variables. Default NULL, meaning labels extracted explore_calibration object. ylab (character) label y-axis. Default NULL, meaning y-axis labeled \"Frequency\". mfrow (numeric) vector specifying number rows columns plot layout, e.g., c(rows, columns). Default NULL, meaning grid arranged automatically based number plots.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/plot_explore_calibration.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Histograms to visualize data from explore_calibration objects — plot_explore_calibration","text":"","code":"# Import raster layers var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  # Import occurrences data(sp_swd, package = \"kuenm2\")  # Explore calibration data calib_hist <- explore_calibration_hist(data = sp_swd,                                        raster_variables = var,                                        include_m = TRUE)  # Plot histograms plot_explore_calibration(explore_calibration = calib_hist, mfrow = c(2, 3))"},{"path":"https://marlonecobos.github.io/kuenm2/reference/plot_importance.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary plot for variable importance in models — plot_importance","title":"Summary plot for variable importance in models — plot_importance","text":"See details plot_importance","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/plot_importance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary plot for variable importance in models — plot_importance","text":"","code":"plot_importance(x, xlab = NULL, ylab = \"Relative contribution\",                 main = \"Variable importance\", extra_info = TRUE, ...)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/plot_importance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary plot for variable importance in models — plot_importance","text":"x data.frame output variable_importance(). xlab (character) label x axis. ylab (character) label y axis. main (character) main title plot. extra_info (logical) results one model, adds information number models using predictor mean contribution found. ... additional arguments passed barplot boxplot. Value barplot boxplot depending number models considered.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/predict.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict method for glmnet_mx (maxnet) models — predict","title":"Predict method for glmnet_mx (maxnet) models — predict","text":"Predict method glmnet_mx (maxnet) models","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/predict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict method for glmnet_mx (maxnet) models — predict","text":"","code":"predict.glmnet_mx(object, newdata, clamp = FALSE,                   type = c(\"link\", \"exponential\", \"cloglog\", \"logistic\",                   \"cumulative\"))"},{"path":"https://marlonecobos.github.io/kuenm2/reference/predict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict method for glmnet_mx (maxnet) models — predict","text":"object glmnet_mx object. newdata data predict . clamp (logical) whether clamp predictions. Default = FALSE. type (character) type prediction performed. Options : \"link\", \"exponential\", \"cloglog\", \"logistic\", cumulative. Defaults \"link\" defined.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/predict.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict method for glmnet_mx (maxnet) models — predict","text":"glmnet_mx (maxnet) prediction.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/predict_selected.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict selected models for a single scenario — predict_selected","title":"Predict selected models for a single scenario — predict_selected","text":"function predicts selected models single set new data using either maxnet glm provides options save output compute consensus results (mean, median, etc.) across replicates models.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/predict_selected.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict selected models for a single scenario — predict_selected","text":"","code":"predict_selected(models, raster_variables, mask = NULL, write_files = FALSE,                  write_replicates = FALSE, out_dir = NULL,                  consensus_per_model = TRUE, consensus_general = TRUE,                  consensus = c(\"median\", \"range\", \"mean\", \"stdev\"),                  extrapolation_type = \"E\", var_to_clamp = NULL,                  type = \"cloglog\", overwrite = FALSE, progress_bar = TRUE)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/predict_selected.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict selected models for a single scenario — predict_selected","text":"models object class fitted_models returned fit_selected() function. raster_variables SpatRaster data.frame predictor variables. names variables must match used calibrate models used run PCA do_pca = TRUE prepare_data() function. mask (SpatRaster, SpatVector, SpatExtent) spatial object used mask variables predict. Default NULL. write_files (logical) whether save predictions (SpatRasters data.frame) disk. Default FALSE. write_replicates (logical) whether save predictions replicate disk. applicable write_files TRUE. Default FALSE. out_dir (character) directory path predictions saved. relevant write_files = TRUE. consensus_per_model (logical) whether compute consensus (mean, median, etc.) model across replicates. Default TRUE. consensus_general (logical) whether compute general consensus across models. Default TRUE. consensus (character) vector specifying types consensus calculate across replicates models. Available options \"median\", \"range\", \"mean\", \"stdev\" (standard deviation). Default c(\"median\", \"range\", \"mean\", \"stdev\"). extrapolation_type (character) extrapolation type model. Models can transferred three options: free extrapolation ('E'), extrapolation clamping ('EC'), extrapolation ('NE'). Default = 'E'. See details. var_to_clamp (character) vector specifying variables clamp extrapolate. applicable extrapolation_type \"EC\" \"NE\". Default NULL, meaning variables clamped extrapolated. type (character) format prediction values. maxnet models, valid options \"raw\", \"cumulative\", \"logistic\", \"cloglog\". glm models, valid options \"cloglog\", \"response\", \"raw\", \"cumulative\" \"link\". Default \"cloglog. overwrite (logical) whether overwrite SpatRasters already exist. applicable write_files = TRUE. Default FALSE. progress_bar (logical) whether display progress bar processing. Default TRUE.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/predict_selected.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict selected models for a single scenario — predict_selected","text":"list containing SpatRaster data.frames predictions replicate, long consensus results model overall general consensus.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/predict_selected.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Predict selected models for a single scenario — predict_selected","text":"predicting areas variables beyond lower upper limits calibration data, users can choose free extrapolate predictions (extrapolation_type = \"E\"), extrapolate clamping (extrapolation_type = \"EC\"), extrapolate (extrapolation_type = \"NE\"). clamping, variables set minimum maximum values established maximum minimum values within calibration data. extrapolation approach, cell least one variable listed var_to_clamp falling outside calibration range assigned suitability value 0.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/predict_selected.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict selected models for a single scenario — predict_selected","text":"","code":"# Import variables to predict on var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  # Example with maxnet # Import example of fitted_models (output of fit_selected()) data(\"fitted_model_maxnet\", package = \"kuenm2\")  # Predict to single scenario p <- predict_selected(models = fitted_model_maxnet, raster_variables = var) #>    |                                                                               |                                                                      |   0%   |                                                                               |===================================                                   |  50%   |                                                                               |======================================================================| 100%  # Example with GLMs # Import example of fitted_models (output of fit_selected()) data(\"fitted_model_glm\", package = \"kuenm2\")  # Predict to single scenario p_glm <- predict_selected(models = fitted_model_glm, raster_variables = var) #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100%  # Plot predictions terra::plot(c(p$General_consensus$median, p_glm$General_consensus$median),             col = rev(terrain.colors(240)), main = c(\"MAXNET\", \"GLM\"),             zlim = c(0, 1))"},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare data for model calibration — prepare_data","title":"Prepare data for model calibration — prepare_data","text":"function prepares data model calibration, including optional PCA, background point generation, k-fold partitioning, creation grid parameter combinations, including regularization multiplier values, feature classes, sets environmental variables.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare data for model calibration — prepare_data","text":"","code":"prepare_data(algorithm, occ, x, y, raster_variables, species = NULL,              mask = NULL, n_background = 1000, features = c(\"lq\", \"lqp\"),              r_multiplier = c(0.1, 0.5, 1, 2, 3), partition_method,              n_replicates = 4, train_proportion = 0.7,              categorical_variables = NULL,              do_pca = FALSE, center = TRUE, scale = TRUE,              exclude_from_pca = NULL, variance_explained = 95,              min_explained = 5, min_number = 2, min_continuous = NULL,              bias_file = NULL, bias_effect = NULL, weights = NULL,              include_xy = TRUE, write_pca = FALSE, pca_directory = NULL,              write_file = FALSE, file_name = NULL, seed = 1)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare data for model calibration — prepare_data","text":"algorithm (character) modeling algorithm, either \"glm\" \"maxnet\". occ (data frame) data.frame containing coordinates (longitude latitude) occurrence records. x (character) string specifying name column occ contains longitude values. y (character) string specifying name column occ contains latitude values. raster_variables (SpatRaster) predictor variables used calibrate models. species (character) string specifying species name (optional). Default NULL. mask spatial object used mask variables area model calibrated. Mask must one following classes: SpatRaster, SpatVector, SpatExtent. Default NULL. n_background (numeric) number points represent background model. Default 1000. features (character) vector feature classes. Default c(\"q\", \"lq\", \"lp\", \"qp\", \"lqp\"). r_multiplier (numeric) vector regularization parameters maxnet. Default c(0.1, 1, 2, 3, 5). partition_method (character) method used data partitioning. Available options \"kfolds\", \"subsample\", \"bootstrap\". See Details information. n_replicates (numeric) number replicates generate. partition_method \"subsample\" \"bootstrap\", defines number partitions. \"kfolds\", specifies number folds. Default 4. train_proportion (numeric) proportion occurrence background points used model training replicate. applicable partition_method \"subsample\" \"bootstrap\". Default 0.7 (.e., 70% training 30% testing). categorical_variables (character) names variables categorical. Default NULL. do_pca (logical) whether perform principal component analysis (PCA) set variables. Default FALSE. center (logical) whether variables zero-centered. Default TRUE. scale (logical) whether variables scaled unit variance analysis takes place. Default FALSE. exclude_from_pca (character) variable names within raster_variables included PCA transformation. Instead, variables added directly final set output variables without modified. default NULL, meaning variables used unless specified otherwise. variance_explained (numeric) cumulative percentage total variance must explained selected principal components. Default 95. min_explained (numeric) minimum percentage total variance principal component must explain retained. Default 5. min_number (numeric) minimum number variables included model formulas generated. min_continuous (numeric) minimum number continuous variables required combination. Default NULL. bias_file (SpatRaster) raster containing bias values (probability weights) influence selection background points. must extent, resolution, number cells raster variables, unless mask provided. Default NULL. bias_effect (character) string specifying values bias_file interpreted. Options \"direct\" \"inverse\". \"direct\", higher values bias file increase likelihood selecting background points. \"inverse\", higher values decrease likelihood. Default = NULL. Must defined bias_file provided. weights (numeric) numeric vector specifying weights occurrence records. Default NULL. include_xy (logical) whether include coordinates (longitude latitude) results preparing data. Columns containing coordinates renamed \"x\" \"y\". Default TRUE. write_pca (logical) whether save PCA-derived raster layers (principal components) disk. Default FALSE. pca_directory (character) path name folder PC raster layers saved. applicable write_pca = TRUE. Default NULL. write_file (logical) whether write resulting prepared_data list local directory. Default FALSE. file_name (character) path name folder resulting list saved. applicable write_file = TRUE. Default NULL. seed (numeric) integer value specify initial seed split data extract background. Default 1.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare data for model calibration — prepare_data","text":"object class prepared_data containing elements run model calibration routine. elements include: species, calibration data, grid model parameters, indices test data cross validation, xy coordinates, names continuous categorical variables, weights, results PCA, modeling algorithm.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Prepare data for model calibration — prepare_data","text":"available data partitioning methods : \"kfolds\": Splits dataset K subsets (folds) approximately equal size. replicate, one fold used test set, remaining folds combined form training set. \"bootstrap\": Creates training dataset sampling observations original dataset replacement (.e., observation can selected multiple times). test set consists observations selected specific replicate. \"subsample\": Similar bootstrap, training set created sampling without replacement (.e., observation selected ). test set includes observations selected training.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare data for model calibration — prepare_data","text":"","code":"# Import occurrences data(occ_data, package = \"kuenm2\")  # Import raster layers var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  # Import a bias file bias <- terra::rast(system.file(\"extdata\", \"bias_file.tif\",                                 package = \"kuenm2\"))  # Prepare data for maxnet model sp_swd <- prepare_data(algorithm = \"maxnet\", occ = occ_data,                        x = \"x\", y = \"y\",                        raster_variables = var,                        species = occ_data[1, 1],                        categorical_variables = \"SoilType\",                        n_background = 500, bias_file = bias,                        bias_effect = \"direct\",                        features = c(\"l\", \"q\", \"p\", \"lq\", \"lqp\"),                        r_multiplier = c(0.1, 1, 2, 3, 5),                        partition_method = \"kfolds\") #> Warning: 27 rows were excluded from database because NAs were found. print(sp_swd) #> prepared_data object summary #> ============================ #> Species: Myrcia hatschbachii  #> Number of Records: 524  #>   - Presence: 51  #>   - Background: 473  #> Partition Method: kfolds  #>   - Number of kfolds: 4  #> Continuous Variables: #>   - bio_1, bio_7, bio_12, bio_15  #> Categorical Variables: #>   - SoilType  #> PCA Information: PCA not performed #> Weights: No weights provided #> Calibration Parameters: #>   - Algorithm: maxnet  #>   - Number of candidate models: 610  #>   - Features classes (responses): l, q, p, lq, lqp  #>   - Regularization multipliers: 0.1, 1, 2, 3, 5   # Prepare data for glm model sp_swd_glm <- prepare_data(algorithm = \"glm\", occ = occ_data,                            x = \"x\", y = \"y\",                            raster_variables = var,                            species = occ_data[1, 1],                            categorical_variables = \"SoilType\",                            n_background = 500, bias_file = bias,                            bias_effect = \"direct\",                            features = c(\"l\", \"q\", \"p\", \"lq\", \"lqp\"),                            partition_method = \"kfolds\") #> Warning: 27 rows were excluded from database because NAs were found. print(sp_swd_glm) #> prepared_data object summary #> ============================ #> Species: Myrcia hatschbachii  #> Number of Records: 524  #>   - Presence: 51  #>   - Background: 473  #> Partition Method: kfolds  #>   - Number of kfolds: 4  #> Continuous Variables: #>   - bio_1, bio_7, bio_12, bio_15  #> Categorical Variables: #>   - SoilType  #> PCA Information: PCA not performed #> Weights: No weights provided #> Calibration Parameters: #>   - Algorithm: glm  #>   - Number of candidate models: 122  #>   - Features classes (responses): l, q, p, lq, lqp"},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_projection.html","id":null,"dir":"Reference","previous_headings":"","what":"Preparation of data for model projections — prepare_projection","title":"Preparation of data for model projections — prepare_projection","text":"function prepared data model projections multiple scenarios, storing paths rasters representing scenario.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_projection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preparation of data for model projections — prepare_projection","text":"","code":"prepare_projection(models = NULL, variable_names = NULL, present_dir = NULL,                    past_dir = NULL, past_period = NULL, past_gcm = NULL,                    future_dir = NULL, future_period = NULL,                    future_pscen = NULL, future_gcm = NULL,                    write_file = FALSE, filename = NULL,                    raster_pattern = \".tif*\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_projection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preparation of data for model projections — prepare_projection","text":"models object class fitted_models returned fit_selected() function. Default NULL. variable_names (character) names variables used fit model PCA prepare_data() function. applicable models argument provided. Default NULL. present_dir (character) path folder containing variables represent current scenario projection. Default NULL. past_dir (character) path folder containing subfolders v ariables representing past scenarios projection. Default NULL. past_period (character) names subfolders within past_dir, representing specific time periods (e.g., 'LGM' 'MID'). past_gcm (character) names subfolders within past_period folders, representing specific General Circulation Models (GCMs). future_dir (character) path folder containing subfolders variables representing future scenarios projection. Default NULL. future_period (character) names subfolders within future_dir, representing specific time periods (e.g., '2041-2060' '2081-2100'). Default NULL. future_pscen (character) names subfolders within future_period, representing specific emission scenarios (e.g., 'ssp126' 'ssp585'). Default NULL. future_gcm (character) names subfolders within future_pscen folders, representing specific General Circulation Models (GCMs). Default NULL. write_file (logical) whether write object containing paths structured folders. object required projecting models across multiple scenarios using project_selected() function. Default FALSE. filename (character) path name folder object saved. applicable write_file = TRUE. Default NULL. raster_pattern (character) pattern used identify format raster files within folders. Default \".tif*\".","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_projection.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preparation of data for model projections — prepare_projection","text":"object class prepared_projection containing following elements: Present, Past, Future: paths variables structured subfolders. Raster_pattern: pattern used identify format raster files within folders. PCA: principal component analysis (PCA) performed set variables prepare_data(), list class \"prcomp\" returned. See ?stats::prcomp() details. variables: names raw predictos variables used project.","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_projection.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preparation of data for model projections — prepare_projection","text":"","code":"# Import example of fitted_models (output of fit_selected()) data(\"fitted_model_maxnet\", package = \"kuenm2\")  # Organize and structure future climate variables from WorldClim # Import the current variables used to fit the model. # In this case, SoilType will be treated as a static variable (constant # across future scenarios). var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  # Create a \"Current_raw\" folder in a temporary directory and copy the raw # variables there. out_dir_current <- file.path(tempdir(), \"Current_raw\") dir.create(out_dir_current, recursive = TRUE)  # Save current variables in temporary directory terra::writeRaster(var, file.path(out_dir_current, \"Variables.tif\"))  # Set the input directory containing the raw future climate variables. # For this example, the data is located in the \"inst/extdata\" folder. in_dir <- system.file(\"extdata\", package = \"kuenm2\")  # Create a \"Future_raw\" folder in a temporary directory and copy the raw # variables there. out_dir_future <- file.path(tempdir(), \"Future_raw\")  # Organize and rename the future climate data, structuring it by year and GCM. # The 'SoilType' variable will be appended as a static variable in each scenario. # The files will be renamed following the \"bio_\" format organize_future_worldclim(input_dir = in_dir,                           output_dir = out_dir_future,                           name_format = \"bio_\", variables = NULL,                           fixed_variables = var$SoilType, mask = NULL,                           overwrite = TRUE) #>    |                                                                               |                                                                      |   0%   |                                                                               |=========                                                             |  12%   |                                                                               |==================                                                    |  25%   |                                                                               |==========================                                            |  38%   |                                                                               |===================================                                   |  50%   |                                                                               |============================================                          |  62%   |                                                                               |====================================================                  |  75%   |                                                                               |=============================================================         |  88%   |                                                                               |======================================================================| 100% #>  #> Variables successfully organized in directory: #> /tmp/RtmpAuudrv/Future_raw  # Prepare projections using fitted models to check variables pr <- prepare_projection(models = fitted_model_maxnet,                          present_dir = out_dir_current,                          past_dir = NULL,                          past_period = NULL,                          past_gcm = NULL,                          future_dir = out_dir_future,                          future_period = c(\"2041-2060\", \"2081-2100\"),                          future_pscen = c(\"ssp126\", \"ssp585\"),                          future_gcm = c(\"ACCESS-CM2\", \"MIROC6\"),                          write_file = FALSE,                          filename = NULL,                          raster_pattern = \".tif*\") pr #> projection_data object summary #> ============================= #> Variables prepared to project models for Present and Future  #> Future projections contain the following periods, scenarios and GCMs: #>   - Periods: 2041-2060 | 2081-2100  #>   - Scenarios: ssp126 | ssp585  #>   - GCMs: ACCESS-CM2 | MIROC6  #> All variables are located in the following root directory: #> /tmp/RtmpAuudrv  # Prepare projections using variables names pr_b <- prepare_projection(models = NULL,                            variable_names = c(\"bio_1\", \"bio_7\", \"bio_12\"),                            present_dir = out_dir_current,                            past_dir = NULL,                            past_period = NULL,                            past_gcm = NULL,                            future_dir = out_dir_future,                            future_period = c(\"2041-2060\", \"2081-2100\"),                            future_pscen = c(\"ssp126\", \"ssp585\"),                            future_gcm = c(\"ACCESS-CM2\", \"MIROC6\"),                            write_file = FALSE,                            filename = NULL,                            raster_pattern = \".tif*\") pr_b #> projection_data object summary #> ============================= #> Variables prepared to project models for Present and Future  #> Future projections contain the following periods, scenarios and GCMs: #>   - Periods: 2041-2060 | 2081-2100  #>   - Scenarios: ssp126 | ssp585  #>   - GCMs: ACCESS-CM2 | MIROC6  #> All variables are located in the following root directory: #> /tmp/RtmpAuudrv"},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_user_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare data for model calibration with user-prepared calibration data — prepare_user_data","title":"Prepare data for model calibration with user-prepared calibration data — prepare_user_data","text":"function prepares data model calibration using user-prepared calibration data. includes optional PCA, k-fold partitioning, creation grid parameter combinations, including distinct regularization multiplier values, various feature classes, different sets environmental variables.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_user_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare data for model calibration with user-prepared calibration data — prepare_user_data","text":"","code":"prepare_user_data(algorithm, user_data, pr_bg, species = NULL, x = NULL,                   y = NULL, features = c(\"lq\", \"lqp\"),                   r_multiplier = c(0.1, 0.5, 1, 2, 3),                   partition_method = \"subsample\", n_replicates = 4,                   train_proportion = 0.7, user_part = NULL,                   categorical_variables = NULL,                   do_pca = FALSE, center = TRUE, scale = TRUE,                   exclude_from_pca = NULL, variance_explained = 95,                   min_explained = 5, min_number = 2, min_continuous = NULL,                   weights = NULL, include_xy = TRUE, write_pca = FALSE,                   pca_directory = NULL, write_file = FALSE, file_name = NULL,                   seed = 1)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_user_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare data for model calibration with user-prepared calibration data — prepare_user_data","text":"algorithm (character) modeling algorithm, either \"glm\" \"maxnet\". user_data (data frame) data.frame column presence (1) background (0) records, together variable values (one variable per column). See example data(\"user_data\", package = \"kuenm2\"). pr_bg (character) name column user_data contains presence/background records. species (character) string specifying species name (optional). Default NULL. x (character) string specifying name column user_data contains longitude values. Default NULL. Must defined present user_data otherwise considered another predictor variable. y (character) string specifying name column user_data contains latitude values. Default NULL. Must defined present user_data otherwise considered another predictor variable. features (character) vector feature classes. Default c(\"q\", \"lq\", \"lp\", \"qp\", \"lqp\"). r_multiplier (numeric) vector regularization parameters maxnet. Default c(0.1, 1, 2, 3, 5). partition_method (character) method used data partitioning. Available options \"kfolds\", \"subsample\", \"bootstrap\". See Details information. n_replicates (numeric) number replicates generate. partition_method \"subsample\" \"bootstrap\", defines number partitions. \"kfolds\", specifies number folds. Default 4. train_proportion (numeric) proportion occurrence background points used model training replicate. applicable partition_method \"subsample\" \"bootstrap\". Default 0.7 (.e., 70% training 30% testing). user_part user provided list replicates folds cross-validation used model calibration. element list contain vector indices indicating test points, used split use_data training testing sets. categorical_variables (character) names variables categorical. Default NULL. do_pca (logical) whether perform principal component analysis (PCA) set variables. Default FALSE. center (logical) whether variables zero-centered. Default TRUE. scale (logical) whether variables scaled unit variance analysis takes place. Default FALSE. exclude_from_pca (character) variable names within raster_variables included PCA transformation. Instead, variables added directly final set output variables without modified. default NULL, meaning variables used unless specified otherwise. variance_explained (numeric) cumulative percentage total variance must explained selected principal components. Default 95. min_explained (numeric) minimum percentage total variance principal component must explain retained. Default 5. min_number (numeric) minimum number variables included model formulas generated. min_continuous (numeric) minimum number continuous variables required combination. Default NULL. weights (numeric) numeric vector specifying weights occurrence records. Default NULL. include_xy (logical) whether include coordinates (longitude latitude) results preparing data. Default TRUE. write_pca (logical) whether save PCA-derived raster layers (principal components) disk. Default FALSE. pca_directory (character) path name folder PC raster layers saved. applicable write_pca = TRUE. Default NULL. write_file (logical) whether write resulting prepared_data list local directory. Default FALSE. file_name (character) path name folder resulting list saved. applicable write_file = TRUE. Default NULL. seed (numeric) integer value specify initial seed split data. Default 1.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_user_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare data for model calibration with user-prepared calibration data — prepare_user_data","text":"object class prepared_data containing elements run model calibration routine. elements include: species, calibration data, grid model parameters, indices k-folds cross validation, xy coordinates, names continuous categorical variables, weights, results PCA, modeling algorithm.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_user_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Prepare data for model calibration with user-prepared calibration data — prepare_user_data","text":"available data partitioning methods : \"kfolds\": Splits dataset K subsets (folds) approximately equal size. replicate, one fold used test set, remaining folds combined form training set. \"bootstrap\": Creates training dataset sampling observations original dataset replacement (.e., observation can selected multiple times). test set consists observations selected specific replicate. \"subsample\": Similar bootstrap, training set created sampling without replacement (.e., observation selected ). test set includes observations selected training. \"leave-one-\": special case kfolds number folds equals number presence records. replicate, single presence left serve test set, remaining observations used training.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_user_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare data for model calibration with user-prepared calibration data — prepare_user_data","text":"","code":"# Import user-prepared data data(\"user_data\", package = \"kuenm2\")  # Prepare data for maxnet model maxnet_swd_user <- prepare_user_data(algorithm = \"maxnet\",                                      user_data = user_data, pr_bg = \"pr_bg\",                                      species = \"Myrcia hatschbachii\",                                      categorical_variables = \"SoilType\",                                      features = c(\"l\", \"q\", \"p\", \"lq\", \"lqp\"),                                      r_multiplier = c(0.1, 1, 2, 3, 5)) maxnet_swd_user #> prepared_data object summary #> ============================ #> Species: Myrcia hatschbachii  #> Number of Records: 527  #>   - Presence: 51  #>   - Background: 476  #> Partition Method: subsample  #>   - Number of replicates: 4  #>   - Train proportion: 0.7  #> Continuous Variables: #>   - bio_1, bio_7, bio_12, bio_15  #> Categorical Variables: #>   - SoilType  #> PCA Information: PCA not performed #> Weights: No weights provided #> Calibration Parameters: #>   - Algorithm: maxnet  #>   - Number of candidate models: 610  #>   - Features classes (responses): l, q, p, lq, lqp  #>   - Regularization multipliers: 0.1, 1, 2, 3, 5   # Prepare data for glm model glm_swd_user <- prepare_user_data(algorithm = \"glm\",                                   user_data = user_data, pr_bg = \"pr_bg\",                                   species = \"Myrcia hatschbachii\",                                   categorical_variables = \"SoilType\",                                   features = c(\"l\", \"q\", \"p\", \"lq\", \"lqp\")) glm_swd_user #> prepared_data object summary #> ============================ #> Species: Myrcia hatschbachii  #> Number of Records: 527  #>   - Presence: 51  #>   - Background: 476  #> Partition Method: subsample  #>   - Number of replicates: 4  #>   - Train proportion: 0.7  #> Continuous Variables: #>   - bio_1, bio_7, bio_12, bio_15  #> Categorical Variables: #>   - SoilType  #> PCA Information: PCA not performed #> Weights: No weights provided #> Calibration Parameters: #>   - Algorithm: glm  #>   - Number of candidate models: 122  #>   - Features classes (responses): l, q, p, lq, lqp"},{"path":"https://marlonecobos.github.io/kuenm2/reference/print.html","id":null,"dir":"Reference","previous_headings":"","what":"Print method for kuenm2 objects — print","title":"Print method for kuenm2 objects — print","text":"Print method kuenm2 objects","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/print.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print method for kuenm2 objects — print","text":"","code":"# S3 method for class 'prepared_data' print(x, ...)  # S3 method for class 'calibration_results' print(x, ...)  # S3 method for class 'fitted_models' print(x, ...)  # S3 method for class 'projection_data' print(x, ...)  # S3 method for class 'model_projections' print(x, ...)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/print.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print method for kuenm2 objects — print","text":"x object classes: prepared_data, calibration_results, fitted_models, projection_data, model_projections. ... additional arguments affecting summary produced. Ignored functions.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/print.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print method for kuenm2 objects — print","text":"printed version object summarizes main elements contained.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/project_selected.html","id":null,"dir":"Reference","previous_headings":"","what":"Project selected models to multiple sets of new data (scenarios) — project_selected","title":"Project selected models to multiple sets of new data (scenarios) — project_selected","text":"function performs predictions selected models multiple scenarios, specified projection_data object created prepare_projection() function. addition generating predictions replicate, function calculates consensus measures (e.g., mean, median) across replicates models.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/project_selected.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Project selected models to multiple sets of new data (scenarios) — project_selected","text":"","code":"project_selected(models, projection_data, out_dir, mask = NULL,                  consensus_per_model = TRUE, consensus_general = TRUE,                  consensus = c(\"median\", \"range\", \"mean\", \"stdev\"),                  write_replicates = FALSE, extrapolation_type = \"E\",                  var_to_clamp = NULL, type = NULL, overwrite = FALSE,                  parallel = FALSE, ncores = NULL,                  progress_bar = TRUE, verbose = TRUE)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/project_selected.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Project selected models to multiple sets of new data (scenarios) — project_selected","text":"models object class fitted_models returned fit_selected() function. projection_data object class projection_data returned prepare_projection() function. file contains paths rasters representing scenario. out_dir (character) path root directory saving raster file projection. mask (SpatRaster, SpatVector, SpatExtent) spatial object used mask variables predict. Default NULL. consensus_per_model (logical) whether calculate consensus across replicates one replicate per model. Default TRUE. consensus_general (logical) whether calculate consensus across models one selected model. Default TRUE. consensus (character) consensus measures calculate. Options available 'median', 'range', 'mean' 'stdev' (standard deviation). Default c(\"median\", \"range\", \"mean\", \"stdev\"). write_replicates (logical) whether write projections replicate. Default FALSE. extrapolation_type (character) extrapolation type model. Models can transferred three options: free extrapolation ('E'), extrapolation clamping ('EC'), extrapolation ('NE'). Default = 'E'. See details. var_to_clamp (character) vector specifying variables clamp. applicable extrapolation_type \"EC\" \"NE\". Default NULL, meaning variables clamped extrapolated. type (character) format prediction values. maxnet models, valid options \"raw\", \"cumulative\", \"logistic\", \"cloglog\". glm models, valid options \"response\" \"raw\". NULL (default), function uses \"cloglog\" maxnet models \"response\" glm models. overwrite (logical) whether overwrite SpatRaster already exists. applicable write_files set TRUE. Default FALSE. parallel (logical) whether fit candidate models parallel. Default FALSE. ncores (numeric) number cores use parallel processing. Default NULL uses available cores - 1. applicable parallel = TRUE. progress_bar (logical) whether display progress bar processing. Default TRUE. verbose (logical) whether display messages processing. Default TRUE.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/project_selected.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Project selected models to multiple sets of new data (scenarios) — project_selected","text":"model_projections object provides paths raster files projection results corresponding thresholds used binarize predictions.","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/project_selected.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Project selected models to multiple sets of new data (scenarios) — project_selected","text":"","code":"# Step 1: Organize variables for current projection ## Import current variables (used to fit models) var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  ## Create a folder in a temporary directory to copy the variables out_dir_current <- file.path(tempdir(), \"Current_raw_wc\") dir.create(out_dir_current, recursive = TRUE)  ## Save current variables in temporary directory terra::writeRaster(var, file.path(out_dir_current, \"Variables.tif\"))   # Step 2: Organize future climate variables (example with WorldClim) ## Directory containing the downloaded future climate variables (example) in_dir <- system.file(\"extdata\", package = \"kuenm2\")  ## Create a folder in a temporary directory to copy the future variables out_dir_future <- file.path(tempdir(), \"Future_raw_wc\")  ## Organize and rename the future climate data (structured by year and GCM) ### 'SoilType' will be appended as a static variable in each scenario organize_future_worldclim(input_dir = in_dir, output_dir = out_dir_future,                           name_format = \"bio_\", fixed_variables = var$SoilType) #>    |                                                                               |                                                                      |   0%   |                                                                               |=========                                                             |  12%   |                                                                               |==================                                                    |  25%   |                                                                               |==========================                                            |  38%   |                                                                               |===================================                                   |  50%   |                                                                               |============================================                          |  62%   |                                                                               |====================================================                  |  75%   |                                                                               |=============================================================         |  88%   |                                                                               |======================================================================| 100% #>  #> Variables successfully organized in directory: #> /tmp/RtmpAuudrv/Future_raw_wc  # Step 3: Prepare data to run multiple projections ## An example with maxnet models ## Import example of fitted_models (output of fit_selected()) data(fitted_model_maxnet, package = \"kuenm2\")  ## Prepare projection data using fitted models to check variables pr <- prepare_projection(models = fitted_model_maxnet,                          present_dir = out_dir_current,                          future_dir = out_dir_future,                          future_period = c(\"2081-2100\"),                          future_pscen = c(\"ssp126\", \"ssp585\"),                          future_gcm = c(\"ACCESS-CM2\", \"MIROC6\"),                          raster_pattern = \".tif*\")  # Step 4: Run multiple model projections ## A folder to save projection results out_dir <- file.path(tempdir(), \"Projection_results/maxnet_projections\") dir.create(out_dir, recursive = TRUE)  ## Project selected models to multiple scenarios p <- project_selected(models = fitted_model_maxnet, projection_data = pr,                       out_dir = out_dir) #>    |                                                                               |                                                                      |   0%   |                                                                               |==============                                                        |  20%   |                                                                               |============================                                          |  40%   |                                                                               |==========================================                            |  60%   |                                                                               |========================================================              |  80%   |                                                                               |======================================================================| 100%"},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_changes.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute changes of suitable areas between scenarios — projection_changes","title":"Compute changes of suitable areas between scenarios — projection_changes","text":"function performs map algebra operations represent suitable areas change compared scenario model trained. Changes identified loss (contraction), gain (expansion) stability. multiple climate models (GCM) used, calculates level agreement among emission scenario.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_changes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute changes of suitable areas between scenarios — projection_changes","text":"","code":"projection_changes(model_projections, reference_id = 1, consensus = \"median\",                    include_id = NULL, user_threshold = NULL, by_gcm = TRUE,                    by_change = TRUE, general_summary = TRUE,                    force_resample = TRUE, write_results = TRUE,                    output_dir = NULL, overwrite = FALSE,                    write_bin_models = FALSE, return_raster = FALSE)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_changes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute changes of suitable areas between scenarios — projection_changes","text":"model_projections model_projections object generated project_selected() function. object contains file paths raster projection results thresholds used binarization predictions. reference_id (numeric) reference ID projections corresponding current time model_projections. Default 1. See details section information. consensus (character) consensus measure use calculating changes. Available options 'mean', 'median', 'range', 'stdev' (standard deviation). Default 'median'. include_id (numeric) vector containing reference IDs include computing changes. Default NULL, meaning projections included. See details section information. user_threshold (numeric) optional threshold binarizing predictions. Default NULL, meaning function apply thresholds stored model_projections, calculated earlier using omission rate calibration(). by_gcm (logical) whether compute changes across GCMs. Default TRUE. by_change (logical) whether compute results separately change, identifying areas gain, loss, stability GCM. Default TRUE. general_summary (logical) whether generate general summary, mapping many GCMs project gain, loss, stability scenario. Default TRUE. force_resample (logical) whether force projection rasters extent resolution raster corresponding reference_id, represents current projections. Default TRUE. write_results (logical) whether write raster files containing computed changes disk. Default TRUE. output_dir (character) directory path resulting raster files containing computed changes saved. relevant write_results = TRUE. overwrite (logical) whether overwrite SpatRaster already exist. applicable write_results set TRUE. Default FALSE. write_bin_models (logical) whether write binarized models GCM disk. Default FALSE. return_raster (logical) whether return list containing SpatRasters computed changes. Default FALSE, meaning function return NULL object. Setting argument TRUE using multiple GCMs large extent fine resolution may overload RAM.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_changes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute changes of suitable areas between scenarios — projection_changes","text":"changes_projections object. return_raster = TRUE, function returns list containing SpatRasters computed changes. list includes following elements: Binarized: binarized models GCM. Results_by_gcm: computed changes GCM. Results_by_change: list SpatRaster represents specific change. Summary_changes: general summary indicates many GCMs project gain, loss, stability scenario root_directory: path directory results saved write_results set TRUE return_raster = FALSE, function returns NULL object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_changes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute changes of suitable areas between scenarios — projection_changes","text":"projecting niche model different temporal scenarios (past future), species’ areas can classified three categories relative current baseline: gain, loss stability. interpretation categories depends temporal direction projection. projecting future scenarios: Gain: Areas currently unsuitable become suitable future. Loss: Areas currently suitable become unsuitable future. Stability: Areas retain current classification future, whether suitable unsuitable. projecting past scenarios: Gain: Areas unsuitable past now suitable present. Loss: Areas suitable past now unsuitable present. Stability: Areas retain past classification present, whether suitable unsuitable. reference scenario (current conditions) can accessed paths element model_projections object (model_projections$path). ID differ 1 one projection current conditions. Specific projections can included excluded analysis using include_id argument. example, setting 'include_id = c(3, 5, 7)' compute changes scenarios 3, 5, 7. Conversely, setting 'include_id = -c(3, 5, 7)' exclude scenarios 3, 5, 7 analysis.","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_changes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute changes of suitable areas between scenarios — projection_changes","text":"","code":"# Step 1: Organize variables for current projection ## Import current variables (used to fit models) var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  ## Create a folder in a temporary directory to copy the variables out_dir_current <- file.path(tempdir(), \"Current_raw3\") dir.create(out_dir_current, recursive = TRUE)  ## Save current variables in temporary directory terra::writeRaster(var, file.path(out_dir_current, \"Variables.tif\"))   # Step 2: Organize future climate variables (example with WorldClim) ## Directory containing the downloaded future climate variables (example) in_dir <- system.file(\"extdata\", package = \"kuenm2\")  ## Create a folder in a temporary directory to copy the future variables out_dir_future <- file.path(tempdir(), \"Future_raw3\")  ## Organize and rename the future climate data (structured by year and GCM) ### 'SoilType' will be appended as a static variable in each scenario organize_future_worldclim(input_dir = in_dir, output_dir = out_dir_future,                           name_format = \"bio_\", fixed_variables = var$SoilType) #>    |                                                                               |                                                                      |   0%   |                                                                               |=========                                                             |  12%   |                                                                               |==================                                                    |  25%   |                                                                               |==========================                                            |  38%   |                                                                               |===================================                                   |  50%   |                                                                               |============================================                          |  62%   |                                                                               |====================================================                  |  75%   |                                                                               |=============================================================         |  88%   |                                                                               |======================================================================| 100% #>  #> Variables successfully organized in directory: #> /tmp/RtmpAuudrv/Future_raw3  # Step 3: Prepare data to run multiple projections ## An example with maxnet models ## Import example of fitted_models (output of fit_selected()) data(fitted_model_maxnet, package = \"kuenm2\")  ## Prepare projection data using fitted models to check variables pr <- prepare_projection(models = fitted_model_maxnet,                          present_dir = out_dir_current,                          future_dir = out_dir_future,                          future_period = c(\"2081-2100\"),                          future_pscen = c(\"ssp585\"),                          future_gcm = c(\"ACCESS-CM2\", \"MIROC6\"),                          raster_pattern = \".tif*\")  # Step 4: Run multiple model projections ## A folder to save projection results out_dir <- file.path(tempdir(), \"Projection_results/maxnet1\") dir.create(out_dir, recursive = TRUE)  ## Project selected models to multiple scenarios p <- project_selected(models = fitted_model_maxnet, projection_data = pr,                       out_dir = out_dir) #>    |                                                                               |                                                                      |   0%   |                                                                               |=======================                                               |  33%   |                                                                               |===============================================                       |  67%   |                                                                               |======================================================================| 100%  # Step 5: Identify areas of change in projections ## Contraction, expansion and stability changes <- projection_changes(model_projections = p, write_results = FALSE,                               return_raster = TRUE)  terra::plot(changes$Binarized)  # SpatRaster with the binarized predictions  terra::plot(changes$Results_by_gcm)  # SpatRaster with changes by GCM  changes$Results_by_change  # List of SpatRaster(s) by changes with GCM agreement #> $`Future_2081-2100_ssp585` #> class       : SpatRaster  #> size        : 52, 40, 3  (nrow, ncol, nlyr) #> resolution  : 0.1666667, 0.1666667  (x, y) #> extent      : -53.5, -46.83333, -30.83333, -22.16667  (xmin, xmax, ymin, ymax) #> coord. ref. : lon/lat WGS 84 (EPSG:4326)  #> source(s)   : memory #> names       :           Unsuitable-stable,           Loss,           Suitable-stable  #> min values  : Unsuitable-stable in 0 GCMs, Loss in 0 GCMs, Suitable-stable in 0 GCMs  #> max values  : Unsuitable-stable in 2 GCMs, Loss in 2 GCMs, Suitable-stable in 2 GCMs  #>  terra::plot(changes$Results_by_change$`Future_2081-2100_ssp585`)  # an example of the previous  terra::plot(changes$Summary_changes)  # SpatRaster with a general summary"},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_mop.html","id":null,"dir":"Reference","previous_headings":"","what":"Analysis of extrapolation risks in projections using the MOP metric — projection_mop","title":"Analysis of extrapolation risks in projections using the MOP metric — projection_mop","text":"Calculates mobility-oriented parity metric sub-products represent dissimilarities non-analogous conditions comparing set reference conditions (M) model projection conditions (G).","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_mop.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Analysis of extrapolation risks in projections using the MOP metric — projection_mop","text":"","code":"projection_mop(data, projection_data, out_dir,                subset_variables = FALSE, mask = NULL, type = \"basic\",                na_in_range = TRUE, calculate_distance = FALSE,                where_distance = \"in_range\", distance = \"euclidean\",                scale = FALSE, center = FALSE, fix_NA = TRUE, percentage = 1,                comp_each = 2000, tol = NULL, rescale_distance = FALSE,                parallel = FALSE, ncores = NULL, progress_bar = TRUE,                overwrite = FALSE)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_mop.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Analysis of extrapolation risks in projections using the MOP metric — projection_mop","text":"data object class fitted_models returned fit_selected() function object class prepared_data returned prepare_data() function. projection_data object class projection_data returned prepare_projection()function. file contains paths rasters representing scenario. out_dir (character) path root directory saving raster file projection. subset_variables (logical) whether include analysis variables present selected models. Default FALSE mask (SpatRaster, SpatVector, SpatExtent) spatial object used mask variables (optional). Default NULL. type (character) type MOP analysis performed. Options available \"basic\", \"simple\" \"detailed\". See Details information. na_in_range (logical) whether assign NA regions within projected area (G) environmental conditions fall within range calibration data (M). TRUE (default), regions assigned NA. FALSE, assigned 0 simple basic MOP outputs, \"within ranges\" detailed MOP output. calculate_distance (logical) whether calculate distances (dissimilarities) m g. default, FALSE, runs rapidly assess dissimilarity levels. where_distance (character) calculate distances, considering conditions g positioned comparison range conditions m. Options available \"in_range\", \"out_range\" \"\". Default \"in_range\". distance (character) distances calculated, euclidean mahalanobis. applicable calculate_distance = TRUE. scale (logical numeric) whether scale scale. Default FALSE. center (logical numeric) whether center scale. Default FALSE. fix_NA (logical) whether fix layers cells NA values layers. Setting FALSE may save time rasters big NA matching problems. Default TRUE. percentage (numeric) percentage m closest conditions used derive mean environmental distances combination conditions g. comp_each (numeric) number combinations g used distance calculations time. Increasing number requires RAM tol (numeric) tolerance detect linear dependencies calculating Mahalanobis distances. default, NULL, uses .Machine$double.eps. rescale_distance (logical) whether re-scale distances 0-1. Re-scaling prevents comparisons dissimilarity values obtained runs different values percentage. parallel (logical) whether fit candidate models parallel. Default FALSE. ncores (numeric) number cores use parallel processing. Default NULL uses available cores - 1. applicable parallel = TRUE. progress_bar (logical) whether display progress bar processing. Default TRUE. overwrite (logical) whether overwrite SpatRaster already exists. applicable write_files set TRUE. Default FALSE.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_mop.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Analysis of extrapolation risks in projections using the MOP metric — projection_mop","text":"object class mop_projections, root directory dataframe containing file paths results stored scenario. paths contain following files: summary - data.frame details data used analysis: variables - names variables considered. type - type MOP analysis performed. scale - value according argument scale. center - value according argument center. calculate_distance - value according argument calculate_distance. distance - option regarding distance used. percentage - percentage m used reference distance calculation. rescale_distance - value according argument rescale_distance. fix_NA - value according argument fix_NA. N_m - total number elements (cells values valid rows) m. N_g - total number elements (cells values valid rows) g. m_min - minimum values (lower limit) variables reference conditions (m). m_max - maximum values (upper limit) variables reference conditions (m). mop_distances - calculate_distance = TRUE, SpatRaster vector distance values set interest (g). Higher values represent greater dissimilarity compared set reference (m). mop_basic - SpatRaster vector, set interest, representing conditions least one variables non-analogous set reference. Values : 1 non-analogous conditions, NA conditions inside ranges reference set. mop_simple - SpatRaster vector, set interest, representing many variables set interest non-analogous reference set. NA used conditions inside ranges reference set. mop_detailed - list containing: interpretation_combined - data.frame help identify combinations variables towards_low_combined towards_high_combined non-analogous m. towards_low_end - SpatRaster matrix variables representing non-analogous conditions found towards low values variable. towards_high_end - SpatRaster matrix variables representing non-analogous conditions found towards high values variable. towards_low_combined - SpatRaster vector values representing identity variables found non-analogous conditions towards low values. vector, interpretation requires use data.frame interpretation_combined. towards_high_combined - SpatRaster vector values representing identity variables found non-analogous conditions towards high values. vector, interpretation requires use data.frame interpretation_combined.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_mop.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Analysis of extrapolation risks in projections using the MOP metric — projection_mop","text":"type options return results differ detail non-analogous conditions identified. basic - makes calculation proposed Owens et al. (2013) doi:10.1016/j.ecolmodel.2013.04.011. simple - calculates many variables set interest non-analogous reference set. detailed - calculates five additional extrapolation metrics. See mop_detailed Value full details. where_distance options determine values used calculate dissimilarity in_range - conditions inside m ranges out_range - conditions outside m ranges - conditions variables used represent conditions different units, scaling centering recommended. step valid Euclidean distances used.","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_mop.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Analysis of extrapolation risks in projections using the MOP metric — projection_mop","text":"","code":"# Step 1: Organize variables for current projection ## Import current variables (used to fit models) var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  ## Create a folder in a temporary directory to copy the variables out_dir_current <- file.path(tempdir(), \"Current_raw4\") dir.create(out_dir_current, recursive = TRUE)  ## Save current variables in temporary directory terra::writeRaster(var, file.path(out_dir_current, \"Variables.tif\"))   # Step 2: Organize future climate variables (example with WorldClim) ## Directory containing the downloaded future climate variables (example) in_dir <- system.file(\"extdata\", package = \"kuenm2\")  ## Create a folder in a temporary directory to copy the future variables out_dir_future <- file.path(tempdir(), \"Future_raw4\")  ## Organize and rename the future climate data (structured by year and GCM) ### 'SoilType' will be appended as a static variable in each scenario organize_future_worldclim(input_dir = in_dir, output_dir = out_dir_future,                           name_format = \"bio_\", fixed_variables = var$SoilType) #>    |                                                                               |                                                                      |   0%   |                                                                               |=========                                                             |  12%   |                                                                               |==================                                                    |  25%   |                                                                               |==========================                                            |  38%   |                                                                               |===================================                                   |  50%   |                                                                               |============================================                          |  62%   |                                                                               |====================================================                  |  75%   |                                                                               |=============================================================         |  88%   |                                                                               |======================================================================| 100% #>  #> Variables successfully organized in directory: #> /tmp/RtmpAuudrv/Future_raw4  # Step 3: Prepare data to run multiple projections ## An example with maxnet models ## Import example of fitted_models (output of fit_selected()) data(fitted_model_maxnet, package = \"kuenm2\")  ## Prepare projection data using fitted models to check variables pr <- prepare_projection(models = fitted_model_maxnet,                          present_dir = out_dir_current,                          future_dir = out_dir_future,                          future_period = c(\"2041-2060\", \"2081-2100\"),                          future_pscen = c(\"ssp126\", \"ssp585\"),                          future_gcm = c(\"ACCESS-CM2\", \"MIROC6\"),                          raster_pattern = \".tif*\")  # Step 4: Perform MOP for all projection scenarios ## Create a folder to save MOP results out_dir <- file.path(tempdir(), \"MOPresults\") dir.create(out_dir, recursive = TRUE)  ## Run MOP kmop <- projection_mop(data = fitted_model_maxnet, projection_data = pr,                        out_dir = out_dir, type = \"detailed\") #>    |                                                                               |                                                                      |   0%   |                                                                               |========                                                              |  11%   |                                                                               |================                                                      |  22%   |                                                                               |=======================                                               |  33%   |                                                                               |===============================                                       |  44%   |                                                                               |=======================================                               |  56%   |                                                                               |===============================================                       |  67%   |                                                                               |======================================================                |  78%   |                                                                               |==============================================================        |  89%   |                                                                               |======================================================================| 100%"},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_variability.html","id":null,"dir":"Reference","previous_headings":"","what":"Explores variance coming from distinct sources in model predictions — projection_variability","title":"Explores variance coming from distinct sources in model predictions — projection_variability","text":"Calculates variance model predictions, distinguishing different sources variation. Potential sources include replicates, model parameterizations, general circulation models (GCMs).","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_variability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Explores variance coming from distinct sources in model predictions — projection_variability","text":"","code":"projection_variability(model_projections, by_replicate = TRUE, by_gcm = TRUE,                        by_model = TRUE, consensus = \"median\",                        write_files = FALSE, output_dir = NULL,                        return_rasters = TRUE, progress_bar = FALSE,                        verbose = TRUE, overwrite = FALSE)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_variability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Explores variance coming from distinct sources in model predictions — projection_variability","text":"model_projections model_projections object generated project_selected() function. object contains file paths raster projection results thresholds used binarizing predictions. by_replicate (logical) whether compute variance originating replicates. by_gcm (logical) whether compute variance originating general circulation models (GCMs) by_model (logical) whether compute variance originating model parameterizations. consensus (character) (character) consensus measure use calculating changes. Available options 'mean', 'median', 'range', 'stdev' (standard deviation). Default 'median'. write_files (logical) whether write raster files containing computed variance disk. Default FALSE. output_dir (character) directory path resulting raster files containing computed changes saved. relevant write_results = TRUE. return_rasters (logical) whether return list containing SpatRasters computed changes. Default TRUE. Setting argument FALSE returns NULL object. progress_bar (logical) whether display progress bar processing. Default TRUE. verbose (logical) whether display messages processing. Default TRUE. overwrite whether overwrite SpatRaster already exists. applicable write_files set TRUE. Default FALSE.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_variability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Explores variance coming from distinct sources in model predictions — projection_variability","text":"object class variability_projections. return_rasters = TRUE, function returns list containing SpatRasters computed variances, categorized replicate, model, GCMs. write_files = TRUE, also returns directory path computed rasters saved disk, object can used import files later import_projections() function. return_rasters = FALSE write_files = FALSE, function returns NULL","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_variability.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Explores variance coming from distinct sources in model predictions — projection_variability","text":"","code":"# Step 1: Organize variables for current projection ## Import current variables (used to fit models) var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  ## Create a folder in a temporary directory to copy the variables out_dir_current <- file.path(tempdir(), \"Current_raw5\") dir.create(out_dir_current, recursive = TRUE)  ## Save current variables in temporary directory terra::writeRaster(var, file.path(out_dir_current, \"Variables.tif\"))   # Step 2: Organize future climate variables (example with WorldClim) ## Directory containing the downloaded future climate variables (example) in_dir <- system.file(\"extdata\", package = \"kuenm2\")  ## Create a folder in a temporary directory to copy the future variables out_dir_future <- file.path(tempdir(), \"Future_raw5\")  ## Organize and rename the future climate data (structured by year and GCM) ### 'SoilType' will be appended as a static variable in each scenario organize_future_worldclim(input_dir = in_dir, output_dir = out_dir_future,                           name_format = \"bio_\", fixed_variables = var$SoilType) #>    |                                                                               |                                                                      |   0%   |                                                                               |=========                                                             |  12%   |                                                                               |==================                                                    |  25%   |                                                                               |==========================                                            |  38%   |                                                                               |===================================                                   |  50%   |                                                                               |============================================                          |  62%   |                                                                               |====================================================                  |  75%   |                                                                               |=============================================================         |  88%   |                                                                               |======================================================================| 100% #>  #> Variables successfully organized in directory: #> /tmp/RtmpAuudrv/Future_raw5  # Step 3: Prepare data to run multiple projections ## An example with maxnet models ## Import example of fitted_models (output of fit_selected()) data(fitted_model_maxnet, package = \"kuenm2\")  ## Prepare projection data using fitted models to check variables pr <- prepare_projection(models = fitted_model_maxnet,                          present_dir = out_dir_current,                          future_dir = out_dir_future,                          future_period = \"2041-2060\",                          future_pscen = \"ssp126\",                          future_gcm = c(\"ACCESS-CM2\", \"MIROC6\"),                          raster_pattern = \".tif*\")  # Step 4: Run multiple model projections ## A folder to save projection results out_dir <- file.path(tempdir(), \"Projection_results/maxnet3\") dir.create(out_dir, recursive = TRUE)  ## Project selected models to multiple scenarios p <- project_selected(models = fitted_model_maxnet, projection_data = pr,                       out_dir = out_dir) #>    |                                                                               |                                                                      |   0%   |                                                                               |=======================                                               |  33%   |                                                                               |===============================================                       |  67%   |                                                                               |======================================================================| 100%  # Step 5: Compute variance from distinct sources v <- projection_variability(model_projections = p, by_replicate = FALSE) #> Calculating variability from distinct models: scenario 1 of 2 #> Calculating variability from distinct models: scenario 2 of 2 #> Calculating variability from distinct GCMs: scenario 2 of 2  #terra::plot(v$Present$by_rep)  # Variance from replicates, present projection terra::plot(v$Present$by_model)  # From models  #terra::plot(v$`Future_2041-2060_ssp126`$by_rep)  # From replicates future projection terra::plot(v$`Future_2041-2060_ssp126`$by_model)  # From models  terra::plot(v$`Future_2041-2060_ssp126`$by_gcm)  # From GCMs"},{"path":"https://marlonecobos.github.io/kuenm2/reference/response_curve.html","id":null,"dir":"Reference","previous_headings":"","what":"Variable response curves for fitted models — response_curve","title":"Variable response curves for fitted models — response_curve","text":"view variable responses fitted models. Responses based single multiple models can plotted.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/response_curve.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Variable response curves for fitted models — response_curve","text":"","code":"response_curve(models, variable, modelID = NULL, n = 100,                by_replicates = FALSE, data = NULL, new_data = NULL,                averages_from = \"pr_bg\", extrapolate = TRUE,                extrapolation_factor = 0.1, add_points = FALSE, p_col = NULL,                l_limit = NULL, u_limit = NULL,                xlab = NULL, ylab = \"Suitability\",                col = \"darkblue\", ...)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/response_curve.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Variable response curves for fitted models — response_curve","text":"models object class fitted_models returned fit_selected() function. variable (character) name variable plotted. modelID (character) vector ModelID(s) considered models object. default models included.Default = NULL. n (numeric) integer guiding number breaks. Default = 100 by_replicates (logical) whether use replicates full_model estimate model's response curve. Default = FALSE. data data.frame matrix data used model calibration step. Default = NULL. new_data SpatRaster, data.frame,  matrix variables representing range variable values area interest. Default = NULL. must defined case model entered explicitly include data component. averages_from (character) specifies averages modes variables calculated. Available options \"pr\" (calculate averages presence localities) \"pr_bg\" (use combined set presence background localities). Default \"pr_bg\". See details. extrapolate (logical) whether allow extrapolation study behavior response outside calibration limits. Ignored new_data defined. Default = TRUE. extrapolation_factor (numeric) multiplier used calculate extrapolation range. Larger values allow broader extrapolation beyond observed data range. Default 0.1. add_points (logical) TRUE, adds original observed points (0/1) plot. Default = FALSE. p_col (character) color observed points add_points = TRUE. valid R color name hexadecimal code. Default = \"black\". l_limit (numeric) specifies lower limit variable. Default NULL, meaning lower limit calculated based data's minimum value extrapolation_factor (extrapolation = TRUE). u_limit (numeric) specifies upper limit variable. Default NULL, meaning upper limit calculated based data's minimum value extrapolation_factor (extrapolation = TRUE). xlab (character) label x axis. default, NULL, uses name defined variable. ylab (character) label y axis. Default = \"Suitability\". col (character) color lines. Default = \"darkblue\". ... additional arguments passed plot.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/response_curve.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Variable response curves for fitted models — response_curve","text":"plot response curve variable.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/response_curve.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Variable response curves for fitted models — response_curve","text":"response curves generated variables set mean values (mode categorical variables), calculated either presence localities (averages_from = \"pr\") combined set presence background localities (averages_from = \"pr_bg\"). categorical variables, bar plot generated error bars showing variability across models (multiple models included).","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/response_curve.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Variable response curves for fitted models — response_curve","text":"","code":"# Example with maxnet # Import example of fitted_models (output of fit_selected()) data(fitted_model_maxnet, package = \"kuenm2\")  #Response curves response_curve(models = fitted_model_maxnet,                variable = \"bio_1\", by_replicates = TRUE)  response_curve(models = fitted_model_maxnet, variable = \"bio_1\",                modelID = \"Model_192\", by_replicates = TRUE)   # Example with GLM # Import example of fitted_models (output of fit_selected()) data(fitted_model_glm, package = \"kuenm2\")  #Response curves response_curve(models = fitted_model_glm,                variable = \"bio_1\", by_replicates = TRUE)  response_curve(models = fitted_model_glm, variable = \"bio_1\",                modelID = \"Model_85\", by_replicates = TRUE)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/select_models.html","id":null,"dir":"Reference","previous_headings":"","what":"Select models that perform the best among candidates — select_models","title":"Select models that perform the best among candidates — select_models","text":"function selects best models according user-defined criteria, evaluating statistical significance (partial ROC), predictive ability (omission rates), model complexity (AIC).","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/select_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select models that perform the best among candidates — select_models","text":"","code":"select_models(calibration_results = NULL, candidate_models = NULL, data = NULL,               algorithm = NULL, compute_proc = FALSE,               addsamplestobackground = TRUE, weights = NULL,               remove_concave = FALSE, omission_rate = NULL,               allow_tolerance = TRUE, tolerance = 0.01,               significance = 0.05, delta_aic = 2, parallel = FALSE,               ncores = NULL, progress_bar = FALSE,verbose = TRUE)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/select_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select models that perform the best among candidates — select_models","text":"calibration_results object class calibration_results returned calibration() function. Default NULL. candidate_models (data.frame) summary evaluation metrics candidate model. Required calibration_results NULL. output calibration(), data.frame located $calibration_results$Summary. Default NULL. data object class prepared_data returned prepare_data() function. Required calibration_results NULL compute_proc TRUE. algorithm (character) model algorithm, either \"glm\" \"maxnet\". default, NULL, uses one defined part calibration_results, data. arguments used, algorithm must defined. compute_proc (logical) whether compute partial ROC tests selected models. required partial ROC calculated candidate models calibration. Default FALSE. addsamplestobackground (logical) whether add background presence sample already . Required compute_proc TRUE calibration_results NULL.Default TRUE. weights (numeric) numeric vector specifying weights occurrence records. Required compute_proc TRUE calibration_results NULL. Default NULL. remove_concave (logical) whether remove candidate models presenting concave curves. Default FALSE. omission_rate (numeric) maximum omission rate candidate model can considered potentially selected model. default, NULL, uses value provided part calibration_results. purposes selection existing results evaluation, value must match one values used omission tests, must manually defined. allow_tolerance (logical) whether allow selection models minimum values omission rates even omission rate surpasses omission_rate. applicable candidate models omission rates higher omission_rate. Default TRUE. tolerance (numeric) value added minimum omission rate exceeds omission_rate. allow_tolerance = TRUE, selected models omission rate equal less minimum rate plus tolerance. Default 0.01. significance (numeric) significance level select models based partial ROC (pROC). Default 0.05. See Details. delta_aic (numeric) value delta AIC used threshold select models. Default 2. parallel (logical) whether calculate PROC candidate models parallel. Default FALSE. ncores (numeric) number cores use parallel processing. Default NULL uses available cores - 1. applicable parallel = TRUE. progress_bar (logical) whether display progress bar processing. Default TRUE. verbose (logical) whether display messages processing. Default TRUE.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/select_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select models that perform the best among candidates — select_models","text":"calibration_results provided, returns new calibration_results new selected models summary. calibration_results NULL, returns list containing following elements: selected_models: data frame ID summary evaluation metrics selected models. summary: list containing delta AIC values model selection, ID values models failed fit, concave curves, non-significant pROC values, omission rates threshold, delta AIC values threshold, selected models.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/select_models.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Select models that perform the best among candidates — select_models","text":"Partial ROC calculated following Peterson et al. (2008).","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/select_models.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Select models that perform the best among candidates — select_models","text":"","code":"# Import example of calibration results (output of calibration function) ## GLM data(calib_results_glm, package = \"kuenm2\")  #Select new best models based on another value of omission rate new_best_model <- select_models(calibration_results = calib_results_glm,                                 algorithm = \"glm\", compute_proc = TRUE,                                 omission_rate = 10)  # Omission error of 10 #> Selecting best among 122 models. #> Calculating pROC... #>  #> Filtering 122 models. #> Removing 0 model(s) because they failed to fit. #> 21 models were selected with omission rate below 10%. #> Selecting 1 final model(s) with delta AIC <2. #> Validating pROC of selected models... #>  #> All selected models have significant pROC values.  # Compare with best models selected previously calib_results_glm$summary$Selected  # Model 86 selected #> [1] 85 new_best_model$summary$Selected  # Models 64, 73 and 86 selected #> [1] 85"},{"path":"https://marlonecobos.github.io/kuenm2/reference/sp_swd.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepared Data for maxnet models — sp_swd","title":"Prepared Data for maxnet models — sp_swd","text":"prepared_data object resulted prepare_data() calibrate models using 'glm' algorithm.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/sp_swd.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepared Data for maxnet models — sp_swd","text":"","code":"data(\"sp_swd\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/sp_swd.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Prepared Data for maxnet models — sp_swd","text":"prepared_data object following elements: species Species names calibration_data data.frame containing variables extracted presence background points formula_grid data.frame ID, formulas, regularization multipliers candidate model part_data list partition data, element corresponds replicate contains indices test points replicate partition_method character indicating partition method n_replicates numeric value indicating number replicates k-folds train_proportion numeric value indicating proportion occurrences used train points partition method 'subsample' 'boostrap' data_xy data.frame coordinates occurrence bakground points continuous_variables character indicating names continuous variables categorical_variables character indicating names categorical variables weights numeric value specifying weights occurrence records. NULL, meaning set weights. pca prcomp object storing PCA information. NULL, meaning PCA performed algorithm character indicanting algorithm (glm)","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/sp_swd_glm.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepared Data for glm models — sp_swd_glm","title":"Prepared Data for glm models — sp_swd_glm","text":"prepared_data object resulted prepare_data() calibrate models using 'glm' algorithm.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/sp_swd_glm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepared Data for glm models — sp_swd_glm","text":"","code":"data(\"sp_swd_glm\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/sp_swd_glm.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Prepared Data for glm models — sp_swd_glm","text":"prepared_data object following elements: species Species names calibration_data data.frame containing variables extracted presence background points formula_grid data.frame ID, formulas, regularization multipliers candidate model part_data list partition data, element corresponds replicate contains indices test points replicate partition_method character indicating partition method n_replicates numeric value indicating number replicates k-folds train_proportion numeric value indicating proportion occurrences used train points partition method 'subsample' 'boostrap' data_xy data.frame coordinates occurrence bakground points continuous_variables character indicating names continuous variables categorical_variables character indicating names categorical variables weights numeric value specifying weights occurrence records. NULL, meaning set weights. pca prcomp object storing PCA information. NULL, meaning PCA performed algorithm character indicanting algorithm (glm)","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/user_data.html","id":null,"dir":"Reference","previous_headings":"","what":"User Custom Calibration Data — user_data","title":"User Custom Calibration Data — user_data","text":"data.frame containing presence background records along environmental variables used demonstrate data preparation user-supplied data.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/user_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"User Custom Calibration Data — user_data","text":"","code":"data(\"user_data\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/user_data.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"User Custom Calibration Data — user_data","text":"data.frame following columns: pr_bg Column indicating presences (1) background (0). bio_1 extracted values variable bio_1 presence background points. bio_7 extracted values variable bio_12 presence background points. bio_12 extracted values variable bio_12 presence background points. bio_15 extracted values variable bio_15 presence background points. bio_15 extracted values variable soilType presence background points.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/var.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing present-day Conditions (WorldClim) — var","title":"SpatRaster Representing present-day Conditions (WorldClim) — var","text":"Raster layer containing bioclimatic variables representing present-day climatic conditions. variables obtained 10 arc-minute resolution masked using m region provided package. Data sourced WorldClim: https://worldclim.org/data/worldclim21.html","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/var.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing present-day Conditions (WorldClim) — var","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/var.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing present-day Conditions (WorldClim) — var","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/var.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing present-day Conditions (WorldClim) — var","text":"","code":"var <- terra::rast(system.file(\"extdata\",                                \"Current_variables.tif\",                                 package = \"kuenm2\")) terra::plot(var)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/variable_importance.html","id":null,"dir":"Reference","previous_headings":"","what":"Variable importance — variable_importance","title":"Variable importance — variable_importance","text":"Variable importance","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/variable_importance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Variable importance — variable_importance","text":"","code":"variable_importance(models, modelID = NULL, parallel = FALSE, ncores = NULL,                     progress_bar = TRUE, verbose = TRUE)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/variable_importance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Variable importance — variable_importance","text":"models object class fitted_models returned fit_selected() function. modelID (character). Default = NULL. parallel (logical) whether calculate importance parallel. Default FALSE. ncores (numeric) number cores use parallel processing. Default NULL uses available cores - 1. applicable parallel = TRUE. progress_bar (logical) whether display progress bar processing. Default TRUE. verbose (logical) whether display detailed messages processing. Default TRUE.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/variable_importance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Variable importance — variable_importance","text":"data.frame containing relative contribution variable. identification distinct models added fitted contains multiple models.","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/variable_importance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Variable importance — variable_importance","text":"","code":"# Example with maxnet # Import example of fitted_models (output of fit_selected()) data(fitted_model_maxnet, package = \"kuenm2\")  # Variable importance imp_maxnet <- variable_importance(models = fitted_model_maxnet) #>  #> Calculating variable contribution for model 1 of 2 #>    |                                                                               |                                                                      |   0%   |                                                                               |============                                                          |  17%   |                                                                               |=======================                                               |  33%   |                                                                               |===================================                                   |  50%   |                                                                               |===============================================                       |  67%   |                                                                               |==========================================================            |  83%   |                                                                               |======================================================================| 100% #>  #> Calculating variable contribution for model 2 of 2 #>    |                                                                               |                                                                      |   0%   |                                                                               |==========                                                            |  14%   |                                                                               |====================                                                  |  29%   |                                                                               |==============================                                        |  43%   |                                                                               |========================================                              |  57%   |                                                                               |==================================================                    |  71%   |                                                                               |============================================================          |  86%   |                                                                               |======================================================================| 100%  # Plot plot_importance(imp_maxnet)   # Example with glm # Import example of fitted_models (output of fit_selected()) data(fitted_model_glm, package = \"kuenm2\")  # Variable importance imp_glm <- variable_importance(models = fitted_model_glm) #>  #> Calculating variable contribution for model 1 of 1 #>    |                                                                               |                                                                      |   0%   |                                                                               |============                                                          |  17%   |                                                                               |=======================                                               |  33%   |                                                                               |===================================                                   |  50%   |                                                                               |===============================================                       |  67%   |                                                                               |==========================================================            |  83%   |                                                                               |======================================================================| 100%  # Plot plot_importance(imp_glm)"}]
