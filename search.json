[{"path":"https://marlonecobos.github.io/kuenm2/articles/basic_data_cleaning.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"1. Basic Data Cleaning","text":"Description Getting ready Import data Basic cleaning steps cleaning steps Saving results","code":""},{"path":"https://marlonecobos.github.io/kuenm2/articles/basic_data_cleaning.html","id":"description","dir":"Articles","previous_headings":"","what":"Description","title":"1. Basic Data Cleaning","text":"modeling technique, ecological niche modeling depends quality input data, particularly species occurrence records. Cleaning data critical step minimize biases, reduce errors, ensure meaningful model outcomes. vignette introduces tools available kuenm2 package facilitate cleaning occurrence data prior modeling. guides users inspecting data, applying cleaning functions, saving cleaned datasets, within reproducible R workflow. want highlight additional data cleaning filtering steps (e.g., spatial thinning) may necessary depending type model modeling approach user intends adopt. tools presented designed assist basic steps cleaning data modeling.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/articles/basic_data_cleaning.html","id":"getting-ready","dir":"Articles","previous_headings":"","what":"Getting ready","title":"1. Basic Data Cleaning","text":"kuenm installed yet, please . See Main guide installation instructions. Load kuenm required packages, define working directory (needed). general, setting working directory R considered good practice, provides better control files read saved . users working within R project, recommend setting working directory, since least one file saved later stages guide. Note: functions packages (.e., base R kuenm) used guide displayed package::function().","code":"# Load packages library(kuenm2) library(terra)  # Current directory getwd()  # Define new directory #setwd(\"YOUR/DIRECTORY\")  # uncomment and modify if setting a new directory  # Saving original plotting parameters original_par <- par(no.readonly = TRUE)"},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/articles/basic_data_cleaning.html","id":"import-data","dir":"Articles","previous_headings":"Cleaning data","what":"Import data","title":"1. Basic Data Cleaning","text":"use occurrence records provided within kuenm package. example data sets package derived Trindade & Marques (2024). occ_data_noclean object contains 51 valid occurrences Myrcia hatschbachii (tree endemic Southern Brazil) group erroneous records demonstrate cleaning steps.  raster layer included package also used example. bioclimatic variable WorldClim 2.1 10 arc-minute resolution. layer masked using polygon generated drawing minimum convex polygon around records 300 km buffer.   Visualize occurrences records geography:","code":"# Import occurrences data(occ_data_noclean, package = \"kuenm2\")  # Check data structure str(occ_data_noclean) #> 'data.frame':    64 obs. of  3 variables: #>  $ species: chr  \"Myrcia hatschbachii\" \"Myrcia hatschbachii\" \"Myrcia hatschbachii\" \"Myrcia hatschbachii\" ... #>  $ x      : num  -51.3 -50.6 -49.3 -49.8 -50.2 ... #>  $ y      : num  -29 -27.6 -27.8 -26.9 -28.2 ... # Import raster layers var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                 package = \"kuenm2\"))  # Keep only one layer var <- var$bio_1  # Check variable terra::plot(var) # Visualize occurrences on one variable ## Create an extent based on the layer and the records to see all errors vext <- terra::ext(var)  # extent of layer pext <- apply(occ_data_noclean[, 2:3], 2, range, na.rm = TRUE)  # extent of records  allext <- terra::ext(c(min(pext[1, 1], vext[1]), max(pext[2, 1], vext[2]),                         min(pext[1, 2], vext[3]), max(pext[2, 2], vext[4]))) + 1  # plotting records on the variable terra::plot(var, ext = allext, main = \"Bio 1\") points(occ_data_noclean[, c(\"x\", \"y\")])"},{"path":"https://marlonecobos.github.io/kuenm2/articles/basic_data_cleaning.html","id":"basic-cleaning-steps","dir":"Articles","previous_headings":"Cleaning data","what":"Basic cleaning steps","title":"1. Basic Data Cleaning","text":"basic data cleaning steps implemented kuenm help : remove missing data, eliminate duplicates, exclude typically (though always) erroneous coordinates 0 longitude 0 latitude, filter records low coordinate precision based number decimal places. example cleaning missing data. example uses data.frame containing columns “Species”, “x”, “y” (“x” “y” represent longitude latitude, respectively). data.frame includes additional columns considered identifying missing values, users can specify columns use via columns argument (default = NULL, includes columns). function, recommend consulting documentation detailed explanations (e.g., help(remove_missing)).  code uses previous results continues process cleaning data removing duplicates. argument columns can used explained . See full documentation help(remove_duplicates).  Continue process removing coordinates values 0 (zero) longitude latitude (always needed, location valid working marine species). See full documentation help(remove_corrdinates_00).  following lines code take previous result remove coordinates low precision. longitude latitude contain decimal places, may rounded, can problematic areas. step recommended users know coordinate rounding issue. filtering process can also applied longitude latitude independently. See full documentation help(filter_decimal_precision).  Users can perform steps single function (initial_cleaning()) follows:","code":"# remove missing data mis <- remove_missing(data = occ_data_noclean, columns = NULL, remove_na = TRUE,                       remove_empty = TRUE)  # quick check nrow(occ_data_noclean) #> [1] 64 nrow(mis) #> [1] 60 # remove exact duplicates mis_dup <- remove_duplicates(data = mis, columns = NULL, keep_all_columns = TRUE)  # quick check nrow(mis) #> [1] 60 nrow(mis_dup) #> [1] 57 # remove records with 0 for x and y coordinates mis_dup_00 <- remove_corrdinates_00(data = mis_dup, x = \"x\", y = \"y\")  # quick check nrow(mis_dup) #> [1] 57 nrow(mis_dup_00) #> [1] 56 # remove coordinates with low decimal precision. mis_dup_00_dec <- filter_decimal_precision(data = mis_dup_00, x = \"x\", y = \"y\",                                             decimal_precision = 2)  # quick check nrow(mis_dup_00) #> [1] 56 nrow(mis_dup_00_dec) #> [1] 51 # all basic cleaning steps clean_init <- initial_cleaning(data = occ_data_noclean, species = \"species\",                                 x = \"x\", y = \"y\", remove_na = TRUE,                                 remove_empty = TRUE, remove_duplicates = TRUE,                                 by_decimal_precision = TRUE,                                decimal_precision = 2)  # quick check nrow(occ_data_noclean)  # original data #> [1] 64 nrow(clean_init)  # data after all basic cleaning steps #> [1] 51  # a final plot to check par(mfrow = c(2, 2))  ## initial data terra::plot(var, ext = allext, main = \"Initial data\") points(occ_data_noclean[, c(\"x\", \"y\")])  ## data after basic cleaning steps terra::plot(var, ext = allext, main = \"After basic cleaning\") points(clean_init[, c(\"x\", \"y\")])  terra::plot(var, main = \"After basic cleaning (zoom)\") points(clean_init[, c(\"x\", \"y\")])"},{"path":"https://marlonecobos.github.io/kuenm2/articles/basic_data_cleaning.html","id":"other-cleaning-steps","dir":"Articles","previous_headings":"Cleaning data","what":"Other cleaning steps","title":"1. Basic Data Cleaning","text":"Two additional cleaning steps implemented kuenm, removing cell duplicates moving points valid cells. Removing cell duplicates involves excluding records exact coordinate duplicates located within pixel. process randomly selects one record cell retain. See full documentation help(remove_cell_duplicates).  following lines code help adjust records fall just outside valid raster cells prevent data loss. Given nature resolution raster layers, valid records sometimes perceived outside boundaries cells data. cases, alternative move records nearest valid cell. distance limit applied avoid relocating records far study area. See example use functionality kuenm. See full documentation help(move_2closest_cell).  function advanced_cleaning() facilitates two processes single step:   Notes: functions move records valid pixels erase points moved. Make sure exclude indicated previous chunk code, needed.","code":"# exclude duplicates based on raster cell (pixel) celldup <- remove_cell_duplicates(data = clean_init, x = \"x\", y = \"y\",                                   raster_layer = var)  # quick check nrow(clean_init)  # data after all basic cleaning steps #> [1] 51 nrow(celldup)  # plus removing cell duplicates #> [1] 42 # move records to valid pixels moved <- move_2closest_cell(data = celldup, x = \"x\", y = \"y\",                              raster_layer = var, move_limit_distance = 10) #> Moving occurrences to closest pixels...  # quick check nrow(celldup)  # basic cleaning and no cell duplicates #> [1] 42 nrow(moved[moved$condition != \"Not_moved\", ])  # plus moved to valid cells #> [1] 41 # move records to valid pixels clean_data <- advanced_cleaning(data = clean_init, x = \"x\", y = \"y\",                                  raster_layer = var, cell_duplicates = TRUE,                                 move_points_inside = TRUE,                                  move_limit_distance = 10) #> Moving occurrences to closest pixels...  # exclude points not moved clean_data <- clean_data[clean_data$condition != \"Not_moved\", 1:3]  # quick check nrow(occ_data_noclean)  # original data #> [1] 64 nrow(clean_init)  # data after all basic cleaning steps #> [1] 51 nrow(clean_data)  # data after all basic cleaning steps #> [1] 41  # a final plot to check par(mfrow = c(3, 2))  ## initial data terra::plot(var, ext = allext, main = \"Initial\") points(occ_data_noclean[, c(\"x\", \"y\")])  ## data after basic cleaning steps terra::plot(var, ext = allext, main = \"Basic cleaning\") points(clean_init[, c(\"x\", \"y\")])  terra::plot(var, main = \"Basic cleaning (zoom)\") points(clean_init[, c(\"x\", \"y\")])  ## data after basic cleaning steps terra::plot(var, main = \"Final data\") points(clean_data[, c(\"x\", \"y\")])  ## zoom to a particular area, initial data terra::plot(var, xlim = c(-48, -50), ylim = c(-26, -25),  main = \"Initial (zoom +)\") points(occ_data_noclean[, c(\"x\", \"y\")])  ## zoom to a particular area, final data terra::plot(var, xlim = c(-48, -50), ylim = c(-26, -25),  main = \"Final (zoom +)\") points(clean_data[, c(\"x\", \"y\")]) # Reset plotting parameters par(original_par)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/basic_data_cleaning.html","id":"saving-results","dir":"Articles","previous_headings":"","what":"Saving results","title":"1. Basic Data Cleaning","text":"results data cleaning steps kuenm simple data.frames may include additional columns fewer records original dataset. easy way save results writing CSV files. Although multiple options exist saving type data, another useful alternative save RDS file directory. See examples :","code":"# Save as CSV write.csv(clean_data, file = \"Clean_data.csv\", row.names = FALSE)  # Save as RDS saveRDS(clean_data, file = \"Clean_data.rds\")"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_calibration.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"3. Model Calibration","text":"Description Getting ready Preparing data Maxnet Models GLM Models Concave curves Re-selecting models Training partion effects Saving calibration_results object","code":""},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_calibration.html","id":"description","dir":"Articles","previous_headings":"","what":"Description","title":"3. Model Calibration","text":"Model calibration one computationally intensive processes automated kuenm2. step, candidate models trained tested using cross-validation approach defined object prepared_data. , models selected based multiple criteria warranty models used later steps robust among candidates. vignette guides users running model calibration examples explore understand options included results process.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_calibration.html","id":"getting-ready","dir":"Articles","previous_headings":"","what":"Getting ready","title":"3. Model Calibration","text":"point assumed kuenm2 installed (, see Main guide). Load kuenm2 required packages, define working directory (needed). Note: functions packages (.e., base R kuenm2) used guide displayed package::function().","code":"# Load packages library(kuenm2) library(terra)  # Current directory getwd()  # Define new directory #setwd(\"YOUR/DIRECTORY\")  # uncomment and modify if setting a new directory  # Saving original plotting parameters original_par <- par(no.readonly = TRUE)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_calibration.html","id":"preparing-data","dir":"Articles","previous_headings":"","what":"Preparing data","title":"3. Model Calibration","text":"start calibration process, need prepared_data object. details data preparation, please refer vignette prepare data model calibration. start, let’s create two prepared_data objects: one using maxnet algorithm, another GLMs:","code":"# Import occurrences data(occ_data, package = \"kuenm2\")  # Import raster layers var <- rast(system.file(\"extdata\", \"Current_variables.tif\", package = \"kuenm2\"))  # Prepare data for maxnet model d_maxnet <- prepare_data(algorithm = \"maxnet\",                          occ = occ_data,                          x = \"x\", y = \"y\",                          raster_variables = var,                          species = \"Myrcia hatschbachii\",                          categorical_variables = \"SoilType\",                           partition_method = \"kfolds\",                           n_partitions = 4,                          n_background = 1000,                          features = c(\"l\", \"q\", \"lq\", \"lqp\"),                          r_multiplier = c(0.1, 1, 2))  # Prepare data for glm model d_glm <- prepare_data(algorithm = \"glm\",                       occ = occ_data,                       x = \"x\", y = \"y\",                       raster_variables = var,                       species = \"Myrcia hatschbachii\",                       categorical_variables = \"SoilType\",                        partition_method = \"bootstrap\",                        n_partitions = 10,                        train_proportion = 0.7,                       n_background = 300,                       features = c(\"l\", \"q\", \"p\", \"lq\", \"lqp\"),                       r_multiplier = NULL)  # Not necessary with glms"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_calibration.html","id":"calibration","dir":"Articles","previous_headings":"","what":"Calibration","title":"3. Model Calibration","text":"calibration() function fits evaluates candidate models considering follow metrics: Unimodality (optional) responses: Assessing coefficients quadratic terms, following Arias-Giraldo & Cobos (2024). Omission error: calculated using models trained separate testing data subsets. Users can specify multiple omission rates considered (e.g., c(5%, 10%)), though one can used threshold selecting best models. Model complexity (AIC): assessed using models generated complete set occurrences. Partial ROC: calculated following Peterson et al. (2008), models meet previous criteria (default).  summary, calibrate evaluate models, function requires prepared_data object following definitions: Omission Errors: Values ranging 0 100, representing percentage potential error attributed various sources uncertainty data. values utilized calculation omission rates partial ROC. Omission Rate Model Selection: specific omission error threshold used select models. value defines maximum omission rate candidate model can considered selection. Removal Concave Curves: specification whether exclude candidate models exhibit concave curves. Optional arguments allow modifications changing delta AIC threshold model selection (default 2), determining whether add presence samples background (default TRUE), whether employ user-specified weights. comprehensive description arguments, refer ?calibration. example, evaluate models considering two omission errors (5% 10%), model selection based 10% omission error. accelerate process, argument parallel can set TRUE specify number cores utilize. detect number available cores machine, run parallel::detectCores().","code":""},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_calibration.html","id":"maxnet-models","dir":"Articles","previous_headings":"Calibration","what":"Maxnet Models","title":"3. Model Calibration","text":"Let’s calibrate maxnet models:  calibration() function returns calibration_results object, list containing various essential pieces information calibration process. elements object can explored printing object indexing . evaluation metrics stored within calibration_results element, see explore :  can also examine details selected models:  printed, calibration_results object provides summary model selection process. includes total number candidate models considered, number models failed fit, number models exhibiting concave curves (along indication whether removed). Additionally, reports number models excluded due non-significant partial ROC (pROC) values, high omission error rates, elevated AIC values. Finally, summary metrics five selected models presented.  example, 300 candidate maxnet models fitted, two selected based significant pROC value, low omission error (<10%), low AIC score (<2).","code":"#Calibrate maxnet models m_maxnet <- calibration(data = d_maxnet,                          error_considered = c(5, 10),                         omission_rate = 10,                         parallel = FALSE,  # Set TRUE to run in parallel                         ncores = 1)  # Define number of cores to run in parallel # Task 1/1: fitting and evaluating models: #   |=====================================================================| 100% #  # Model selection step: # Selecting best among 300 models. # Calculating pROC... #  # Filtering 300 models. # Removing 0 model(s) because they failed to fit. # 135 models were selected with omission rate below 10%. # Selecting 2 final model(s) with delta AIC <2. # Validating pROC of selected models... #   |=====================================================================| 100% # All selected models have significant pROC values. # See first rows of the summary of calibration results head(m_maxnet$calibration_results$Summary[, c(\"ID\", \"Omission_rate_at_10.mean\",                                                \"AICc\", \"Is_concave\")]) #>   ID Omission_rate_at_10.mean     AICc Is_concave #> 1  1                   0.0978 665.8779      FALSE #> 2  2                   0.0978 665.9493      FALSE #> 3  3                   0.0978 665.8956      FALSE #> 4  4                   0.1378 678.2084      FALSE #> 5  5                   0.1378 678.1407      FALSE #> 6  6                   0.1170 678.1182      FALSE # See first rows of the summary of calibration results m_maxnet$selected_models[, c(\"ID\", \"Formulas\", \"R_multiplier\",                              \"Omission_rate_at_10.mean\", \"AICc\", \"Is_concave\")] #>      ID #> 192 192 #> 219 219 #>                                                                                      Formulas #> 192                        ~bio_1 + bio_7 + bio_15 + I(bio_1^2) + I(bio_7^2) + I(bio_15^2) -1 #> 219 ~bio_1 + bio_7 + bio_12 + bio_15 + I(bio_1^2) + I(bio_7^2) + I(bio_12^2) + I(bio_15^2) -1 #>     R_multiplier Omission_rate_at_10.mean     AICc Is_concave #> 192          0.1                   0.0769 608.8669      FALSE #> 219          0.1                   0.0962 610.0462      FALSE print(m_maxnet) #> calibration_results object summary (maxnet) #> ============================================================= #> Species: Myrcia hatschbachii  #> Number of candidate models: 300  #>   - Models removed because they failed to fit: 0  #>   - Models identified with concave curves: 39  #>   - Model with concave curves not removed  #>   - Models removed with non-significant values of pROC: 0  #>   - Models removed with omission error > 10%: 165  #>   - Models removed with delta AIC > 2: 133  #> Selected models: 2  #>   - Up to 5 printed here: #>      ID #> 192 192 #> 219 219 #>                                                                                      Formulas #> 192                        ~bio_1 + bio_7 + bio_15 + I(bio_1^2) + I(bio_7^2) + I(bio_15^2) -1 #> 219 ~bio_1 + bio_7 + bio_12 + bio_15 + I(bio_1^2) + I(bio_7^2) + I(bio_12^2) + I(bio_15^2) -1 #>     Features R_multiplier pval_pROC_at_10.mean Omission_rate_at_10.mean #> 192       lq          0.1                    0                   0.0769 #> 219       lq          0.1                    0                   0.0962 #>         dAIC Parameters #> 192 0.000000          6 #> 219 1.179293          7"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_calibration.html","id":"glm-models","dir":"Articles","previous_headings":"Calibration","what":"GLM Models","title":"3. Model Calibration","text":"Now, let’s calibrate GLM Models see different models factors selected algorithm:  Now, instead two selected models, one:","code":"#Calibrate maxnet models m_glm <- calibration(data = d_glm,                       error_considered = c(5, 10),                      omission_rate = 10,                      parallel = FALSE,  # Set TRUE to run in parallel                      ncores = 1)  # Define number of cores to run in parallel # Task 1/1: fitting and evaluating models: #   |=====================================================================| 100% # Model selection step: # Selecting best among 122 models. # Calculating pROC... #  # Filtering 122 models. # Removing 0 model(s) because they failed to fit. # 21 models were selected with omission rate below 10%. # Selecting 1 final model(s) with delta AIC <2. # Validating pROC of selected models... #   |=====================================================================| 100% # All selected models have significant pROC values. m_glm #> calibration_results object summary (glm) #> ============================================================= #> Species: Myrcia hatschbachii  #> Number of candidate models: 122  #>   - Models removed because they failed to fit: 0  #>   - Models identified with concave curves: 18  #>   - Model with concave curves not removed  #>   - Models removed with non-significant values of pROC: 0  #>   - Models removed with omission error > 10%: 101  #>   - Models removed with delta AIC > 2: 20  #> Selected models: 1  #>   - Up to 5 printed here: #>    ID                                                        Formulas Features #> 85 85 ~bio_1 + bio_7 + bio_12 + I(bio_1^2) + I(bio_7^2) + I(bio_12^2)       lq #>    pval_pROC_at_10.mean Omission_rate_at_10.mean dAIC Parameters #> 85                    0                   0.0904    0          6"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_calibration.html","id":"concave-curves","dir":"Articles","previous_headings":"Calibration","what":"Concave curves","title":"3. Model Calibration","text":"worth noting maxnet glm algorithm, models identified concave response curves. Concave (bimodal) curves indicate higher suitability found variable values around point lower suitability. example, shown right panel figure , higher suitability observed driest wettest regions, lower suitabilities occurring intermediate precipitation levels. Figure 1. Representation convex (left) concave (right) response curves. Dashed lines indicate limist envrionmental conditions model training.  example, none maxnet selected models concave responses, GLM selected least one concave response:  shows model concave responses selected low omission rate AIC values. ensure none selected models concave curves, can set remove_concave = TRUE within calibration() function. Let’s test maxnet algorithm:  Note process now divided two tasks: Task 1: candidate models include quadratic terms fitted. GLMs, models quadratic terms tested. maxnet models, version models quadratic terms highest regularization multiplier tested. maxnet model produces concave response high regularization value, lower values. Task 2: step, function fits evaluates two groups models: (1) models without quadratic terms; (2) models quadratic terms, formulas produce concave responses Task 1.","code":"#Selected maxnet models m_maxnet$selected_models[, c(\"ID\", \"Formulas\", \"Is_concave\")] #>      ID #> 192 192 #> 219 219 #>                                                                                      Formulas #> 192                        ~bio_1 + bio_7 + bio_15 + I(bio_1^2) + I(bio_7^2) + I(bio_15^2) -1 #> 219 ~bio_1 + bio_7 + bio_12 + bio_15 + I(bio_1^2) + I(bio_7^2) + I(bio_12^2) + I(bio_15^2) -1 #>     Is_concave #> 192      FALSE #> 219      FALSE  #Selected glm models m_glm$selected_models[, c(\"ID\", \"Formulas\", \"Is_concave\")] #>    ID                                                        Formulas #> 85 85 ~bio_1 + bio_7 + bio_12 + I(bio_1^2) + I(bio_7^2) + I(bio_12^2) #>    Is_concave #> 85       TRUE m_unimodal <- calibration(data = d_maxnet,                            remove_concave = TRUE,  # Ensures concave models are not selected                           error_considered = c(5, 10),                           omission_rate = 10) # Task 1/2: checking for concave responses in models: #   |=====================================================================| 100% #  # Task 2/2: fitting and evaluating models with no concave responses: #   |=====================================================================| 100% #  # Model selection step: # Selecting best among 300 models. # Calculating pROC... #  # Filtering 300 models. # Removing 0 model(s) because they failed to fit. # Removing 39 model(s) with concave curves. # 110 models were selected with omission rate below 10%. # Selecting 2 final model(s) with delta AIC <2. # Validating pROC of selected models... #   |=====================================================================| 100% # All selected models have significant pROC values."},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_calibration.html","id":"re-selecting-models","dir":"Articles","previous_headings":"","what":"Re-selecting models","title":"3. Model Calibration","text":"model selection procedure conducted internally calibration process. However, possible re-select models considering omission rates (since calculated calibration), model complexity (delta AIC), removing models concave responses. optimize computational time, calibration() calculates pROC values models pre-selected based omission complexity considerations (default). Consequently, pROC values models pre-selected filled NA.  pROC calculated models calibration(), select_models() function requires prepared_data used calibration step, compute_proc must set TRUE. instance, let’s re-select maxnet models applying omission rate 5% instead 10%:  calibration_results object provided, select_models() return calibration_results output selected models summary updated. Note now different selected models maxnet algorithm:  can also provide data.frame containing evaluation metrics candidate model directly select_models(). data.frame available output calibration() function object$calibration_results$Summary. case, function return list containing selected models along summaries model selection process.","code":"# See first rows of the summary of calibration results (pROC values) head(m_maxnet$calibration_results$Summary[, c(\"ID\", \"Mean_AUC_ratio_at_10.mean\",                                               \"pval_pROC_at_10.mean\")]) #>   ID Mean_AUC_ratio_at_10.mean pval_pROC_at_10.mean #> 1  1                        NA                   NA #> 2  2                        NA                   NA #> 3  3                        NA                   NA #> 4  4                        NA                   NA #> 5  5                        NA                   NA #> 6  6                        NA                   NA # See pROC values of selected models m_maxnet$selected_models[, c(\"ID\", \"Mean_AUC_ratio_at_10.mean\",                               \"pval_pROC_at_10.mean\")] #>      ID Mean_AUC_ratio_at_10.mean pval_pROC_at_10.mean #> 192 192                  1.497376                    0 #> 219 219                  1.502309                    0 # Re-select maxnet models new_m_maxnet <- select_models(calibration_results = m_maxnet,                                data = d_maxnet,  # Needed to compute pROC                               compute_proc = TRUE,                                omission_rate = 5)  # New omission rate #> Selecting best among 300 models. #> Calculating pROC... #>  #> Filtering 300 models. #> Removing 0 model(s) because they failed to fit. #> 116 model(s) were selected with omission rate below 5%. #> Selecting 2 final model(s) with delta AIC <2. #> Validating pROC of selected models... #>  #> All selected models have significant pROC values.  print(new_m_maxnet) #> calibration_results object summary (maxnet) #> ============================================================= #> Species: Myrcia hatschbachii  #> Number of candidate models: 300  #>   - Models removed because they failed to fit: 0  #>   - Models identified with concave curves: 39  #>   - Model with concave curves not removed  #>   - Models removed with non-significant values of pROC: 0  #>   - Models removed with omission error > 5%: 184  #>   - Models removed with delta AIC > 2: 114  #> Selected models: 2  #>   - Up to 5 printed here: #>      ID                                                           Formulas #> 159 159                        ~bio_1 + bio_7 + I(bio_1^2) + I(bio_7^2) -1 #> 189 189 ~bio_1 + bio_7 + bio_12 + I(bio_1^2) + I(bio_7^2) + I(bio_12^2) -1 #>     Features R_multiplier pval_pROC_at_5.mean Omission_rate_at_5.mean      dAIC #> 159       lq          0.1                   0                  0.0192 0.8581936 #> 189       lq          0.1                   0                  0.0192 0.0000000 #>     Parameters #> 159          4 #> 189          6 new_m_maxnet$selected_models[,c(\"ID\", \"Formulas\", \"R_multiplier\",                                  \"Omission_rate_at_5.mean\",                                  \"Mean_AUC_ratio_at_5.mean\",                                 \"AICc\", \"Is_concave\")] #>      ID                                                           Formulas #> 159 159                        ~bio_1 + bio_7 + I(bio_1^2) + I(bio_7^2) -1 #> 189 189 ~bio_1 + bio_7 + bio_12 + I(bio_1^2) + I(bio_7^2) + I(bio_12^2) -1 #>     R_multiplier Omission_rate_at_5.mean Mean_AUC_ratio_at_5.mean     AICc #> 159          0.1                  0.0192                 1.476587 622.7677 #> 189          0.1                  0.0192                 1.512957 621.9095 #>     Is_concave #> 159      FALSE #> 189      FALSE #Re-select models using data.frame new_summary <- select_models(candidate_models = m_maxnet$calibration_results$Summary,                              data = d_maxnet,  # Needed to compute pROC                              compute_proc = TRUE,                               omission_rate = 5) #> Selecting best among 300 models. #> Calculating pROC... #>  #> Filtering 300 models. #> Removing 0 model(s) because they failed to fit. #> 116 model(s) were selected with omission rate below 5%. #> Selecting 2 final model(s) with delta AIC <2. #> Validating pROC of selected models... #>  #> All selected models have significant pROC values.  #Get class of object class(new_summary) #> [1] \"list\"  #See selected models new_summary$selected_models[, c(\"ID\", \"Formulas\", \"R_multiplier\",                                  \"Omission_rate_at_5.mean\",                                  \"Mean_AUC_ratio_at_5.mean\",                                 \"AICc\", \"Is_concave\")] #>      ID                                                           Formulas #> 159 159                        ~bio_1 + bio_7 + I(bio_1^2) + I(bio_7^2) -1 #> 189 189 ~bio_1 + bio_7 + bio_12 + I(bio_1^2) + I(bio_7^2) + I(bio_12^2) -1 #>     R_multiplier Omission_rate_at_5.mean Mean_AUC_ratio_at_5.mean     AICc #> 159          0.1                  0.0192                 1.478714 622.7677 #> 189          0.1                  0.0192                 1.508287 621.9095 #>     Is_concave #> 159      FALSE #> 189      FALSE"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_calibration.html","id":"training-partion-effects","dir":"Articles","previous_headings":"","what":"Training partion effects","title":"3. Model Calibration","text":"model calibration, selected models can explored understand effect leaving testing data every cross-validation process. can help understand leaving testing partition changes dramatically response curves compared using sets data. explorations can also used understand ability models predict testing records derives extrapolation , whether extrapolations safe. function partition_response_curves() can used perform explorations mentioned selected models calibration_results object.   multi-panel plot produced , can see response curves variables used model. panel shows: response curve variable model fit portion data leaves partition labeled y axis. Points records used testing, ones corresponding partition left . Dashed lines representing limits environmental values data used fit models. can done models selected. plot second model selected example.","code":"# ID of models that were selected m_maxnet$selected_models$ID #> [1] 192 219  # Response curves for model 192 partition_response_curves(calibration_results = m_maxnet, modelID = 192) # Response curves for model 219 partition_response_curves(calibration_results = m_maxnet, modelID = 219) # Reset plotting parameters par(original_par)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_calibration.html","id":"saving-a-calibration_results-object","dir":"Articles","previous_headings":"","what":"Saving a calibration_results object","title":"3. Model Calibration","text":"calibrating selecting best-performing models, can proceed fit final models (see vignette model exploration) using calibration_results object. object essentially list, users can save local directory using saveRDS(). Saving object facilitates loading back R session later using readRDS(). See example :","code":"# Set directory to save (here, in a temporary directory) dir_to_save <- file.path(tempdir())  # Save the data saveRDS(m_maxnet, file.path(dir_to_save, \"Candidates_maxnet.rds\"))  # Import data m_maxnet <- readRDS(file.path(dir_to_save, \"Candidates_maxnet.rds\"))"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_exploration.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"4. Fit and Explore Selected Models","text":"Description Getting ready Fitting selected models response curves Customized response curves Bivariate response curves Variable importance Model evaluation independent data Saving fitted_models object","code":""},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_exploration.html","id":"description","dir":"Articles","previous_headings":"","what":"Description","title":"4. Fit and Explore Selected Models","text":"best performing models selected, users need fit models (using fit_selected()) order explore characteristics continue next steps. Fitted models can used assess variable importance models, well explore variable response curves. Selected models can also evaluated using independent records used calibration. vignettes contains examples explore multiple options available fit explore selected models.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_exploration.html","id":"getting-ready","dir":"Articles","previous_headings":"","what":"Getting ready","title":"4. Fit and Explore Selected Models","text":"point assumed kuenm2 installed (, see Main guide). Load kuenm2 required packages, define working directory (needed). Note: functions packages (.e., base R kuenm2) used guide displayed package::function().","code":"# Load packages library(kuenm2) library(terra)  # Current directory getwd()  # Define new directory #setwd(\"YOUR/DIRECTORY\")  # uncomment and modify if setting a new directory  # Saving original plotting parameters original_par <- par(no.readonly = TRUE)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_exploration.html","id":"fitting-selected-models","dir":"Articles","previous_headings":"","what":"Fitting selected models","title":"4. Fit and Explore Selected Models","text":"fit selected models, need calibration_results object. details model calibration, please refer vignette Model Calibration. calibration_results object generated vignette available data example package. Let’s load .  object contains results candidate models calibrated using maxnet algorithm. package also provides similar example wit models created using glm algorithm. See load calib_results_glm object case want explore .  Note calibration_results object stores information related calibration process model evaluation, include fitted maxnet (glm) models. obtain fitted models, need use fit_selected() function. default, function fits full model (.e., without replicates without splitting data training testing sets). However, can configure fit final models replicates desired. example, ’ll fit final models 4 k-fold replicates (leaving one fold ), Model Calibration vignette.  fit_selected() function returns fitted_models object, list contains essential information fitted models, required subsequent steps. can explore contents fitted_models object indexing elements. example, fitted maxnet (glm) model objects stored within Models element. Note Models nested list: selected model (case, models 192 219), includes replicates (fitted replicates) full model.  fitted_models object also stores thresholds can used binarize models suitable unsuitable areas. thresholds correspond omission error used model selection (e.g., 5% 10%). can access omission error used calculate thresholds directly object:  omission error used calculate thresholds 10%, meaning predictions binarized, approximately 10% presence records used calibrate models fall cells predicted values threshold. thresholds summarized two ways: mean median across replicates model, consensus mean median across selected models (one model selected):","code":"# Import an example of calibration results  data(\"calib_results_maxnet\", package = \"kuenm2\")  # Print calibration result calib_results_maxnet #> calibration_results object summary (maxnet) #> ============================================================= #> Species: Myrcia hatschbachii  #> Number of candidate models: 300  #>   - Models removed because they failed to fit: 0  #>   - Models identified with concave curves: 39  #>   - Model with concave curves not removed  #>   - Models removed with non-significant values of pROC: 0  #>   - Models removed with omission error > 10%: 165  #>   - Models removed with delta AIC > 2: 133  #> Selected models: 2  #>   - Up to 5 printed here: #>      ID #> 192 192 #> 219 219 #>                                                                                      Formulas #> 192                        ~bio_1 + bio_7 + bio_15 + I(bio_1^2) + I(bio_7^2) + I(bio_15^2) -1 #> 219 ~bio_1 + bio_7 + bio_12 + bio_15 + I(bio_1^2) + I(bio_7^2) + I(bio_12^2) + I(bio_15^2) -1 #>     Features R_multiplier pval_pROC_at_10.mean Omission_rate_at_10.mean #> 192       lq          0.1                    0                   0.0769 #> 219       lq          0.1                    0                   0.0962 #>         dAIC Parameters #> 192 0.000000          6 #> 219 1.179293          7 # Import calib_results_glm data(\"calib_results_glm\", package = \"kuenm2\")  # Print calibration result calib_results_glm #> calibration_results object summary (glm) #> ============================================================= #> Species: Myrcia hatschbachii  #> Number of candidate models: 122  #>   - Models removed because they failed to fit: 0  #>   - Models identified with concave curves: 18  #>   - Model with concave curves not removed  #>   - Models removed with non-significant values of pROC: 0  #>   - Models removed with omission error > 10%: 101  #>   - Models removed with delta AIC > 2: 20  #> Selected models: 1  #>   - Up to 5 printed here: #>    ID                                                        Formulas Features #> 85 85 ~bio_1 + bio_7 + bio_12 + I(bio_1^2) + I(bio_7^2) + I(bio_12^2)       lq #>    pval_pROC_at_10.mean Omission_rate_at_10.mean dAIC Parameters #> 85                    0                   0.0904    0          6 # Fit selected models using calibration results fm <- fit_selected(calibration_results = calib_results_maxnet,                     replicate_method = \"kfolds\", n_replicates = 4) # Fitting replicates... #   |========================================================================| 100% # Fitting full models... #   |========================================================================| 100% # See names of selected models names(fm$Models) #> [1] \"Model_192\" \"Model_219\"  # See models inside Model 192 names(fm$Models$Model_192) #> [1] \"Replicate_1\" \"Replicate_2\" \"Replicate_3\" \"Replicate_4\" \"Full_model\" # Get omission error used to select models and calculate the theshold values fm$omission_rate #> [1] 10 fm$thresholds #> $Model_192 #> $Model_192$mean #> [1] 0.2449115 #>  #> $Model_192$median #> [1] 0.266498 #>  #>  #> $Model_219 #> $Model_219$mean #> [1] 0.260294 #>  #> $Model_219$median #> [1] 0.2695625 #>  #>  #> $consensus #> $consensus$mean #> [1] 0.2526028 #>  #> $consensus$median #> [1] 0.2680302 #>  #>  #> $type #> [1] \"cloglog\""},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_exploration.html","id":"response-curves","dir":"Articles","previous_headings":"","what":"Response curves","title":"4. Fit and Explore Selected Models","text":"selected models fitted_models object can used generate response curves. Individual variable response curves illustrate environmental variables influences suitability, keeping variables constant. default, curves generated variables set mean values (mode categorical variables). mean mode calculated combined set presence background data (averages_from = \"pr_bg\"). can change behavior use presence localities setting averages_from = \"pr\".","code":""},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_exploration.html","id":"all-response-curves","dir":"Articles","previous_headings":"Response curves","what":"All response curves","title":"4. Fit and Explore Selected Models","text":"easy way explore response curves variables follows:   previous plot, dashed lines indicate range variables values data used fit models. multiple models fitted, variability shown via Generalized Additive Model (GAM) using mean 95% confidence interval. detailed view response curve models fitted look like, argument show_lines can set TRUE.   explore response curves variables model independently, argument model_ID can used. plot show variable response curves full model.   get view variability response curves model produced replicates, use show_variability = TRUE. , GAM multiple-line approaches can used show variability.","code":"# Produce response curves for all variables in all fitted models all_response_curves(fm) # All response curves showing each model as a different line all_response_curves(fm, show_lines = TRUE) # All response curves for model 219 all_response_curves(fm, modelID = \"Model_219\") # All response curves for model 219 (GAM for variability) all_response_curves(fm, modelID = \"Model_219\", show_variability = TRUE) # All response curves for model 219 (each curve is a replicate) all_response_curves(fm, modelID = \"Model_219\", show_variability = TRUE,                      show_lines = TRUE)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_exploration.html","id":"customized-response-curves","dir":"Articles","previous_headings":"Response curves","what":"Customized response curves","title":"4. Fit and Explore Selected Models","text":"previous plots help gain quick understanding response curves fitted models, little can done make panel look better. produce nicer plots, response curves variable time can produced. Let’s check variables available plot examining coefficients full models:  variables bio_1, bio_7, bio_12 bio_15have non-zero coefficient values, means contribute model available generating response curves. Remember response curves computed using selected models. time let’s change margins labels plot   can also specify selected models used generate response curves:   default, response curve extends beyond range model fitting limits (dashed lines) based observed range values extrapolation_factor (extrapolation = TRUE). default extrapolation factor set 10% range. extrapolation = FALSE, extrapolation occurs, plot limits match fitting data range exactly. can increase extrapolation factor allow broader range beyond observed data. response curve plotted extrapolation factor 2:   Note response curve now extends beyond observed data range. Optionally, can manually set lower upper limits variables. example, since bio_12 represents annual precipitation negative values realistic, can set lower limit 0:   Now, lower limit plot bio_12 set 0. Since specify upper limit, plot uses extrapolation factor (, 0.1) define upper limit. aspect notice increasing extrapolation factor curves looked like perfectly unimodal. Unfortunately, using GAM summarize represent variability can times generate effect. However, plotting responses model independent curves show real shape curve, see differences setting show_lines = TRUE.   Optionally, can add presence background points used fit models response curve plot setting add_points = TRUE:","code":"# Get variables with non-zero coefficients in the models fm$Models[[1]]$Full_model$betas  # From the first model selected #>        bio_1        bio_7       bio_15   I(bio_1^2)   I(bio_7^2)  I(bio_15^2)  #> 11.572321659  0.215970079  0.369077970 -0.356605446 -0.020306099 -0.006200151 fm$Models[[2]]$Full_model$betas  # From the second model selected #>         bio_1        bio_12        bio_15    I(bio_1^2)    I(bio_7^2)  #>  1.178814e+01  1.638570e-02  3.406100e-01 -3.625405e-01 -1.440450e-02  #>   I(bio_12^2)   I(bio_15^2)  #> -5.261860e-06 -5.623987e-03 par(mfrow = c(2, 2), mar = c(4, 4, 1, 0.5))  # Set grid of plot response_curve(models = fm, variable = \"bio_1\", las = 1) response_curve(models = fm, variable = \"bio_7\", ylab = \"\", las = 1) response_curve(models = fm, variable = \"bio_12\", las = 1) response_curve(models = fm, variable = \"bio_15\", ylab = \"\", las = 1) par(mfrow = c(2, 2), mar = c(4, 4, 2.5, 0.5))  # Set grid of plot response_curve(models = fm, variable = \"bio_1\", modelID = \"Model_192\",                 main = \"Model_192\", las = 1) response_curve(models = fm, variable = \"bio_1\", modelID = \"Model_219\",                 main = \"Model_219\", ylab = \"\", las = 1) response_curve(models = fm, variable = \"bio_7\", modelID = \"Model_192\",                 main = \"Model_192\", las = 1) response_curve(models = fm, variable = \"bio_7\", modelID = \"Model_219\",                 main = \"Model_219\", ylab = \"\", las = 1) par(mfrow = c(2, 2), mar = c(4, 4, 1, 0.5))  # Set grid of plot response_curve(models = fm, variable = \"bio_1\", extrapolation_factor = 2,                 las = 1) response_curve(models = fm, variable = \"bio_7\", extrapolation_factor = 2,                ylab = \"\", las = 1) response_curve(models = fm, variable = \"bio_12\", extrapolation_factor = 2,                las = 1) response_curve(models = fm, variable = \"bio_15\", extrapolation_factor = 2,                ylab = \"\", las = 1) response_curve(models = fm, variable = \"bio_12\", extrapolation_factor = 0.1,                 l_limit = 0) par(mfrow = c(1, 2), mar = c(4, 4, 1, 0.5))  # Set grid of plot response_curve(models = fm, variable = \"bio_1\", extrapolation_factor = 0.5,                 las = 1) response_curve(models = fm, variable = \"bio_1\", extrapolation_factor = 0.5,                 show_lines = TRUE, ylab = \"\", las = 1) response_curve(models = fm, variable = \"bio_1\", show_lines = TRUE,                add_points = TRUE)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_exploration.html","id":"bivariate-response-curves","dir":"Articles","previous_headings":"Response curves","what":"Bivariate response curves","title":"4. Fit and Explore Selected Models","text":"previous curve plots showing responses individual variables. Exploring responses two variables time can help understand small differences models fitted distinct terms. Let’s see example :   previous plot, dashed lines represent limits data used model fitting, darker colors represent higher suitability. Let’s now compare two models check differences. Multiple bivariate response plots can put together single plot suitability bar legend used.   two models look similar, noticeable difference suitability values threshold extend farther variable bio_15 model 219. noticeable effects can seen bivariate representations fitted models include product terms. However, example, even small variation number type terms included model can change predictions look like.","code":"# First, let's check the terms in the models fitted ## Model 192 fm$Models$Model_192$Full_model$betas #>        bio_1        bio_7       bio_15   I(bio_1^2)   I(bio_7^2)  I(bio_15^2)  #> 11.572321659  0.215970079  0.369077970 -0.356605446 -0.020306099 -0.006200151  ## Model 219 fm$Models$Model_219$Full_model$betas #>         bio_1        bio_12        bio_15    I(bio_1^2)    I(bio_7^2)  #>  1.178814e+01  1.638570e-02  3.406100e-01 -3.625405e-01 -1.440450e-02  #>   I(bio_12^2)   I(bio_15^2)  #> -5.261860e-06 -5.623987e-03  # Example of a bivariate response plot  bivariate_response(models = fm, variable1 = \"bio_1\", variable2 = \"bio_15\",                     modelID = \"Model_192\") par(mfrow = c(1, 2), mar = c(4, 4, 2.5, 0.5))  # Bivariate response model 192  bivariate_response(models = fm, variable1 = \"bio_1\", variable2 = \"bio_15\",                     modelID = \"Model_192\", add_bar = FALSE, main = \"Model 192\")  # Bivariate response model 219  bivariate_response(models = fm, variable1 = \"bio_1\", variable2 = \"bio_15\",                     modelID = \"Model_219\", add_bar = FALSE, main = \"Model 219\",                     ylab = \"\")"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_exploration.html","id":"variable-importance","dir":"Articles","previous_headings":"","what":"Variable importance","title":"4. Fit and Explore Selected Models","text":"relative importance model predictors can calculated exploring deviance using var_importance() function. process starts fitting full model (maxnet glm), includes predictors. , function fits separate models excluding one predictor time, quantifies removal affects model performance. values contribution computed variables default, explorations terms also possible using argument by_terms (.e., model includes linear quadratic terms variable, process run separately). default, function runs single core. can enable parallel processing setting parallel = TRUE specifying number cores ncores. Note parallelization speeds computation many variables (e.g., >17) calibration dataset large (e.g., 15,000 total points). Variable importance computed selected models default:  function returns data.frame relative contribution variable. multiple models included fitted object, additional column identifies model.  can visualize variable importance using plot_importance() function. fitted_models object contains one selected model, contributions displayed boxplot, along mean contribution, number (N) fitted models predictor.   variable importance computed single model, plot displays bars instead boxes:   previous examples show important variables checking change models remove variable completely equation. compute contribution variable terms (.e., important keep distinct terms variable) set by_terms = TRUE:","code":"# Calculate variable importance imp <- variable_importance(models = fm)  # Calculating variable contribution for model 1 of 2 #   |======================================================================| 100% # Calculating variable contribution for model 2 of 2 #   |======================================================================| 100% imp #>   predictor contribution    Models #> 1     bio_1  0.567815429 Model_192 #> 2    bio_15  0.219231619 Model_192 #> 3     bio_7  0.212952953 Model_192 #> 4     bio_1  0.721574882 Model_219 #> 5    bio_15  0.248819713 Model_219 #> 6    bio_12  0.025950948 Model_219 #> 7     bio_7  0.003654457 Model_219 plot_importance(imp) # Calculate variable importance for a specific selected Model imp_192 <- variable_importance(models = fm, modelID = \"Model_192\",                                 progress_bar = FALSE)  # Plot variable contribution for model 192 plot_importance(imp_192, main = \"Variable importance: Model 192\") # Calculate variable importance for a specific selected Model imp_terms <- variable_importance(models = fm, by_terms = TRUE,                                   progress_bar = FALSE) #>  #> Calculating variable contribution for model 1 of 2 #>  #> Calculating variable contribution for model 2 of 2  # Check results imp_terms #>      predictor contribution    Models #> 1   I(bio_1^2)  0.330546900 Model_192 #> 2        bio_1  0.295001412 Model_192 #> 3       bio_15  0.209668355 Model_192 #> 4  I(bio_15^2)  0.156926301 Model_192 #> 5   I(bio_7^2)  0.006279520 Model_192 #> 6        bio_7  0.001577512 Model_192 #> 7   I(bio_1^2)  0.368476226 Model_219 #> 8        bio_1  0.332782286 Model_219 #> 9       bio_15  0.155708606 Model_219 #> 10 I(bio_15^2)  0.095186390 Model_219 #> 11 I(bio_12^2)  0.022722557 Model_219 #> 12      bio_12  0.021793617 Model_219 #> 13  I(bio_7^2)  0.003330318 Model_219  # Plot variable contribution for model 192 par(cex = 0.7, mar = c(3, 4, 2.5, 0.5))  # Set grid of plot plot_importance(imp_terms, main = \"Importance of variable terms\")"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_exploration.html","id":"model-evaluation-with-independent-data","dir":"Articles","previous_headings":"","what":"Model evaluation with independent data","title":"4. Fit and Explore Selected Models","text":"Selected models can evaluated using independent set presence records (used fitting selected models). approach especially useful new records become available. can useful assess models invasive species, model fitting usually done native areas transfers potential invasive areas. independent_evaluation() function computes omission rates (.e., proportion independent records predicted suitability threshold) partial ROC. function also assesses whether environmental conditions independent data analogous models fit using mobility oriented-parity metric (MOP; Cobos et al. 2024). Let’s use new_occ example independent data (provided package). dataset contains coordinates Myrcia hatschbachii sourced NeotropicTree (Oliveira-Filho, 2017), part data used fit models.  order evaluate predictive ability models analogous conditions independent data fitting conditions, environmental values need extracted new records. Let’s import raster variables extract values new_occ:  Finding non-analogous conditions independent records uncommon, especially regions outside model calibration areas. better illustrate case, let’s add three fake records, variables non-analogous values, either higher upper limit lower lower limit observed data used fit models.  Now, let’s evaluate models independent dataset (keep mind last three records fake):  output list three elements: evaluation metrics (omission rate pROC) selected model, well overall consensus. mop_results, list containing output MOP analysis comparing conditions independent data used fit models. happens perform_mop argument set TRUE (default). predictions continuous binary suitability independent records (default; return_predictions = TRUE). addition, MOP results included records.  selected models significant pROC values, show higher omission rates expected based 10% omission threshold used calibration (~35% independent records predicted unsuitable) . records, following MOP results provided: mop_distance: environmental distance (.e., dissimilarity) independent record nearest set conditions considered fit models. inside_range: whether environmental conditions location independent record fall within model fitting range. n_var_out: number variables independent record outside model fitting range. towards_low: names variables values lower minimum observed model fitting data. towards_high: names variables values higher maximum observed model fitting data.  Note two three fake records added new_data non-analogous environmental conditions. One falls location bio_7 bio_1 values lower minimum observed model fitting data, whereas bio_12 value higher maximum. Another record location bio_12 model fitting range, bio_1 exceeds upper limit.  set return_predictions = TRUE (default), function also returns predictions selected model general consensus:  results can help determine whether independent data incorporated calibration dataset re-run models. omission rates independent records high, pROC values non-significant, records show non-analogous environmental conditions, might good idea re-run models including independent records.","code":"# Import independent records data(\"new_occ\", package = \"kuenm2\")  # See data structure str(new_occ) #> Classes 'data.table' and 'data.frame':   82 obs. of  3 variables: #>  $ species: chr  \"Myrcia hatschbachii\" \"Myrcia hatschbachii\" \"Myrcia hatschbachii\" \"Myrcia hatschbachii\" ... #>  $ x      : num  -48.3 -49.1 -49.9 -49.4 -49.9 ... #>  $ y      : num  -25.2 -25 -24.5 -24.5 -24.8 ... #>  - attr(*, \".internal.selfref\")=<externalptr> # Import raster layers var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  # Extract variables to occurrences new_data <- extract_occurrence_variables(occ = new_occ, x = \"x\", y = \"y\",                                          raster_variables = var)  # See data structure str(new_data) #> 'data.frame':    82 obs. of  8 variables: #>  $ pr_bg   : num  1 1 1 1 1 1 1 1 1 1 ... #>  $ x       : num  -48.3 -49.1 -49.9 -49.4 -49.9 ... #>  $ y       : num  -25.2 -25 -24.5 -24.5 -24.8 ... #>  $ bio_1   : num  20.2 18 16.6 17.8 16.7 ... #>  $ bio_7   : num  16.7 18.2 19.9 19.4 20.1 ... #>  $ bio_12  : num  2015 1456 1526 1414 1578 ... #>  $ bio_15  : num  43.8 33.7 29.1 32.2 26.5 ... #>  $ SoilType: num  6 6 6 6 6 10 10 10 10 10 ... # Add some fake data beyond the limits of calibration ranges fake_data <- data.frame(\"pr_bg\" = c(1, 1, 1),                         \"x\" = c(NA, NA, NA),                         \"y\" = c(NA, NA, NA),                         \"bio_1\" = c(10, 15, 23),                         \"bio_7\" = c(12, 16, 20),                         \"bio_12\" = c(2300, 2000, 1000),                         \"bio_15\" = c(30, 40, 50),                         \"SoilType\" = c(1, 1, 1))  # Bind data new_data <- rbind(new_data, fake_data) # Evaluate models with independent data res_ind <- independent_evaluation(fitted_models = fm, new_data = new_data) res_ind$evaluation #>               Model consensus Omission_rate_at_10 Mean_AUC_ratio pval_pROC #> 1         Model_192      mean           0.3647059       1.167024         0 #> 2         Model_192    median           0.3764706       1.175520         0 #> 3         Model_219      mean           0.3882353       1.144704         0 #> 4         Model_219    median           0.3764706       1.145789         0 #> 5 General_consensus    median           0.3882353       1.163761         0 #> 6 General_consensus      mean           0.3764706       1.157164         0 # Show the mop results for the last 5 independent records res_ind$predictions$continuous[81:85 , c(\"mop_distance\", \"inside_range\",                                           \"n_var_out\", \"towards_low\",                                           \"towards_high\")] #>    mop_distance inside_range n_var_out  towards_low towards_high #> 81     7.753558         TRUE         0         <NA>         <NA> #> 82     6.622770         TRUE         0         <NA>         <NA> #> 83   157.782905        FALSE         3 bio_7, bio_1       bio_12 #> 84    21.045482         TRUE         0         <NA>         <NA> #> 85   183.905420        FALSE         2       bio_12        bio_1 # Show the continuous predictions for the last 5 independent records # Round to two decimal places round(res_ind$predictions$continuous[81:85, 1:6], 2) #>    Model_192.mean Model_192.median Model_219.mean Model_219.median #> 81           0.49             0.48           0.52             0.51 #> 82           0.44             0.44           0.45             0.42 #> 83           0.00             0.00           0.00             0.00 #> 84           0.95             0.96           0.76             0.78 #> 85           0.00             0.00           0.00             0.00 #>    General_consensus.median General_consensus.mean #> 81                     0.50                   0.50 #> 82                     0.44                   0.44 #> 83                     0.00                   0.00 #> 84                     0.94                   0.85 #> 85                     0.00                   0.00  # Show the binary predictions for the last 5 independent records res_ind$predictions$binary[81:85, 1:6] #>    Model_192.mean Model_192.median Model_219.mean Model_219.median #> 81              1                1              1                1 #> 82              1                1              1                1 #> 83              0                0              0                0 #> 84              1                1              1                1 #> 85              0                0              0                0 #>    General_consensus.mean General_consensus.median #> 81                      1                        1 #> 82                      1                        1 #> 83                      0                        0 #> 84                      1                        1 #> 85                      0                        0 # Reset plotting parameters par(original_par)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_exploration.html","id":"saving-a-fitted_models-object","dir":"Articles","previous_headings":"","what":"Saving a fitted_models object","title":"4. Fit and Explore Selected Models","text":"fitting best-performing models fit_selected(), can proceed predict models single multiple scenarios. object essentially list, users can save local directory using saveRDS(). Saving object facilitates loading back R session later using readRDS(). See example :","code":"# Set directory to save (here, in a temporary directory) dir_to_save <- file.path(tempdir())  # Save the data: saveRDS(fm, file.path(dir_to_save, \"fitted_models.rds\"))  # Import data fm <- readRDS(file.path(dir_to_save, \"fitted_models.rds\"))"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_predictions.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"5. Project Models to a Single Scenario","text":"selected models fit using fit_selected(), projections single multiple scenarios can performed. predict_selected() function designed projections single scenarios. predict using selected models, fitted_models object required. detailed information model fitting, please consult vignette Fit Explore Selected Models. fitted_models object generated vignette included example dataset within package. Let’s load . compare results, let’s import fitted_models object generated using GLM algorithm:","code":"#Load packages library(kuenm2) library(terra) #> terra 1.8.86  #Import calib_results_maxnet data(\"fitted_model_maxnet\", package = \"kuenm2\") #Print calibration result fitted_model_maxnet #> fitted_models object summary #> ============================ #> Species: Myrcia hatschbachii  #> Algortihm: maxnet  #> Number of fitted models: 2  #> Only full models fitted, no replicates #Import calib_results_maxnet data(\"fitted_model_glm\", package = \"kuenm2\") #Print calibration result fitted_model_glm #> fitted_models object summary #> ============================ #> Species: Myrcia hatschbachii  #> Algortihm: glm  #> Number of fitted models: 1  #> Only full models fitted, no replicates"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_predictions.html","id":"predict-selected-models-for-a-single-scenario","dir":"Articles","previous_headings":"","what":"Predict Selected Models for a Single Scenario","title":"5. Project Models to a Single Scenario","text":"predict selected models single scenario, need fitted_models object corresponding predictor variables. predictor variables can provided either SpatRaster data.frame. names variables (columns data.frame) must precisely match used model calibration used running PCA (do_pca = TRUE set prepare_data() function; see Prepare Data Model Calibration details).","code":""},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_predictions.html","id":"predict-to-spatraster","dir":"Articles","previous_headings":"Predict Selected Models for a Single Scenario","what":"Predict to SpatRaster","title":"5. Project Models to a Single Scenario","text":"Let’s use raster variables used prepare data calibrate models. included example data within package:  Let’s check variables used calibrate models. available calibration_data element object: first column, “pr_bg”, indicates presence (1) absence (0) records, columns represent environmental variables. case, variables bio_1, bio_7, bio_12, bio_15, SoilType. variables present SpatRaster (var) imported. Therefore, can now predict models raster. Let’s begin predicting maxnet model: default, function computes consensus metrics (mean, median, range, standard deviation) model across replicates (one model selected), well general consensus across models. case, output list containing SpatRaster predictions replicate, along consensus results model overall general consensus: Let’s plot general consensus:  can also plot results replicate consensus model:    comparison, let’s predict GLM model:","code":"# Import raster layers var <- rast(system.file(\"extdata\", \"Current_variables.tif\", package = \"kuenm2\")) #Plot raster layers plot(var) # Variables used to calibrate maxnet models colnames(fitted_model_maxnet$calibration_data) #> [1] \"pr_bg\"    \"bio_1\"    \"bio_7\"    \"bio_12\"   \"bio_15\"   \"SoilType\"  #Variables used to calibrate glm models colnames(fitted_model_glm$calibration_data) #> [1] \"pr_bg\"    \"bio_1\"    \"bio_7\"    \"bio_12\"   \"bio_15\"   \"SoilType\" p_maxnet <- predict_selected(models = fitted_model_maxnet,                               new_variables = var,                              progress_bar = FALSE) #See objects in the output of predict_selected names(p_maxnet) #> [1] \"Model_192\"         \"Model_219\"         \"General_consensus\" plot(p_maxnet$General_consensus) #Predictions for each replicate from model 192 plot(p_maxnet$Model_192$Replicates) #Consensus across each replicate from model 192 plot(p_maxnet$Model_192$Model_consensus) # Predict glm model p_glm <- predict_selected(models = fitted_model_glm,                            new_variables = var,                           progress_bar = FALSE) #See selected models that were predicted names(p_glm) #> [1] \"Model_85\"          \"General_consensus\"  #Compare general consensus (mean) between maxnet and glm par(mfrow= c(1, 2)) #Set grid to plot plot(p_maxnet$General_consensus$mean, main = \"Maxnet\") plot(p_glm$General_consensus, main = \"GLM\") on.exit() #Reinitiate grid"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_predictions.html","id":"predict-to-data-frame","dir":"Articles","previous_headings":"Predict Selected Models for a Single Scenario","what":"Predict to data.frame","title":"5. Project Models to a Single Scenario","text":"Instead SpatRaster, can also predict models data.frame stores variable values. see example, let’s convert raster variables var data.frame: Note column stores values variable. Let’s predict Maxnet models data.frame: Now, instead SpatRaster objects, function returns data.frame objects predictions:","code":"var_df <- as.data.frame(var) head(var_df) #>       bio_1    bio_7 bio_12   bio_15 SoilType #> 11 22.77717 18.12400   1180 48.03594       NA #> 12 22.76711 17.74400   1191 49.31194       10 #> 13 22.68580 17.46575   1206 51.51922       10 #> 14 22.50121 17.84525   1228 53.90265       10 #> 15 22.07609 18.14125   1254 54.10397       10 #> 16 21.88485 18.80800   1276 54.07279       10 p_df <- predict_selected(models = fitted_model_maxnet,                           new_variables = var_df, #Now, a data.frame                          progress_bar = FALSE) #Results by replicate of the model 192 head(p_df$Model_192$Replicates) #>   Partition_1  Partition_2  Partition_3  Partition_4   Full_model #> 1 0.006521501 0.0006209852 0.0005883615 9.831561e-05 9.526648e-08 #> 2 0.006446437 0.0005356316 0.0005713501 9.009486e-05 8.993584e-08 #> 3 0.006233583 0.0003396879 0.0004975279 6.967025e-05 8.559934e-08 #> 4 0.005797668 0.0001458500 0.0003605775 4.303576e-05 8.306910e-08 #> 5 0.008513515 0.0002034105 0.0006532983 8.529550e-05 4.220225e-07 #> 6 0.009240381 0.0001753492 0.0006784553 9.035171e-05 6.540127e-07  #Consensus across replicates of the model 192 head(p_df$Model_192$Model_consensus) #>         median       range        mean       stdev #> 1 0.0005883615 0.006521406 0.001565852 0.002784420 #> 2 0.0005356316 0.006446347 0.001528721 0.002761027 #> 3 0.0003396879 0.006233497 0.001428111 0.002693874 #> 4 0.0001458500 0.005797585 0.001269443 0.002535186 #> 5 0.0002034105 0.008513093 0.001891188 0.003710540 #> 6 0.0001753492 0.009239727 0.002037038 0.004035351  #General consensus across all models head(p_df$General_consensus) #>         median       range        mean       stdev #> 1 0.0005892047 0.006521444 0.001506370 0.002502583 #> 2 0.0005348599 0.006446378 0.001477821 0.002497636 #> 3 0.0003390597 0.006233519 0.001390363 0.002461362 #> 4 0.0001459093 0.005797595 0.001250202 0.002350678 #> 5 0.0002032855 0.008513101 0.001870336 0.003456477 #> 6 0.0001758720 0.009239727 0.002031620 0.003796839"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_predictions.html","id":"binarize-models","dir":"Articles","previous_headings":"Predict Selected Models for a Single Scenario","what":"Binarize Models","title":"5. Project Models to a Single Scenario","text":"fitted_models object stores thresholds can used binarize models suitable unsuitable areas. thresholds correspond omission error rate used model selection (e.g., 5% 10%). can access omission error rate used calculate thresholds directly object: models, 10% omission error rate used calculate thresholds. means predictions binarized, approximately 10% presence records used model calibration fall areas classified unsuitable. thresholds summarized two ways: mean median across replicates model, consensus mean median across selected models (one model selected). Let’s check thresholds general consensus: Let’s use thresholds binarize models (functionality available predicting SpatRaster):","code":"#Get omission error used to select models and calculate the thesholds ## For maxnet model fitted_model_maxnet$omission_rate #> [1] 10  ## For glm model fitted_model_glm$omission_rate #> [1] 10 #For maxnet fitted_model_maxnet$thresholds$consensus #> $mean #> [1] 0.3095083 #>  #> $median #> [1] 0.259534  #For glm fitted_model_glm$thresholds$consensus #> $mean #> [1] 0.1204713 #>  #> $median #> [1] 0.1204713 #Get the thersholds for models (general consensus) thr_mean_maxnet <- fitted_model_maxnet$thresholds$consensus$mean #Maxnet thr_mean_glm <- fitted_model_glm$thresholds$consensus$mean #glm  #Binarize models mean_maxnet_bin <- (p_maxnet$General_consensus$mean > thr_mean_maxnet) * 1 mean_glm_bin <- (p_glm$General_consensus > thr_mean_glm) * 1  #Compare results par(mfrow= c(1, 2)) #Set grid to plot plot(mean_maxnet_bin, main = \"Maxnet\") plot(mean_glm_bin, main = \"GLM\") on.exit() #Reinitiate grid"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_predictions.html","id":"clamping-variables","dir":"Articles","previous_headings":"Predict Selected Models for a Single Scenario","what":"Clamping Variables","title":"5. Project Models to a Single Scenario","text":"default, predictions performed free extrapolation (extrapolation_type = \"E\"). can problematic peak suitability occurs extremes predictor’s range. example, let’s examine response curve Maxnet model bio_7 (Temperature Annual Range):  Note higher suitability occurs low values temperature range. However, lower limit calibration data used fit models (dashed line) 15.7ºC. premise suitability increase stabilize lower values bio_7 extrapolation model (area left dashed line). ’s possible suitability begin decrease extremely low values, rendering extrapolation inaccurate, calibration data insufficient model predict . One way address clamping variables. means values outside calibration range (lower value upper value) set respective lower upper limits calibration range. example, calibration data Maxnet models, lower upper limits bio_7 15.7ºC 23.3ºC, respectively: observe effect clamping variable, let’s create hypothetical (extreme) scenario bio_7 extremely low values:  Let’s predict Maxnet models new scenario free extrapolation (extrapolation_type = \"E\") clamped variables (extrapolation_type = \"EC\"):  Note clamp variables, regions extremely low values (hypothetical) bio_7 exhibit lower predicted suitabilities compared free extrapolation allowed. default, extrapolation_type = \"EC\" set, predictor variables clamped. can specify variables clamp using var_to_clamp argument.","code":"response_curve(models = fitted_model_maxnet, variable = \"bio_7\",                 extrapolation_factor = 1) range(fitted_model_maxnet$calibration_data$bio_7) #> [1] 15.71120 23.30475 #From bio_7, reduce values new_bio7 <- var$bio_7 - 3 #Create new scenario new_var <- var #Replace bio_7 with new_bio7 in this scenario new_var$bio_7 <- new_bio7  #Plot the differences par(mfrow = c(1,2)) plot(var$bio_7, main = \"Original bio_7\", range = c(5, 25)) plot(new_var$bio_7, main = \"New bio_7\", range = c(5, 25)) on.exit() #Reinitiate grid #Predict to hypothetical scenario with free extrapolation p_free_extrapolation <- predict_selected(models = fitted_model_maxnet,                                           new_variables = new_var, #New scenario                                          consensus = \"mean\",                                          extrapolation_type = \"E\", #Free extrapolation (Default)                                          progress_bar = FALSE)  #Predict to hypothetical scenario with clamping p_clamping <- predict_selected(models = fitted_model_maxnet,                                 new_variables = new_var, #New scenario                                consensus = \"mean\",                                extrapolation_type = \"EC\", #Extrapolation with clamping                                progress_bar = FALSE)  #Get and see differences p_difference <- p_free_extrapolation$General_consensus$mean - p_clamping$General_consensus$mean  #Plot the differences par(mfrow = c(2,2)) plot(p_free_extrapolation$General_consensus$mean, main = \"Free extrapolation\",      zlim = c(0, 1)) plot(p_clamping$General_consensus$mean, main = \"Clamping\",      zlim = c(0, 1)) plot(p_difference, main = \"Difference\") plot(new_bio7, main = \"Hypothetical bio_7\", type = \"interval\") on.exit() #Reinitiate grid"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_predictions.html","id":"no-extrapolation","dir":"Articles","previous_headings":"Predict Selected Models for a Single Scenario","what":"No Extrapolation","title":"5. Project Models to a Single Scenario","text":"rigorous approach predict extrapolation, regions outside limits calibration data assigned suitability value 0. Let’s predict Maxnet models using hypothetical scenario created previous step observe difference:  n example, large portion predicted area shows zero suitability. , hypothetical scenario, much region bio_7 values lower calibration data, minimum 15ºC. Suitability values greater zero predicted areas bio_7 falls within range calibration data. default, extrapolation_type = \"NE\" set, predictor variables considered process. can specify subset variables considered extrapolation using var_to_clamp argument.","code":"#Predict to hypothetical scenario with no extrapolation p_no_extrapolation <- predict_selected(models = fitted_model_maxnet,                                         new_variables = new_var, #New scenario                                        consensus = \"mean\",                                        extrapolation_type = \"NE\", #No extrapolation                                        progress_bar = FALSE) #Plot the differences par(mfrow = c(2,2)) plot(p_free_extrapolation$General_consensus$mean, main = \"Free extrapolation\",      zlim = c(0, 1)) plot(p_clamping$General_consensus$mean, main = \"Clamping\",      zlim = c(0, 1)) plot(p_no_extrapolation$General_consensus$mean, main = \"No extrapolation\",      zlim = c(0, 1)) plot(new_bio7, main = \"Hypothetical bio_7\", type = \"interval\") on.exit() #Reinitiate grid"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_predictions.html","id":"output-type","dir":"Articles","previous_headings":"Predict Selected Models for a Single Scenario","what":"Output Type","title":"5. Project Models to a Single Scenario","text":"Maximum entropy models (maxnet) produce four different types output predictions: raw, cumulative, logistic, cloglog. described Merow et al. 2013 Phillips et al. 2017. four output types monotonically related. Therefore, rank-based metrics model fit (e.g., omission rate partial ROC) identical. However, output types different scaling, leads distinct interpretations visually different prediction maps. Raw (exponential) output interpreted Relative Occurrence Rate (ROR). ROR sums 1 predicted calibration data. Cumulative output assigns location sum raw values less equal raw value location, rescales range 0 100. Cumulative output can interpreted terms omission rate thresholding value c predict suitable/unsuitable cell omit approximately c% presences. Cloglog output (Default) transforms raw values scale relative suitability ranging 0 1, using logistic transformation based user-specified parameter ‘τ\\tau’, represents probability presence ‘average’ presence locations. context, tau value defaults τ≈0.632\\tau \\approx 0.632. Logistic output similar Cloglog, assumes τ=0.5\\tau = 0.5. Let’s examine differences four output types Maxnet models:","code":"p_cloglog <- predict_selected(models = fitted_model_maxnet, new_variables = var,                                type = \"cloglog\", progress_bar = FALSE) p_logistic <- predict_selected(models = fitted_model_maxnet, new_variables = var,                                type = \"logistic\", progress_bar = FALSE) p_cumulative <- predict_selected(models = fitted_model_maxnet, new_variables = var,                                type = \"cumulative\", progress_bar = FALSE) p_raw <- predict_selected(models = fitted_model_maxnet, new_variables = var,                                type = \"raw\", progress_bar = FALSE)  #Plot the differences par(mfrow = c(2,2)) plot(p_cloglog$General_consensus$mean, main = \"Cloglog (Default)\",      zlim = c(0, 1)) plot(p_logistic$General_consensus$mean, main = \"Logistic\",      zlim = c(0, 1)) plot(p_cumulative$General_consensus$mean, main = \"Cumulative\",      zlim = c(0, 1)) plot(p_raw$General_consensus$mean, main = \"Raw\",      zlim = c(0, 1)) on.exit() #Reinitiate grid"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_predictions.html","id":"saving-predictions","dir":"Articles","previous_headings":"","what":"Saving Predictions","title":"5. Project Models to a Single Scenario","text":"can save predictions disk setting write_files = TRUE. option enabled, must provide directory path out_dir argument. new_variables SpatRaster, function save output files GeoTIFF (.tif) files. new_variables data.frame, function save output files Comma Separated Value (.csv) files. Alternatively, can use writeRaster() save specific output predictions manually. example, save mean layer general consensus results:","code":"p_save <- predict_selected(models = fitted_model_maxnet,                             new_variables = var,                             write_files = TRUE, #To save to the disk                            write_replicates = TRUE, #To save predictions for each replicate                            out_dir = tempdir(), #A path to save the resuls (here, the temporary directory)                            progress_bar = FALSE) writeRaster(p_maxnet$General_consensus$mean,              filename = file.path(tempdir(), \"Mean_consensus.tif\"))"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_projections.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"6. Project Models to Multiple Scenarios","text":"selected models fit using fit_selected(), projections single multiple scenarios can performed. project_selected() function designed projections multiple scenarios. project using selected models, fitted_models object required. detailed information model fitting, please consult vignette Fit Explore Selected Models. fitted_models object generated vignette included example dataset within package. Let’s load .","code":"#Load packages library(kuenm2) library(terra) #> terra 1.8.86  #Import calib_results_maxnet data(\"fitted_model_maxnet\", package = \"kuenm2\") #Print calibration result fitted_model_maxnet #> fitted_models object summary #> ============================ #> Species: Myrcia hatschbachii  #> Algortihm: maxnet  #> Number of fitted models: 2  #> Only full models fitted, no replicates"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_projections.html","id":"pre-processing-raster-predictors","dir":"Articles","previous_headings":"","what":"Pre-processing raster predictors","title":"6. Project Models to Multiple Scenarios","text":"predicting models single scenario requires single SpatRaster object containing predictor variables (detailed Predict models single scenario), projecting models multiple scenarios necessitates folder stores predictor variables scenario. folders must organized specific hierarchical manner: root directory contain nested folders representing different scenarios, raster variables stored within. first level inside root folder, subfolders correspond distinct time periods (e.g., future years like “2070” “2100,” past periods “Mid-holocene” “LGM”). Within period folder, applicable, include subfolders emission scenario (e.g., “ssp126”, “ssp585”). Finally, within emission scenario time period folder, include separate folder General Circulation Model (GCM) (e.g., “BCC-CSM2-MR”, “MIROC6”). structured organization enables function automatically access process data according period, emission scenario, GCM.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_projections.html","id":"organize-and-structure-future-climate-variables-from-worldclim","dir":"Articles","previous_headings":"Pre-processing raster predictors","what":"Organize and structure future climate variables from WorldClim","title":"6. Project Models to Multiple Scenarios","text":"package provides function import future climate variables downloaded WorldClim (version 2.1). function renames files organizes folders categorized period/year, emission scenario (Shared Socioeconomic Pathways; SSPs), General Circulation Model (GCM). simplifies preparation climate data, ensuring required variables properly structured modeling projections. use function, download future raster variables WorldClim 2.1 save within folder. rename files variables, function relies patterns provided original files work properly. package also provides example raw variables downloaded WorldClim 2.1. example includes bioclimatic predictions periods “2041-2060” “2081-2100”, two SSPs (125 585) two GCMs (ACCESS-CM2 MIROC6), 10 arc-minutes resolution. Note variables folder retain original names provided WorldClim. can download variables directly WorldClim using geodata R package: Let’s check variables inside “geodata_dir” folder: Now, can organize structure files using organize_future_worldclim function.","code":"# See raster files with future predictors provided as example # The data is located in the \"inst/extdata\" folder. in_dir <- system.file(\"extdata\", package = \"kuenm2\") list.files(in_dir) #>  [1] \"bias_file.tif\"                                  #>  [2] \"CHELSA_LGM_CCSM4.tif\"                           #>  [3] \"CHELSA_LGM_CNRM-CM5.tif\"                        #>  [4] \"CHELSA_LGM_FGOALS-g2.tif\"                       #>  [5] \"CHELSA_LGM_IPSL-CM5A-LR.tif\"                    #>  [6] \"CHELSA_LGM_MIROC-ESM.tif\"                       #>  [7] \"CHELSA_LGM_MPI-ESM-P.tif\"                       #>  [8] \"CHELSA_LGM_MRI-CGCM3.tif\"                       #>  [9] \"Current_CHELSA.tif\"                             #> [10] \"Current_variables.tif\"                          #> [11] \"m.gpkg\"                                         #> [12] \"wc2.1_10m_bioc_ACCESS-CM2_ssp126_2041-2060.tif\" #> [13] \"wc2.1_10m_bioc_ACCESS-CM2_ssp126_2081-2100.tif\" #> [14] \"wc2.1_10m_bioc_ACCESS-CM2_ssp585_2041-2060.tif\" #> [15] \"wc2.1_10m_bioc_ACCESS-CM2_ssp585_2081-2100.tif\" #> [16] \"wc2.1_10m_bioc_MIROC6_ssp126_2041-2060.tif\"     #> [17] \"wc2.1_10m_bioc_MIROC6_ssp126_2081-2100.tif\"     #> [18] \"wc2.1_10m_bioc_MIROC6_ssp585_2041-2060.tif\"     #> [19] \"wc2.1_10m_bioc_MIROC6_ssp585_2081-2100.tif\"     #> [20] \"world.gpkg\" #Install geodata if necessary if(!require(\"geodata\")){   install.packages(\"geodata\") } #Load geodata library(geodata) #Create folder to save the raster files #Here, in a temporary directory geodata_dir <- file.path(tempdir(), \"Future_worldclim\") dir.create(geodata_dir) #Define GCMs, SSPs and time periods gcms <- c(\"ACCESS-CM2\", \"MIROC6\") ssps <- c(\"126\", \"585\") periods <- c(\"2041-2060\", \"2061-2080\") #Create a grid of combination of periods, ssps and gcms g <- expand.grid(\"period\" = periods, \"ssps\" = ssps, \"gcms\" = gcms) g #Each line is a specific scenario for future #Loop to download variables for each scenario lapply(1:nrow(g), function(i){   cmip6_world(model = g$gcms[i],                ssp = g$ssps[i],                time = g$period[i],                var = \"bioc\",                res = 10, path = geodata_dir) }) #It will take a while... list.files(geodata_dir, recursive = TRUE) #> [1] \"climate/wc2.1_10m/wc2.1_10m_bioc_ACCESS-CM2_ssp126_2041-2060.tif\" #> [2] \"climate/wc2.1_10m/wc2.1_10m_bioc_ACCESS-CM2_ssp126_2061-2080.tif\" #> [3] \"climate/wc2.1_10m/wc2.1_10m_bioc_ACCESS-CM2_ssp585_2041-2060.tif\" #> [4] \"climate/wc2.1_10m/wc2.1_10m_bioc_ACCESS-CM2_ssp585_2061-2080.tif\" #> [5] \"climate/wc2.1_10m/wc2.1_10m_bioc_MIROC6_ssp126_2041-2060.tif\"     #> [6] \"climate/wc2.1_10m/wc2.1_10m_bioc_MIROC6_ssp126_2061-2080.tif\"     #> [7] \"climate/wc2.1_10m/wc2.1_10m_bioc_MIROC6_ssp585_2041-2060.tif\"     #> [8] \"climate/wc2.1_10m/wc2.1_10m_bioc_MIROC6_ssp585_2061-2080.tif\"  #>  #> #Set climate as input directory #> in_dir <- file.path(geodata_dir, \"climate\")"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_projections.html","id":"format-for-renaming","dir":"Articles","previous_headings":"Pre-processing raster predictors > Organize and structure future climate variables from WorldClim","what":"Format for renaming","title":"6. Project Models to Multiple Scenarios","text":"important argument name_format, defines format renaming variables. names variables SpatRaster must precisely match used model calibration running PCA (do_pca = TRUE set prepare_data() function; see Prepare Data Model Calibration details). Therefore, variables used calibrate models named “bio_1”, “bio_2”, etc., variables future raster layers must also named “bio_1”, “bio_2”, etc. However, variables different pattern, starting uppercase letters using zeros single-digit numbers (e.g., “Bio_01”, “Bio_02”, etc.), must named “Bio_01”, “Bio_02”, etc. function provides four options: \"bio_\": Variables renamed bio_1, bio_2, bio_3, bio_10, etc. \"bio_0\": Variables renamed bio_01, bio_02, bio_03, bio_10, etc. \"Bio_\": Variables renamed Bio_1, Bio_2, Bio_3, Bio_10, etc. \"Bio_0\": Variables renamed Bio_01, Bio_02, Bio_03, Bio_10, etc. Let’s check variables named fitted_model: variables follows standards first option (\"bio_\").","code":"fitted_model_maxnet$continuous_variables #> [1] \"bio_1\"  \"bio_7\"  \"bio_12\" \"bio_15\""},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_projections.html","id":"fixed-variables","dir":"Articles","previous_headings":"Pre-processing raster predictors > Organize and structure future climate variables from WorldClim","what":"Fixed variables","title":"6. Project Models to Multiple Scenarios","text":"predicting times, can assume variables static (.e., remain unchanged projected scenarios). fixed_variables argument allows append static variables alongside bioclimatic variables. , let’s assume soilType remain static future scenarios:","code":"# Import raster layers (same used to calibrate and fit final models) var <- rast(system.file(\"extdata\", \"Current_variables.tif\", package = \"kuenm2\")) #Get soilType soiltype <- var$SoilType"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_projections.html","id":"organize-and-structure-worldclim-files","dir":"Articles","previous_headings":"Pre-processing raster predictors > Organize and structure future climate variables from WorldClim","what":"Organize and structure WorldClim files","title":"6. Project Models to Multiple Scenarios","text":"Now, let’s organize WorldClim files organize_future_worldclim() function: can check files structured hierarchically nested folders using dir_tree() function fs package: organizing variables, next step create prepared_projection object.","code":"#Create folder to save structured files out_dir_future <- file.path(tempdir(), \"Future_raw\") #Here, in a temporary directory #Organize organize_future_worldclim(input_dir = in_dir, #Path to the raw variables from WorldClim                           output_dir = out_dir_future,                            name_format = \"bio_\", #Name format                           fixed_variables = var$SoilType) #Static variables #>   |                                                                              |                                                                      |   0%  |                                                                              |=========                                                             |  12%  |                                                                              |==================                                                    |  25%  |                                                                              |==========================                                            |  38%  |                                                                              |===================================                                   |  50%  |                                                                              |============================================                          |  62%  |                                                                              |====================================================                  |  75%  |                                                                              |=============================================================         |  88%  |                                                                              |======================================================================| 100% #>  #> Variables successfully organized in directory: #> /tmp/RtmpH6PVxF/Future_raw  # Check files organized dir(out_dir_future, recursive = TRUE) #> [1] \"2041-2060/ssp126/ACCESS-CM2/Variables.tif\" #> [2] \"2041-2060/ssp126/MIROC6/Variables.tif\"     #> [3] \"2041-2060/ssp585/ACCESS-CM2/Variables.tif\" #> [4] \"2041-2060/ssp585/MIROC6/Variables.tif\"     #> [5] \"2081-2100/ssp126/ACCESS-CM2/Variables.tif\" #> [6] \"2081-2100/ssp126/MIROC6/Variables.tif\"     #> [7] \"2081-2100/ssp585/ACCESS-CM2/Variables.tif\" #> [8] \"2081-2100/ssp585/MIROC6/Variables.tif\" #Install package if necessary if(!require(\"fs\")){   install.packages(\"fs\") } dir_tree(out_dir_future) #> Temp\\RtmpkhmGWN/Future_raw #> ├── 2041-2060 #> │   ├── ssp126 #> │   │   ├── ACCESS-CM2 #> │   │   │   └── Variables.tif #> │   │   └── MIROC6 #> │   │       └── Variables.tif #> │   └── ssp585 #> │       ├── ACCESS-CM2 #> │       │   └── Variables.tif #> │       └── MIROC6 #> │           └── Variables.tif #> └── 2081-2100 #>     ├── ssp126 #>     │   ├── ACCESS-CM2 #>     │   │   └── Variables.tif #>     │   └── MIROC6 #>     │       └── Variables.tif #>     └── ssp585 #>         ├── ACCESS-CM2 #>         │   └── Variables.tif #>         └── MIROC6 #>             └── Variables.tif"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_projections.html","id":"preparation-of-data-for-model-projections","dir":"Articles","previous_headings":"","what":"Preparation of data for model projections","title":"6. Project Models to Multiple Scenarios","text":"Now, let’s prepare data model projections across multiple scenarios, storing paths rasters representing scenario. contrast predict_selected(), requires SpatRaster object, need paths folders raster files stored. includes variables present time, used calibrate fit models. Currently, future climate files. present-day predictor variables must reside root directory processed future variables. Let’s copy rasters used model calibration fitting folder: Now, can prepare data projections. addition storing paths variables scenario, function also verifies variables used fit final models available across scenarios. perform check, need provide either fitted_models object intend use projection simply variable names. strongly suggest using fitted_models object minimize projection errors. also need define root directory containing scenarios projection (present, past, /future), along additional information regarding time periods, SSPs, GCMs. print projection_data object, summarizes scenarios predict also shows root directory predictor rasters stored: check structure prepared_projection object, can see ’s list containing: Paths variables representing distinct scenarios subfolders. pattern used identify format raster files within folders (default, *.tif). names predictors. list class prcomp Principal Component Analysis (PCA) performed set variables prepare_data().","code":"# Create a \"Current_raw\" folder in a temporary directory # and copy the rawvariables there. out_dir_current <- file.path(tempdir(), \"Current_raw\") dir.create(out_dir_current, recursive = TRUE)  # Save current variables in temporary directory terra::writeRaster(var, file.path(out_dir_current, \"Variables.tif\"))  #Check folder list.files(out_dir_current) #> [1] \"Variables.tif\" # Prepare projections using fitted models to check variables pr <- prepare_projection(models = fitted_model_maxnet,                          present_dir = out_dir_current, #Directory with present-day variables                          past_dir = NULL, #NULL because we won't project to the past                          past_period = NULL, #NULL because we won't project to the past                          past_gcm = NULL, #NULL because we won't project to the past                          future_dir = out_dir_future, #Directory with future variables                          future_period = c(\"2041-2060\", \"2081-2100\"),                          future_pscen = c(\"ssp126\", \"ssp585\"),                          future_gcm = c(\"ACCESS-CM2\", \"MIROC6\")) pr #> projection_data object summary #> ============================= #> Variables prepared to project models for Present and Future  #> Future projections contain the following periods, scenarios and GCMs: #>   - Periods: 2041-2060 | 2081-2100  #>   - Scenarios: ssp126 | ssp585  #>   - GCMs: ACCESS-CM2 | MIROC6  #> All variables are located in the following root directory: #> /tmp/RtmpH6PVxF #Open prepared_projection in a new window View(pr)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_projections.html","id":"project-selected-models-to-multiple-scenarios","dir":"Articles","previous_headings":"","what":"Project selected models to multiple scenarios","title":"6. Project Models to Multiple Scenarios","text":"preparing data, can use project_selected() function predict selected models across multiple scenarios specified prepare_projections: function returns model_projections object. object similar prepared_data object, storing information predicted scenarios folder resulting projection rasters saved. Note results saved hierarchically nested subfolders, representing distinct scenario. root directory, function also saves file named “Projection_paths.RDS”, model_projections object. object can imported R using readRDS(file.path(out_dir, \"Projection_paths.RDS\")). default, scenario, function computes consensus metrics (mean, median, range, standard deviation) model across replicates (one model selected), well general consensus across models. Note selected model can also replicates. default, function output individual replicates unless write_replicates = TRUE set. important write replicates intend compute variability across using projection_variability(). details, check vignette Explore Variability Uncertainty Projections. function accepts several parameters control predictions predict_selected(), consensus compute, extrapolation type (free extrapolation (E), extrapolation clamping (EC), extrapolation (NE)), variables clamp, format prediction values (raw, cumulative, logistic, default cloglog). details, consult vignette Predict models single scenario.","code":"## Create a folder to save projection results #Here, in a temporary directory out_dir <- file.path(tempdir(), \"Projection_results/maxnet\") dir.create(out_dir, recursive = TRUE)  ## Project selected models to multiple scenarios p <- project_selected(models = fitted_model_maxnet,                        projection_data = pr,                       out_dir = out_dir,                        write_replicates = TRUE,                       progress_bar = FALSE) #Do not print progress bar print(p) #> model_projections object summary #> ================================ #> Models projected for Present and Future  #> Future projections contain the following periods, scenarios and GCMs: #>   - Periods: 2041-2060 | 2081-2100  #>   - Scenarios:   #>   - GCMs: ACCESS-CM2 | MIROC6  #> All raster files containing the projection results are located in the following root directory: #>  /tmp/RtmpH6PVxF/Projection_results/maxnet dir_tree(out_dir) #> Temp\\Projection_results/maxnet #> ├── Future #> │   ├── 2041-2060 #> │   │   ├── ssp126 #> │   │   │   ├── ACCESS-CM2 #> │   │   │   │   ├── General_consensus.tif #> │   │   │   │   ├── Model_192_consensus.tif #> │   │   │   │   ├── Model_192_replicates.tif #> │   │   │   │   ├── Model_219_consensus.tif #> │   │   │   │   └── Model_219_replicates.tif #> │   │   │   └── MIROC6 #> │   │   │       ├── General_consensus.tif #> │   │   │       ├── Model_192_consensus.tif #> │   │   │       ├── Model_192_replicates.tif #> │   │   │       ├── Model_219_consensus.tif #> │   │   │       └── Model_219_replicates.tif #> │   │   └── ssp585 #> │   │       ├── ACCESS-CM2 #> │   │       │   ├── General_consensus.tif #> │   │       │   ├── Model_192_consensus.tif #> │   │       │   ├── Model_192_replicates.tif #> │   │       │   ├── Model_219_consensus.tif #> │   │       │   └── Model_219_replicates.tif #> │   │       └── MIROC6 #> │   │           ├── General_consensus.tif #> │   │           ├── Model_192_consensus.tif #> │   │           ├── Model_192_replicates.tif #> │   │           ├── Model_219_consensus.tif #> │   │           └── Model_219_replicates.tif #> │   └── 2081-2100 #> │       ├── ssp126 #> │       │   ├── ACCESS-CM2 #> │       │   │   ├── General_consensus.tif #> │       │   │   ├── Model_192_consensus.tif #> │       │   │   ├── Model_192_replicates.tif #> │       │   │   ├── Model_219_consensus.tif #> │       │   │   └── Model_219_replicates.tif #> │       │   └── MIROC6 #> │       │       ├── General_consensus.tif #> │       │       ├── Model_192_consensus.tif #> │       │       ├── Model_192_replicates.tif #> │       │       ├── Model_219_consensus.tif #> │       │       └── Model_219_replicates.tif #> │       └── ssp585 #> │           ├── ACCESS-CM2 #> │           │   ├── General_consensus.tif #> │           │   ├── Model_192_consensus.tif #> │           │   ├── Model_192_replicates.tif #> │           │   ├── Model_219_consensus.tif #> │           │   └── Model_219_replicates.tif #> │           └── MIROC6 #> │               ├── General_consensus.tif #> │               ├── Model_192_consensus.tif #> │               ├── Model_192_replicates.tif #> │               ├── Model_219_consensus.tif #> │               └── Model_219_replicates.tif #> ├── Present #> │   └── Present #> │       ├── General_consensus.tif #> │       ├── Model_192_consensus.tif #> │       ├── Model_192_replicates.tif #> │       ├── Model_219_consensus.tif #> │       └── Model_219_replicates.tif #> └── Projection_paths.RDS"},{"path":"https://marlonecobos.github.io/kuenm2/articles/model_projections.html","id":"import-rasters-resulting-from-projections","dir":"Articles","previous_headings":"","what":"Import rasters resulting from projections","title":"6. Project Models to Multiple Scenarios","text":"model_projections object stores paths resultant rasters. import results, can use import_projections() function. default, imports consensus metrics (“median”, “range”, “mean”, “stdev”) scenarios (time periods, SSPs, GCMs) available model_projections object. Let’s import mean scenarios:  Alternatively, can import results specific scenarios. example, let’s import results “2041-2060” time period SSP 126:  model_projections object, can compute changes suitable areas scenarios (see projection_changes function), explore variance stemming replicates, model parameterizations, GCMs (see projection_variability), perform analysis extrapolation risks (see projection_mop). details, check vignette Explore Variability Uncertainty Projections.","code":"#Import mean of each projected scenario p_mean <- import_projections(projection = p, consensus = \"mean\") #Plot all scenarios plot(p_mean, cex.main = 0.8) p_2060_ssp126 <- import_projections(projection = p, consensus = \"mean\",                                      present = FALSE, #Do not import present projections                                     future_period = \"2041-2060\",                                     future_pscen = \"ssp126\") #Plot all scenarios plot(p_2060_ssp126, cex.main = 0.8)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"2. Prepare Data for Model Calibration","text":"Description Getting ready Example data First steps preparing data Using pre-processed data Comparative histograms Distribution data models Similarity assessment partitions Custom formulas Using bias file Internal PCA External PCA ENMeval partitions flexsdm partitions Saving prepared data","code":""},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"description","dir":"Articles","previous_headings":"","what":"Description","title":"2. Prepare Data for Model Calibration","text":"starting ENM process, data must formatted specific structure required functions kuenm2. vignette guides users steps necessary prepare occurrence data environmental predictors using built-tools. covers use main functions, prepare_data() prepare_user_data(), generate standardized objects essential model calibration. guide also demonstrates options compute principal components variables (PCA), incorporating sampling bias, integrating data partitioning schemes external methods, exploring prepared data, saving data object later use.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"getting-ready","dir":"Articles","previous_headings":"","what":"Getting ready","title":"2. Prepare Data for Model Calibration","text":"kuenm2 installed yet, please . See Main guide installation instructions. See also basic data cleaning guide data cleaning steps. Use following lines code load kuenm2 required packages, define working directory (needed). general, setting working directory R considered good practice, provides better control files read saved . users working within R project, recommend setting working directory, since least one file saved later stages guide. Note: functions packages (.e., base R kuenm2 ) used guide displayed package::function().","code":"# Load packages library(kuenm2) library(terra)  # Current directory getwd()  # Define new directory #setwd(\"YOUR/DIRECTORY\")  # uncomment and modify if setting a new directory  # Saving original plotting parameters original_par <- par(no.readonly = TRUE)"},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"example-data","dir":"Articles","previous_headings":"Preparing data","what":"Example data","title":"2. Prepare Data for Model Calibration","text":"use occurrence records provided within kuenm2 package. example data package derived Trindade & Marques (2024). occ_data object contains 51 occurrences Myrcia hatschbachii, tree endemic Southern Brazil. Although example data set three columns (species, x, y), two numeric columns longitude latitude coordinates required.  predictor variables, use another dataset included package. dataset comprises four bioclimatic variables WorldClim 2.1 10 arc-minute resolution, categorical variable (SoilType) SoilGrids aggregated 10 arc-minutes. variables masked using polygon delimits area model calibration. polygon generated drawing minimum convex polygon around records, 300 km buffer.   Visualize occurrences records geography:","code":"# Import occurrences data(occ_data, package = \"kuenm2\")  # Check data structure str(occ_data) #> 'data.frame':    51 obs. of  3 variables: #>  $ species: chr  \"Myrcia hatschbachii\" \"Myrcia hatschbachii\" \"Myrcia hatschbachii\" \"Myrcia hatschbachii\" ... #>  $ x      : num  -51.3 -50.6 -49.3 -49.8 -50.2 ... #>  $ y      : num  -29 -27.6 -27.8 -26.9 -28.2 ... # Import raster layers var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                 package = \"kuenm2\"))  # Check variables terra::plot(var) # Visualize occurrences on one variable terra::plot(var[[\"bio_1\"]], main = \"Bio 1\")  points(occ_data[, c(\"x\", \"y\")], col = \"black\")"},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"first-steps-in-preparing-data","dir":"Articles","previous_headings":"Preparing data","what":"First steps in preparing data","title":"2. Prepare Data for Model Calibration","text":"functions prepare_data() prepare_user_data() central getting data ready model calibration. handles several key steps: Defining algorithm: Users can choose maxnet glm. Generating background points: Background points sampled raster layers (prepare_data()), unless provided user (prepare_user_data()). Background points serve reference contrast presence records. Principal component analysis (PCA): optional step can done variables provided. Preparing calibration data: Presence records background points associate predictor values put together data.frame used ENM. Calibration data provided user prepare_user_data(). Data partitioning: function divides data prepare training testing sets via cross-validation process. partitioning methods directly available include kfolds, subsample, bootstrap. Defining grid model parameters: helps setting combinations feature classes (FCs), regularization multiplier (RM) values (Maxnet), sets predictor variables. explanation roles RMs FCs Maxent models see Merow et al. 2013. function, recommend checking documentation help(prepare_data) detailed explanations. Now, let’s prepare data model calibration, using 4 k-folds partition training testing datasets:  prepare_data() function returns prepared_data object, list contains several essential components model calibration. example object’s printed output, provides summary contents.  parts prepared_data object can explored detail indexing following example.  algorithms can selected \"maxnet\" \"glm\". using GLMs, regularization multipliers used. Now, let’s run example using glm, time using subsample partitioning method, total 10 partitions, 70% dataset used training every iteration.","code":"# Prepare data for maxnet model d <- prepare_data(algorithm = \"maxnet\",                   occ = occ_data,                   x = \"x\", y = \"y\",                   raster_variables = var,                   species = \"Myrcia hatschbachii\",                   categorical_variables = \"SoilType\",                    partition_method = \"kfolds\",                    n_partitions = 4,                   n_background = 1000,                   features = c(\"l\", \"q\", \"lq\", \"lqp\"),                   r_multiplier = c(0.1, 1, 2)) #> Warning in handle_missing_data(occ_bg, weights): 43 rows were excluded from #> database because NAs were found. print(d) #> prepared_data object summary #> ============================ #> Species: Myrcia hatschbachii  #> Number of Records: 1008  #>   - Presence: 51  #>   - Background: 957  #> Partition Method: kfolds  #>   - Number of kfolds: 4  #> Continuous Variables: #>   - bio_1, bio_7, bio_12, bio_15  #> Categorical Variables: #>   - SoilType  #> PCA Information: PCA not performed #> Weights: No weights provided #> Calibration Parameters: #>   - Algorithm: maxnet  #>   - Number of candidate models: 300  #>   - Features classes (responses): l, q, lq, lqp  #>   - Regularization multipliers: 0.1, 2, 1 # Check the algorithm selected d$algorithm #> [1] \"maxnet\"  # See first rows of calibration data head(d$calibration_data) #>   pr_bg    bio_1    bio_7 bio_12   bio_15 SoilType #> 1     1 16.49046 18.66075   1778 12.96107       19 #> 2     1 15.46644 19.65775   1560 14.14697       19 #> 3     1 15.70560 17.99450   1652 23.27548        6 #> 4     1 17.78899 19.55600   1597 18.91694        1 #> 5     1 15.50116 18.30750   1497 15.39440       19 #> 6     1 17.42421 17.25875   1760 34.17664        6  # See first rows of formula grid head(d$formula_grid) #>   ID           Formulas R_multiplier Features #> 1  1  ~bio_1 + bio_7 -1          0.1        l #> 2  2  ~bio_1 + bio_7 -1          2.0        l #> 3  3  ~bio_1 + bio_7 -1          1.0        l #> 4  4 ~bio_1 + bio_12 -1          2.0        l #> 5  5 ~bio_1 + bio_12 -1          1.0        l #> 6  6 ~bio_1 + bio_12 -1          0.1        l # Prepare data selecting GLM as the algorithm d_glm <- prepare_data(algorithm = \"glm\",                       occ = occ_data,                       x = \"x\", y = \"y\",                       raster_variables = var,                       species = \"Myrcia hatschbachii\",                       categorical_variables = \"SoilType\",                        partition_method = \"subsample\",                        n_partitions = 10,                        train_proportion = 0.7,                       n_background = 300,                       features = c(\"l\", \"q\", \"p\", \"lq\", \"lqp\"),                       r_multiplier = NULL)  # Not necessary with GLMs #> Warning in handle_missing_data(occ_bg, weights): 8 rows were excluded from #> database because NAs were found.  # Print object d_glm #> prepared_data object summary #> ============================ #> Species: Myrcia hatschbachii  #> Number of Records: 343  #>   - Presence: 51  #>   - Background: 292  #> Partition Method: subsample  #>   - Number of replicates:  #>   - Train proportion: 0.7  #> Continuous Variables: #>   - bio_1, bio_7, bio_12, bio_15  #> Categorical Variables: #>   - SoilType  #> PCA Information: PCA not performed #> Weights: No weights provided #> Calibration Parameters: #>   - Algorithm: glm  #>   - Number of candidate models: 122  #>   - Features classes (responses): l, q, p, lq, lqp"},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"using-pre-processed-data","dir":"Articles","previous_headings":"Preparing data","what":"Using pre-processed data","title":"2. Prepare Data for Model Calibration","text":"cases, users already data prepared model calibration (e.g., data prepared time-specific applications). case, function prepare_user_data() can take pre-processed data get ready next analyses. User-prepared data must data.frame includes column zeros ones, indicating presence (1) background (0) records, along columns values variables. package includes example data.frame reference (see ).  prepare_user_data() function operates similarly prepare_data(), key differences. main difference instead requiring table coordinates SpatRaster variables, takes already prepared data.frame. See full documentation help(prepare_user_data).  function also allows users provide list identifying points used testing cross-validation iteration. can useful keep data partitions stable among distinct model calibration routines. user_folds NULL (default), function partitions data according partition_method, n_partitions, train_proportion.","code":"data(\"user_data\", package = \"kuenm2\")  head(user_data) #>   pr_bg    bio_1    bio_7 bio_12   bio_15 SoilType #> 1     1 16.49046 18.66075   1778 12.96107       19 #> 2     1 15.46644 19.65775   1560 14.14697       19 #> 3     1 15.70560 17.99450   1652 23.27548        6 #> 4     1 17.78899 19.55600   1597 18.91694        1 #> 5     1 15.50116 18.30750   1497 15.39440       19 #> 7     1 17.42421 17.25875   1760 34.17664        6 # Prepare data for maxnet model data_user <- prepare_user_data(algorithm = \"maxnet\",                                user_data = user_data,  # user-prepared data.frame                                pr_bg = \"pr_bg\",                                species = \"Myrcia hatschbachii\",                                categorical_variables = \"SoilType\",                                partition_method = \"bootstrap\",                                features = c(\"l\", \"q\", \"p\", \"lq\", \"lqp\"),                                r_multiplier = c(0.1, 1, 2, 3, 5))  data_user  #> prepared_data object summary #> ============================ #> Species: Myrcia hatschbachii  #> Number of Records: 527  #>   - Presence: 51  #>   - Background: 476  #> Partition Method: bootstrap  #>   - Number of replicates:  #>   - Train proportion: 0.7  #> Continuous Variables: #>   - bio_1, bio_7, bio_12, bio_15  #> Categorical Variables: #>   - SoilType  #> PCA Information: PCA not performed #> Weights: No weights provided #> Calibration Parameters: #>   - Algorithm: maxnet  #>   - Number of candidate models: 610  #>   - Features classes (responses): l, q, p, lq, lqp  #>   - Regularization multipliers: 0.1, 1, 2, 3, 5"},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"exploring-prepared-data","dir":"Articles","previous_headings":"","what":"Exploring prepared data","title":"2. Prepare Data for Model Calibration","text":"following examples, ’ll use object d, prepared using maxnet algorithm. can done prepared_data using glm de algorithm.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"comparative-histograms","dir":"Articles","previous_headings":"Exploring prepared data","what":"Comparative histograms","title":"2. Prepare Data for Model Calibration","text":"Users can visualize distribution predictor values occurrence records, background points, entire calibration area using histograms. example presented . See full documentation help(explore_calibration_hist) help(plot_explore_calibration).   previous plot, gray represents values across entire calibration area, blue background values, green values presence records (magnified factor 2 enhance visualization). colors magnification factor can customized. raster_variables available, exclude argument include_m running function explore_calibration_hist(). happen users pre-processed data run prepare_user_data().","code":"# Prepare histogram data calib_hist <- explore_calibration_hist(data = d, raster_variables = var,                                        include_m = TRUE)  # Plot histograms plot_calibration_hist(explore_calibration = calib_hist)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"distribution-of-data-for-models","dir":"Articles","previous_headings":"Exploring prepared data","what":"Distribution of data for models","title":"2. Prepare Data for Model Calibration","text":"Additionally, users can explore geographic distribution occurrences background, well partitioned. See full documentation help(explore_partition_geo).   Note , default, background points selected randomly within calibration area. However, users can influence selection providing bias file (see section options prepare data).","code":"# Explore spatial distribution pbg <- explore_partition_geo(data = d, raster_variables = var[[1]])  # Plot exploration results in geography terra::plot(pbg)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"similarity-assessment-in-partitions","dir":"Articles","previous_headings":"Exploring prepared data","what":"Similarity assessment in partitions","title":"2. Prepare Data for Model Calibration","text":"partitioning data, testing points may fall environments different training points . forces model evaluate extrapolation, testing predictions conditions outside training range. explore_partition_extrapolation() function calculates dissimilarity detects non-analogous conditions testing points comparing training data partition. Dissimilarity tests performed using mobility-oriented parity metric (MOP; Owens et al. 2013) Cobos et al. (2024). analysis requires prepared_data object. default, function explore_partition_extrapolation() uses training data (presences backgrounds) define environmental space reference, computes MOP testing records. computing MOP test background points needed, set include_test_background = TRUE.  previous run returns list main outcome Mop_results. data.frame, row testing record; columns : mop_distance: MOP distances; inside_range indicator whether environmental conditions testing record fall within training range; n_var_out: number variables outside training range; towards_low towards_high : names variables values lower higher training range; SoilType: prepared_data object includes categorical variable, also contains column indicating categories testing data present training data.  Now can use function plot_explore_partition() visualize records partition geographic environmental spaces. comparisons performed environmental space, visualize results geographic space plotting functions uses simplified map world, another spatial object can defined calibration_area needed. type MOP result plot can specified : “simple” show records partition within envrionmental range partitions; “distance” display distance record nearest set conditions partitions.      data used example partitioned using k-folds reduces chances finding novel conditions comparing testing vs training sets data. However, might case using data partitioning methods spatial blocks (see Using external data partitions).","code":"# Run extrapolation risk analysis mop_partition <- explore_partition_extrapolation(data = d,                                                   include_test_background = TRUE) # Check some of the results head(mop_partition$Mop_results) #>      Partition pr_bg         x         y mop_distance inside_range n_var_out #> 1  Partition_1     1 -51.29778 -29.02639     3.813469         TRUE         0 #> 3  Partition_1     1 -49.32222 -27.81167     3.671747         TRUE         0 #> 17 Partition_1     1 -51.03556 -28.58194     5.315553         TRUE         0 #> 18 Partition_1     1 -50.57972 -27.29056     3.240024         TRUE         0 #> 19 Partition_1     1 -49.82139 -27.40639     4.342539         TRUE         0 #> 26 Partition_1     1 -49.27361 -25.38528     4.623356         TRUE         0 #>    towards_low towards_high SoilType #> 1         <NA>         <NA>     <NA> #> 3         <NA>         <NA>     <NA> #> 17        <NA>         <NA>     <NA> #> 18        <NA>         <NA>     <NA> #> 19        <NA>         <NA>     <NA> #> 26        <NA>         <NA>     <NA>  # Number of testing records with values outside training ranges nrow(mop_partition$Mop_results[mop_partition$Mop_results$n_var_out > 1, ]) #> [1] 0 # Simple plot in geographic space plot_explore_partition(explore_partition = mop_partition, space = \"G\",                         type_of_plot = \"simple\") # Distance plot in geographic space plot_explore_partition(explore_partition = mop_partition, space = \"G\",                         type_of_plot = \"distance\",                        lwd_legend = 4) # Simple plot in environmental space plot_explore_partition(explore_partition = mop_partition, space = \"E\",                         type_of_plot = \"simple\",                         variables = c(\"bio_7\", \"bio_15\")) # Distance plot in environmental space plot_explore_partition(explore_partition = mop_partition, space = \"E\",                         type_of_plot = \"distance\",                        variables = c(\"bio_7\", \"bio_15\"),                         lwd_legend = 4)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"more-options-to-prepare-data","dir":"Articles","previous_headings":"","what":"More options to prepare data","title":"2. Prepare Data for Model Calibration","text":"examples data preparation show options can used get data ready start ENM process. next sections demonstrate options available data preparation, including: (1) customizing list model formulas test model calibration; (2) principal component analysis variables; (3) using external data partitioning methods.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"custom-formulas","dir":"Articles","previous_headings":"More options to prepare data","what":"Custom formulas","title":"2. Prepare Data for Model Calibration","text":"default, kuenm2 builds formula grid automatically using variables provided feature classes defined. instance, raster_variables user_data contain bio_1 bio_12, set features lq (linear + quadratic), formula include linear quadratic terms variable. example, resulting formula :  Instead letting functions build formulas based selected features, users can provide custom formulas. useful full control terms included needed (example, including quadratic version specific variables):","code":"\"~ bio_1 + bio_12 + I(bio_1^2) + I(bio_12^2)\" #> [1] \"~ bio_1 + bio_12 + I(bio_1^2) + I(bio_12^2)\" # Set custom formulas my_formulas <- c(\"~ bio_1 + bio_12 + I(bio_1^2) + I(bio_12^2)\",                  \"~ bio_1 + bio_12 + I(bio_1^2)\",                  \"~ bio_1 + bio_12 + I(bio_12^2)\",                  \"~ bio_1 + I(bio_1^2) + I(bio_12^2)\")  # Prepare data using custom formulas d_custom_formula <- prepare_data(algorithm = \"maxnet\",                                  occ = occ_data,                                  x = \"x\", y = \"y\",                                  raster_variables = var,                                  species = \"Myrcia hatschbachii\",                                  categorical_variables = \"SoilType\",                                  partition_method = \"kfolds\",                                  n_partitions = 4,                                  n_background = 1000,                                  user_formulas = my_formulas,  # Custom formulas                                  r_multiplier = c(0.1, 1, 2)) #> Warning in handle_missing_data(occ_bg, weights): 43 rows were excluded from #> database because NAs were found.  # Check formula grid d_custom_formula$formula_grid #>    ID                                       Formulas R_multiplier Features #> 1   1 ~ bio_1 + bio_12 + I(bio_1^2) + I(bio_12^2) -1          0.1   User_q #> 2   2               ~ bio_1 + bio_12 + I(bio_1^2) -1          0.1   User_q #> 3   3              ~ bio_1 + bio_12 + I(bio_12^2) -1          0.1   User_q #> 4   4          ~ bio_1 + I(bio_1^2) + I(bio_12^2) -1          0.1   User_q #> 5   5 ~ bio_1 + bio_12 + I(bio_1^2) + I(bio_12^2) -1          1.0   User_q #> 6   6               ~ bio_1 + bio_12 + I(bio_1^2) -1          1.0   User_q #> 7   7              ~ bio_1 + bio_12 + I(bio_12^2) -1          1.0   User_q #> 8   8          ~ bio_1 + I(bio_1^2) + I(bio_12^2) -1          1.0   User_q #> 9   9 ~ bio_1 + bio_12 + I(bio_1^2) + I(bio_12^2) -1          2.0   User_q #> 10 10               ~ bio_1 + bio_12 + I(bio_1^2) -1          2.0   User_q #> 11 11              ~ bio_1 + bio_12 + I(bio_12^2) -1          2.0   User_q #> 12 12          ~ bio_1 + I(bio_1^2) + I(bio_12^2) -1          2.0   User_q"},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"using-a-bias-file","dir":"Articles","previous_headings":"More options to prepare data","what":"Using a bias file","title":"2. Prepare Data for Model Calibration","text":"bias file SpatRaster object contains values influence selection background points within calibration area. can particularly useful mitigating sampling bias. instance, want selection background points informed historical sampling , layer density records target group can used (see Ponder et al. 2001, Anderson et al. 2003, Barber et al. 2020). bias file must extent, resolution, number cells raster variables. Let’s illustrate process example bias file included package. SpatRaster lower values center higher values towards borders area:   bias layer used prepare two new datasets: one “direct” bias effect (higher probability selecting background points regions higher bias values) another “inverse” effect (oposite).  Let’s use explore_partition_geo function see effect using bias file.","code":"# Import a bias file bias <- terra::rast(system.file(\"extdata\", \"bias_file.tif\", package = \"kuenm2\"))  # Check the bias values terra::plot(bias) # Using a direct bias effect in sampling d_bias_direct <- prepare_data(algorithm = \"maxnet\",                               occ = occ_data,                               x = \"x\", y = \"y\",                               raster_variables = var,                               species = \"Myrcia hatschbachii\",                               categorical_variables = \"SoilType\",                               n_background = 1000,                                partition_method = \"kfolds\",                               bias_file = bias, bias_effect = \"direct\",  # bias parameters                               features = c(\"l\", \"q\", \"p\", \"lq\", \"lqp\"),                               r_multiplier = c(0.1, 1, 2, 3, 5)) #> Warning in handle_missing_data(occ_bg, weights): 57 rows were excluded from #> database because NAs were found.  # Using an indirect bias effect  d_bias_inverse <- prepare_data(algorithm = \"maxnet\",                                occ = occ_data,                                x = \"x\", y = \"y\",                                raster_variables = var,                                species = \"Myrcia hatschbachii\",                                categorical_variables = \"SoilType\",                                n_background = 1000,                                partition_method = \"kfolds\",                                bias_file = bias, bias_effect = \"inverse\",  # bias parameters                                features = c(\"l\", \"q\", \"p\", \"lq\", \"lqp\"),                                r_multiplier = c(0.1, 1, 2, 3, 5)) #> Warning in handle_missing_data(occ_bg, weights): 45 rows were excluded from #> database because NAs were found. # Explore spatial distribution of points ## No bias geo_dist <- explore_partition_geo(data = d, raster_variables = var)  ## Direct bias effect geo_dist_bias <- explore_partition_geo(data = d_bias_direct,                                        raster_variables = var)  ## Inverse bias effect geo_dist_bias_inv <- explore_partition_geo(data = d_bias_inverse,                                            raster_variables = var)  ## Adjusting plotting grid par(mfrow = c(2, 2))    ## The plots to show sampling bias effects terra::plot(bias, main = \"Bias file\") terra::plot(geo_dist$Background, main = \"Random Background\",              plg = list(cex = 0.75))  # Decrease size of legend text) terra::plot(geo_dist_bias$Background, main = \"Direct Bias Effect\",              plg = list(cex = 0.75))  # Decrease size of legend text) terra::plot(geo_dist_bias_inv$Background, main = \"Inverse Bias Effect\",              plg = list(cex = 0.75))  # Decrease size of legend text)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"pca-for-variables","dir":"Articles","previous_headings":"More options to prepare data","what":"PCA for variables","title":"2. Prepare Data for Model Calibration","text":"common approach ENM involves summarizing information set variables smaller set orthogonal variables using Principal Component Analysis (PCA) (see Cruz-Cardenaz et al. 2014 example). kuenm2 options perform PCA internally use variables externally prepared PCs.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"internal-pca","dir":"Articles","previous_headings":"More options to prepare data > PCA for variables","what":"Internal PCA","title":"2. Prepare Data for Model Calibration","text":"kuenm2 can perform PCA transformations internally, eliminates need transforming raw variables PCs producing projections later . particularly advantageous projecting model results across multiple scenarios (e.g., various Global Climate Models different future periods). performing PCA internally, need provide raw environmental variables (e.g., bio_1, bio_2, etc.) making projections; helper functions handle PCA transformation internally. Let’s explore implement :  elements calibration_data formula_grid now generated considering principal components (PCs). default, continuous variables included PCA, categorical variables (e.g., “SoilType”) excluded. default settings define number PCs retain keeps many PCs needed collectively explain 95% total variance, filter , keeping axes individually explain least 5% variance. parameters can changed using arguments function prepare_data().","code":"# Prepare data for maxnet models using PCA parameters d_pca <- prepare_data(algorithm = \"maxnet\",                       occ = occ_data,                       x = \"x\", y = \"y\",                       raster_variables = var,                        do_pca = TRUE, center = TRUE, scale = TRUE,  # PCA parameters                       species = \"Myrcia hatschbachii\",                       categorical_variables = \"SoilType\",                       n_background = 1000,                       partition_method = \"kfolds\",                       features = c(\"l\", \"q\", \"p\", \"lq\", \"lqp\"),                       r_multiplier = c(0.1, 1, 2, 3, 5)) #> Warning in handle_missing_data(occ_bg, weights): 43 rows were excluded from #> database because NAs were found.  print(d_pca) #> prepared_data object summary #> ============================ #> Species: Myrcia hatschbachii  #> Number of Records: 1008  #>   - Presence: 51  #>   - Background: 957  #> Partition Method: kfolds  #>   - Number of kfolds: 4  #> Continuous Variables: #>   - bio_1, bio_7, bio_12, bio_15  #> Categorical Variables: #>   - SoilType  #> PCA Information: #>   - Variables included: bio_1, bio_7, bio_12, bio_15  #>   - Number of PCA components: 4  #> Weights: No weights provided #> Calibration Parameters: #>   - Algorithm: maxnet  #>   - Number of candidate models: 610  #>   - Features classes (responses): l, q, p, lq, lqp  #>   - Regularization multipliers: 0.1, 1, 2, 3, 5 # Check calibration data head(d_pca$calibration_data) #>   pr_bg         PC1        PC2        PC3         PC4 SoilType #> 1     1  1.48690341 1.01252697  0.1180156 -0.09119257       19 #> 2     1  1.46028074 0.17701144  1.1573461 -0.12326796       19 #> 3     1  0.82676494 1.21965795  0.8145129 -0.67588891        6 #> 4     1  0.62680441 0.03967459  0.1525997  0.18784282        1 #> 5     1  0.94584897 0.93302089  1.4382424 -0.03192094       19 #> 6     1 -0.07597437 1.55268331 -0.2007953 -0.98153204        6  # Check formula grid head(d_pca$formula_grid) #>   ID      Formulas R_multiplier Features #> 1  1 ~PC1 + PC2 -1          0.1        l #> 2  2 ~PC1 + PC2 -1          1.0        l #> 3  3 ~PC1 + PC2 -1          2.0        l #> 4  4 ~PC1 + PC2 -1          3.0        l #> 5  5 ~PC1 + PC2 -1          5.0        l #> 6  6 ~PC1 + PC3 -1          5.0        l  # Explore variables distribution calib_hist_pca <- explore_calibration_hist(data = d_pca, raster_variables = var,                                            include_m = TRUE, breaks = 7)  plot_calibration_hist(explore_calibration = calib_hist_pca)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"external-pca","dir":"Articles","previous_headings":"More options to prepare data > PCA for variables","what":"External PCA","title":"2. Prepare Data for Model Calibration","text":"Users can choose perform PCA data using perform_pca() function, one preference. Se example perform_pca() :   Now, let’s use PCs generated perform_pca() prepare data:  Note since PCA performed externally, do_pca = FALSE set prepare_data(). crucial setting TRUE incorrectly apply PCA variables already PCs. prepared_data object scenario store PCA-related information. Therefore, users must provide PCs instead raw raster variables projecting models.","code":"# PCA with raw raster variables pca_var <- perform_pca(raster_variables = var, exclude_from_pca = \"SoilType\",                        center = TRUE, scale = TRUE)  # Plot terra::plot(pca_var$env) # Prepare data for maxnet model using PCA variables d_pca_extern <- prepare_data(algorithm = \"maxnet\",                              occ = occ_data,                              x = \"x\", y = \"y\",                              raster_variables = pca_var$env,  # Output of perform_pca()                              do_pca = FALSE,  # Set to FALSE because variables are PCs                              species = \"Myrcia hatschbachii\",                              categorical_variables = \"SoilType\",                               n_background = 1000,                               partition_method = \"kfolds\",                              features = c(\"l\", \"q\", \"p\", \"lq\", \"lqp\"),                              r_multiplier = c(0.1, 1, 2, 3, 5)) #> Warning in handle_missing_data(occ_bg, weights): 43 rows were excluded from #> database because NAs were found.  # Check the object print(d_pca_extern) #> prepared_data object summary #> ============================ #> Species: Myrcia hatschbachii  #> Number of Records: 1008  #>   - Presence: 51  #>   - Background: 957  #> Partition Method: kfolds  #>   - Number of kfolds: 4  #> Continuous Variables: #>   - PC1, PC2, PC3, PC4  #> Categorical Variables: #>   - SoilType  #> PCA Information: PCA not performed #> Weights: No weights provided #> Calibration Parameters: #>   - Algorithm: maxnet  #>   - Number of candidate models: 610  #>   - Features classes (responses): l, q, p, lq, lqp  #>   - Regularization multipliers: 0.1, 1, 2, 3, 5  # Check formula grid head(d_pca_extern$formula_grid) #>   ID      Formulas R_multiplier Features #> 1  1 ~PC1 + PC2 -1          0.1        l #> 2  2 ~PC1 + PC2 -1          1.0        l #> 3  3 ~PC1 + PC2 -1          2.0        l #> 4  4 ~PC1 + PC2 -1          3.0        l #> 5  5 ~PC1 + PC2 -1          5.0        l #> 6  6 ~PC1 + PC3 -1          5.0        l"},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"using-external-data-partitions","dir":"Articles","previous_headings":"More options to prepare data","what":"Using external data partitions","title":"2. Prepare Data for Model Calibration","text":"functions prepare_data() prepare_user_data() kuenm2 package include four built-methods data partitioning: “kfolds”: Splits dataset K subsets (folds) approximately equal size. partition, one fold used test set, remaining folds combined form training set. “bootstrap”: Creates training dataset sampling observations original dataset replacement (.e., observation can selected multiple times). test set consists observations selected specific replicate. “subsample”: Similar bootstrap, training set created sampling without replacement (.e., observation selected ). test set includes observations selected training. methods data partitioning also available, including based spatial rules (Radosavljevic Anderson, 2014). Although kuenm2 currently implement spatial partitioning techniques, possible use ones implemented R packages. partitioning data using packages, results can used replace part_data section prepared_data object kuenm2.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"enmeval-partitions","dir":"Articles","previous_headings":"More options to prepare data > Using external data partitions","what":"ENMeval partitions","title":"2. Prepare Data for Model Calibration","text":"ENMeval package (Kass et al. 2021) provides three spatial partitioning methods: Spatial block: Divides occurrence data four groups based latitude longitude, aiming groups roughly equal number occurrences. Checkerboard 1 (basic): Generates checkerboard grid study area assigns points groups based location grid. Checkerboard 2 (hierarchical): Aggregates input raster two hierarchical spatial scales using specified aggregation factors. Points grouped based position resulting hierarchical checkerboard. Let’s use spatial block method example. use object d, prepared_data created previous steps.  output get.block() list two elements: occs.grp bg.grp. occs.grp vector occurrence records bg.grp background points. vectors contain numeric values 1 4, indicating spatial group. kuenm2 stores partitioned data list vectors, vector partition, containing indices points left testing. indices include presence background points.  can convert spatial group information stored enmeval_block list compatible kuenm2:  now list containing four vectors, storing indices test data defined using get.block() function ENMeval package. final step replace original part_data prepared_data object new_part_data. also update partitioning method reflect change.  Let’s check spatial distribution partitioned data:   environmental conditions often vary region, using spatial blocks increases chances testing records outside training environmental ranges. Let’s explore effect using prepared_data object, partitioned ENMeval spatial blocks.    Note partition 1, occurrence records fall outside envrionmental range partitions (happens many background records).","code":"# Install ENMeval if not already installed if(!require(\"ENMeval\")){   install.packages(\"ENMeval\") }  # Extract calibration data from the prepared_data object and  # separate presence and background records calib_occ <- d$data_xy[d$calibration_data$pr_bg == 1, ]  # Presences calib_bg <- d$data_xy[d$calibration_data$pr_bg == 0, ]  # Background  # Apply spatial block partitioning using ENMeval enmeval_block <- ENMeval::get.block(occs = calib_occ, bg = calib_bg)  # Inspect the structure of the output str(enmeval_block) #> List of 2 #>  $ occs.grp: num [1:51] 1 1 2 2 1 4 2 2 4 2 ... #>  $ bg.grp  : num [1:957] 2 4 4 1 3 3 3 1 1 3 ... str(d$part_data) #> List of 4 #>  $ Partition_1: num [1:253] 1 3 12 13 14 20 24 25 34 43 ... #>  $ Partition_2: num [1:252] 5 7 8 10 15 17 23 27 31 33 ... #>  $ Partition_3: num [1:251] 4 9 19 21 28 29 30 32 35 36 ... #>  $ Partition_4: num [1:252] 2 6 11 16 18 22 26 37 38 40 ... # Identify unique spatial blocks id_blocks <- sort(unique(unlist(enmeval_block)))  # Create a list of test indices for each spatial block new_part_data <- lapply(id_blocks, function(i) {   # Indices of occurrence records in group i   rep_i_presence <- which(enmeval_block$occs.grp == i)      # Indices of background records in group i   rep_i_bg <- which(enmeval_block$bg.grp == i)      # To get the right indices for background,    # we need to sum the total number of records to background indices   rep_i_bg <- rep_i_bg + nrow(occ_data)      # Combine presence and background indices for the test set   c(rep_i_presence, rep_i_bg) })  # Assign names to each partition names(new_part_data) <- paste0(\"Partition_\", id_blocks)  # Inspect the structure of the new partitioned data str(new_part_data) #> List of 4 #>  $ Partition_1: int [1:406] 1 2 5 12 13 14 15 29 31 42 ... #>  $ Partition_2: int [1:108] 3 4 7 8 10 16 18 22 27 36 ... #>  $ Partition_3: int [1:367] 11 19 20 21 24 26 30 33 34 38 ... #>  $ Partition_4: int [1:127] 6 9 17 23 25 28 32 35 37 39 ... # Replace the original partition data with the new spatial blocks d_spatial_block <- d d_spatial_block$part_data <- new_part_data  # Update the partitioning method to reflect the new approach d_spatial_block$partition_method <- \"Spatial block (ENMeval)\"  # Any name works  # Print the updated prepared_data object print(d_spatial_block) #> prepared_data object summary #> ============================ #> Species: Myrcia hatschbachii  #> Number of Records: 1008  #>   - Presence: 51  #>   - Background: 957  #> Partition Method: Spatial block (ENMeval)  #> Continuous Variables: #>   - bio_1, bio_7, bio_12, bio_15  #> Categorical Variables: #>   - SoilType  #> PCA Information: PCA not performed #> Weights: No weights provided #> Calibration Parameters: #>   - Algorithm: maxnet  #>   - Number of candidate models: 300  #>   - Features classes (responses): l, q, lq, lqp  #>   - Regularization multipliers: 0.1, 2, 1 # Explore data partitioning in geography geo_block <- explore_partition_geo(d_spatial_block, raster_variables = var[[1]])  # Plot data partition in geography terra::plot(geo_block[[c(\"Presence\", \"Background\")]]) # Run extrapolation risk analysis mop_blocks <- explore_partition_extrapolation(data = d_spatial_block,                                                include_test_background = TRUE)  # Check some testing records with values outside training ranges head(mop_blocks$Mop_results[mop_blocks$Mop_results$n_var_out > 1, ]) #>       Partition pr_bg         x         y mop_distance inside_range n_var_out #> 87  Partition_1     0 -50.75000 -30.75000     12.57765        FALSE         2 #> 129 Partition_1     0 -51.75000 -29.91667     10.91112        FALSE         2 #> 193 Partition_1     0 -53.41667 -27.08333     43.59743        FALSE         2 #> 199 Partition_1     0 -52.41667 -26.75000    171.15194        FALSE         2 #> 252 Partition_1     0 -51.58333 -30.75000     15.76803        FALSE         2 #> 261 Partition_1     0 -53.25000 -27.08333     43.59052        FALSE         2 #>     towards_low towards_high SoilType #> 87       bio_15         <NA>       21 #> 129      bio_15         <NA>       21 #> 193      bio_15        bio_7     <NA> #> 199      bio_15       bio_12     <NA> #> 252      bio_15         <NA>       21 #> 261      bio_15        bio_7     <NA>  # Check simple extrapolation in geographic space plot_explore_partition(explore_partition = mop_blocks, space = \"G\",                         type_of_plot = \"simple\") # Now in environmental space plot_explore_partition(explore_partition = mop_blocks, space = \"E\",                         type_of_plot = \"simple\",                         variables = c(\"bio_7\", \"bio_15\"))"},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"flexsdm-partitions","dir":"Articles","previous_headings":"More options to prepare data > Using external data partitions","what":"flexsdm partitions","title":"2. Prepare Data for Model Calibration","text":"package flexsdm (Velazco et al. 2022) also offers multiple partitioning methods. use method spatial block cross-validation. method, number size blocks optimized internally considering spatial autocorrelation, environmental similarity, number presence background records within partition. details, see Data partitioning package website.  output flexsdm differs ENMeval. Instead returning list separate vectors spatial group IDs, flexsdm appends group assignments new column part element output. ENMeval example, can transform spatial group information stored flexsdm_block format compatible kuenm2:  Let’s check spatial distribution partitioned data:","code":"# Install flexsdm if not already installed if (!require(\"flexsdm\")) {   if (!require(\"remotes\")) {     install.packages(\"remotes\")   }      remotes::install_github(\"sjevelazco/flexsdm\")  # needs compilation tools to work }  # Combine calibration data with spatial coordinates calib_data <- cbind(d$data_xy, d$calibration_data)  # Split data in test and train using flexsdm flexsdm_block <- flexsdm::part_sblock(env_layer = var, data = calib_data,                                        x = \"x\", y = \"y\", pr_ab = \"pr_bg\",                                        min_res_mult = 1, max_res_mult = 500,                                        num_grids = 30, n_part = 4,                                        prop = 0.5) #> The following grid cell sizes will be tested: #> 0.17 | 3.03 | 5.9 | 8.77 | 11.64 | 14.51 | 17.37 | 20.24 | 23.11 | 25.98 |  #> 28.84 | 31.71 | 34.58 | 37.45 | 40.32 | 43.18 | 46.05 | 48.92 | 51.79 |  #> 54.66 | 57.52 | 60.39 | 63.26 | 66.13 | 68.99 | 71.86 | 74.73 | 77.6 |  #> 80.47 | 83.33 #>  #> Creating basic raster mask... #>  #> Searching for the optimal grid size...  # See the structure of the object str(flexsdm_block$part) #> Classes ‘tbl_df’, ‘tbl’ and 'data.frame':    1008 obs. of  4 variables: #>  $ x    : num  -51.3 -50.6 -49.3 -49.8 -50.2 ... #>  $ y    : num  -29 -27.6 -27.8 -26.9 -28.2 ... #>  $ pr_ab: num  1 1 1 1 1 1 1 1 1 1 ... #>  $ .part: int  3 1 3 2 3 3 1 4 4 3 ... # Identify unique spatial blocks from flexsdm output id_blocks <- sort(unique(flexsdm_block$part$.part))  # Create a list of test indices for each spatial block new_part_data_flexsdm <- lapply(id_blocks, function(i) {   # Indices of occurrences and background points in group i   which(flexsdm_block$part$.part == i) })  # Assign names to each partition partition names(new_part_data_flexsdm) <- paste0(\"Partition_\", id_blocks)  # Inspect the structure of the new partitioned data str(new_part_data_flexsdm) #> List of 4 #>  $ Partition_1: int [1:248] 2 7 13 21 24 25 26 27 36 40 ... #>  $ Partition_2: int [1:266] 4 12 15 17 18 19 29 31 32 33 ... #>  $ Partition_3: int [1:247] 1 3 5 6 10 16 20 28 34 35 ... #>  $ Partition_4: int [1:247] 8 9 11 14 22 23 30 37 38 41 ...  # Replace the partition data in the prepared_data object d_block_flexsdm <- d d_block_flexsdm$part_data <- new_part_data_flexsdm  # Update the partition method description d_block_flexsdm$partition_method <- \"Spatial block (flexsdm)\"  # any name works  # Display the updated prepared_data object print(d_block_flexsdm) #> prepared_data object summary #> ============================ #> Species: Myrcia hatschbachii  #> Number of Records: 1008  #>   - Presence: 51  #>   - Background: 957  #> Partition Method: Spatial block (flexsdm)  #> Continuous Variables: #>   - bio_1, bio_7, bio_12, bio_15  #> Categorical Variables: #>   - SoilType  #> PCA Information: PCA not performed #> Weights: No weights provided #> Calibration Parameters: #>   - Algorithm: maxnet  #>   - Number of candidate models: 300  #>   - Features classes (responses): l, q, lq, lqp  #>   - Regularization multipliers: 0.1, 2, 1 # Explore data partitioning in geography geo_block_flexsdm <- explore_partition_geo(d_block_flexsdm,                                             raster_variables = var[[1]])  # Plot data partition in geography terra::plot(geo_block_flexsdm[[c(\"Presence\", \"Background\")]]) # Reset plotting parameters par(original_par)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/prepare_data.html","id":"saving-prepared-data","dir":"Articles","previous_headings":"","what":"Saving prepared data","title":"2. Prepare Data for Model Calibration","text":"prepared_data object crucial next step ENM workflow kuenm2, model calibration. object essentially list, users can save local directory using saveRDS(). Saving object facilitates loading back R session later using readRDS(). See example :","code":"# Set directory to save (here, in a temporary directory) dir_to_save <- file.path(tempdir())  # Save the data saveRDS(d, file.path(dir_to_save, \"Data.rds\"))  # Import data d <- readRDS(file.path(dir_to_save, \"Data.rds\"))"},{"path":"https://marlonecobos.github.io/kuenm2/articles/projections_chelsa.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"8. Example of Projections Using CHELSA Data","text":"Introduction Standardize Raster File Names Model Projections Download variables LGM Download variables Current Period Merge Rename Variables Scenario Organize structure variables Preparation data model projection Project selected models multiple scenarios Compute changes","code":""},{"path":"https://marlonecobos.github.io/kuenm2/articles/projections_chelsa.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"8. Example of Projections Using CHELSA Data","text":"“Project models multiple scenarios” vignette, show predict models multiple scenarios . used example future scenarios available WorldClim, specific function (organize_future_worldclim) organize files specific hierarchical manner compatible kuenm2. Specifically, need root directory containing nested folders representing different scenarios, raster variables stored within. first level inside root folder, subfolders correspond distinct time periods (e.g., future years like “2070” “2100,” past periods “Mid-holocene” “LGM”). Within period folder, applicable, include subfolders emission scenario (e.g., “ssp126”, “ssp585”). Finally, within emission scenario time period folder, need include separate folder General Circulation Model (GCM) (e.g., “BCC-CSM2-MR”, “MIROC6”) , show organize files manually, making possible project models using scenarios available different sources. example, use variables CHELSA representing scenarios Last Glacial Maximum (LGM, 21,000 year ago). WARNING: November 2025, CHELSA updated removed LGM variables many GCMs. keep example users can still see function handles past climate projections, even though specific data shown may longer available.","code":"#Load packages library(kuenm2) library(terra) #> terra 1.8.86"},{"path":"https://marlonecobos.github.io/kuenm2/articles/projections_chelsa.html","id":"standardize-raster-file-names-for-model-projections","dir":"Articles","previous_headings":"","what":"Standardize Raster File Names for Model Projections","title":"8. Example of Projections Using CHELSA Data","text":"using organize_for_projection() function, climate variables must first saved separate .tif files — one file per scenario. Filenames follow consistent pattern clearly indicates time period, GCM, , future scenarios, emission scenario (SSP). example: file representing past conditions “LGM” period using “MIROC6” GCM named: “Past_LGM_MIROC6.tif” file representing future conditions period “2081–2100” emission scenario “ssp585” GCM “ACCESS-CM2” named: “Future_2081-2100_ssp585_ACCESS-CM2.tif” scenario files must contain variable names (e.g., bio1, bio2, etc.) units used model calibration. Let’s see achieve practice.","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/articles/projections_chelsa.html","id":"download-variables-from-lgm","dir":"Articles","previous_headings":"Getting variables from CHELSA","what":"Download variables from LGM","title":"8. Example of Projections Using CHELSA Data","text":"first step downloading variables representing LGM condition CHELSA. can download files directly link, follow script . need specify folder save variables, General Circulation Models (GCMs) variables:","code":"# Define variables to download var_to_use <- c(\"BIO_01\", \"BIO_07\", \"BIO_12\", \"BIO_15\") # Define GCMs gcms <- c(\"CCSM4\", \"CNRM-CM5\", \"FGOALS-g2\", \"IPSL-CM5A-LR\", \"MIROC-ESM\",           \"MPI-ESM-P\", \"MRI-CGCM3\") # Create a grid combining variables and GCMs g <- expand.grid(\"gcm\" = gcms, \"var\" = var_to_use)  # Create links to download l <- sapply(1:nrow(g), function(i){   gcm_i <- g$gcm[i]   var_i <- g$var[i]   #Create link to download   l_i <- paste0(\"https://os.zhdk.cloud.switch.ch/chelsav1/pmip3/bioclim/CHELSA_PMIP_\",                 gcm_i, \"_\", var_i, \".tif\") }) # See links head(l) #> [1] \"https://os.zhdk.cloud.switch.ch/chelsav1/pmip3/bioclim/CHELSA_PMIP_CCSM4_BIO_01.tif\"        #> [2] \"https://os.zhdk.cloud.switch.ch/chelsav1/pmip3/bioclim/CHELSA_PMIP_CNRM-CM5_BIO_01.tif\"     #> [3] \"https://os.zhdk.cloud.switch.ch/chelsav1/pmip3/bioclim/CHELSA_PMIP_FGOALS-g2_BIO_01.tif\"    #> [4] \"https://os.zhdk.cloud.switch.ch/chelsav1/pmip3/bioclim/CHELSA_PMIP_IPSL-CM5A-LR_BIO_01.tif\" #> [5] \"https://os.zhdk.cloud.switch.ch/chelsav1/pmip3/bioclim/CHELSA_PMIP_MIROC-ESM_BIO_01.tif\"    #> [6] \"https://os.zhdk.cloud.switch.ch/chelsav1/pmip3/bioclim/CHELSA_PMIP_MPI-ESM-P_BIO_01.tif\"  # Create a directory to save the raw variables raw_past_chelsa <- file.path(tempdir(), \"Raw_past\") #Here, in a temporary directory dir.create(raw_past_chelsa)  # Download files and save in the Raw_past directory options(timeout = 300) #For avoiding errors with timeout sapply(l, function(i){   #Donwload only if the file has not been downloaded yet   if(!file.exists(file.path(raw_past_chelsa, basename(i))))   download.file(url = i,                 destfile = file.path(raw_past_chelsa, basename(i)),                 method = \"curl\") }) #It will take a while  #Check the files in the Raw_past list.files(raw_past_chelsa) #> [1] \"CHELSA_PMIP_CCSM4_BIO_01.tif\"        \"CHELSA_PMIP_CCSM4_BIO_07.tif\" #> [3] \"CHELSA_PMIP_CCSM4_BIO_12.tif\"        \"CHELSA_PMIP_CCSM4_BIO_15.tif\" #> [5] \"CHELSA_PMIP_CNRM-CM5_BIO_01.tif\"     \"CHELSA_PMIP_CNRM-CM5_BIO_07.tif\" #> [7] \"CHELSA_PMIP_CNRM-CM5_BIO_12.tif\"     \"CHELSA_PMIP_CNRM-CM5_BIO_15.tif\" #> [9] \"CHELSA_PMIP_FGOALS-g2_BIO_01.tif\"    \"CHELSA_PMIP_FGOALS-g2_BIO_07.tif\" #> [11] \"CHELSA_PMIP_FGOALS-g2_BIO_12.tif\"    \"CHELSA_PMIP_FGOALS-g2_BIO_15.tif\" #> [13] \"CHELSA_PMIP_IPSL-CM5A-LR_BIO_01.tif\" \"CHELSA_PMIP_IPSL-CM5A-LR_BIO_07.tif\" #> [15] \"CHELSA_PMIP_IPSL-CM5A-LR_BIO_12.tif\" \"CHELSA_PMIP_IPSL-CM5A-LR_BIO_15.tif\" #> [17] \"CHELSA_PMIP_MIROC-ESM_BIO_01.tif\"    \"CHELSA_PMIP_MIROC-ESM_BIO_07.tif\" #> [19] \"CHELSA_PMIP_MIROC-ESM_BIO_12.tif\"    \"CHELSA_PMIP_MIROC-ESM_BIO_15.tif\" #> [21] \"CHELSA_PMIP_MPI-ESM-P_BIO_01.tif\"    \"CHELSA_PMIP_MPI-ESM-P_BIO_07.tif\" #> [23] \"CHELSA_PMIP_MPI-ESM-P_BIO_12.tif\"    \"CHELSA_PMIP_MPI-ESM-P_BIO_15.tif\" #> [25] \"CHELSA_PMIP_MRI-CGCM3_BIO_01.tif\"    \"CHELSA_PMIP_MRI-CGCM3_BIO_07.tif\" #> [27] \"CHELSA_PMIP_MRI-CGCM3_BIO_12.tif\"    \"CHELSA_PMIP_MRI-CGCM3_BIO_15.tif\""},{"path":"https://marlonecobos.github.io/kuenm2/articles/projections_chelsa.html","id":"download-variables-from-current-period","dir":"Articles","previous_headings":"Getting variables from CHELSA","what":"Download variables from Current Period","title":"8. Example of Projections Using CHELSA Data","text":"also need variables representing current conditions (1981-2010) CHELSA. can download files directly link, follow script :","code":"#Create directory to save tha variables present_dir <- file.path(tempdir(), \"Present_raw\") dir.create(present_dir)  #Define variables to download var_present <-  c(\"bio1\", \"bio7\", \"bio12\", \"bio15\")  #Create links, download and save in the Present_raw directory l_present <- sapply(var_present, function(i){   #Create link to download   l_present_i <- paste0(\"https://os.zhdk.cloud.switch.ch/chelsav2/GLOBAL/climatologies/1981-2010/bio/CHELSA_\", i, \"_1981-2010_V.2.1.tif\")   #Donwload only if the file has not been downloaded yet   if(!file.exists(file.path(present_dir, basename(l_present_i))))   download.file(url = l_present_i,                 destfile = file.path(present_dir, basename(l_present_i)),                 method = \"curl\") }) #It will take a while  #Check the files in the directory list.files(present_dir) #> [1] \"CHELSA_bio1_1981-2010_V.2.1.tif\"     \"CHELSA_bio12_1981-2010_V.2.1.tif\" #> [3] \"CHELSA_bio15_1981-2010_V.2.1.tif\"    \"CHELSA_bio7_1981-2010_V.2.1.tif\""},{"path":"https://marlonecobos.github.io/kuenm2/articles/projections_chelsa.html","id":"merge-and-rename-variables-from-the-same-scenario","dir":"Articles","previous_headings":"Getting variables from CHELSA","what":"Merge and Rename Variables from the Same Scenario","title":"8. Example of Projections Using CHELSA Data","text":"downloading files, need merge variables single SpatRaster scenario. general, past scenarios differs terms GCM, future scenarios characterized different emission scenarios (.e., SSP1-26 SSP5-85) GCMs. First, let’s merge variables present scenarios. speed analysis example, also resample rasters 30 arc-seconds 10 arc-minutes, crop variables using calibration area (M), provided example package defined drawing minimum convex polygons around species’ occurrences added buffer 300km. also rename variables pattern “bio1”, “bio12”, “bio15”, “bio7”. Now, let’s thing variables representing LGM conditions, starting masking variables using m resampling 10arc-min: Note variables names contains gcms used download. trick using patterns grouping variables: One important thing note Technical Specifications CHELSA specifies variables LGM different units current ones. current time, bio_1 values ºC, LGM values K * 10. bio_7 values ºC current time ºC * 10 LGM. current precipitation variables millimeters (mm) percentage variation (bio_15), LGM mm * 10 % * 10. need convert variables units current variables: Now variables LGM grouped scenarios (GCMs), names units current variables, can write raster disk. Remember filenames must time period (LGM) GCM scenario (variables representing future scenarios, also must SSPs emission scenarios):","code":"# Import M m <- vect(system.file(\"extdata\", \"m.gpkg\",                          package = \"kuenm2\"))  # Import present variables present_files <- list.files(present_dir, full.names = TRUE) #List files present_var <- rast(present_files)  #Mask variables using the calibration area (m) present_m <- crop(present_var, m, mask = TRUE)  # Check variables names names(present_m) #> [1] \"CHELSA_bio1_1981-2010_V.2.1\"  \"CHELSA_bio12_1981-2010_V.2.1\" #> [3] \"CHELSA_bio15_1981-2010_V.2.1\" \"CHELSA_bio7_1981-2010_V.2.1\"  # Rename variables names(present_m) <- c(\"bio1\", \"bio12\", \"bio15\", \"bio7\") names(present_m) #> [1] \"bio1\"  \"bio12\" \"bio15\" \"bio7\"  # Check current resolution (30arc-sec) res(present_m) #> [1] 0.008333333 0.008333333  #Decrease resolution to 10arc-min present_chelsa <- aggregate(present_m, fact = 20, fun = \"mean\")  #Save processed raster dir_current <- file.path(tempdir(), \"Current_CHELSA\") dir.create(dir_current)  writeRaster(present_chelsa, filename = file.path(dir_current, \"Current_CHELSA.tif\")) # Import LGM variables lgm_files <- list.files(raw_past_chelsa, full.names = TRUE) #List files lgm_var <- rast(lgm_files)  #Mask variables using the calibration area (m) lgm_m <- crop(lgm_var, m, mask = TRUE)  #Decrease resolution to 10arc-min lgm_chelsa <- aggregate(lgm_m, fact = 20, fun = \"mean\")  #Check variables names names(lgm_chelsa) #>  [1] \"CHELSA_PMIP_CCSM4_BIO_01\"        \"CHELSA_PMIP_CCSM4_BIO_07\"        #>  [3] \"CHELSA_PMIP_CCSM4_BIO_12\"        \"CHELSA_PMIP_CCSM4_BIO_15\"        #>  [5] \"CHELSA_PMIP_CNRM-CM5_BIO_01\"     \"CHELSA_PMIP_CNRM-CM5_BIO_07\"     #>  [7] \"CHELSA_PMIP_CNRM-CM5_BIO_12\"     \"CHELSA_PMIP_CNRM-CM5_BIO_15\"     #>  [9] \"CHELSA_PMIP_FGOALS-g2_BIO_01\"    \"CHELSA_PMIP_FGOALS-g2_BIO_07\"    #> [11] \"CHELSA_PMIP_FGOALS-g2_BIO_12\"    \"CHELSA_PMIP_FGOALS-g2_BIO_15\"    #> [13] \"CHELSA_PMIP_IPSL-CM5A-LR_BIO_01\" \"CHELSA_PMIP_IPSL-CM5A-LR_BIO_07\" #> [15] \"CHELSA_PMIP_IPSL-CM5A-LR_BIO_12\" \"CHELSA_PMIP_IPSL-CM5A-LR_BIO_15\" #> [17] \"CHELSA_PMIP_MIROC-ESM_BIO_01\"    \"CHELSA_PMIP_MIROC-ESM_BIO_07\"    #> [19] \"CHELSA_PMIP_MIROC-ESM_BIO_12\"    \"CHELSA_PMIP_MIROC-ESM_BIO_15\"    #> [21] \"CHELSA_PMIP_MPI-ESM-P_BIO_01\"    \"CHELSA_PMIP_MPI-ESM-P_BIO_07\"    #> [23] \"CHELSA_PMIP_MPI-ESM-P_BIO_12\"    \"CHELSA_PMIP_MPI-ESM-P_BIO_15\"    #> [25] \"CHELSA_PMIP_MRI-CGCM3_BIO_01\"    \"CHELSA_PMIP_MRI-CGCM3_BIO_07\"    #> [27] \"CHELSA_PMIP_MRI-CGCM3_BIO_12\"    \"CHELSA_PMIP_MRI-CGCM3_BIO_15\" # In each iteration, 'i' is a GCM lgm_by_gcm <- lapply(gcms, function(i){   #Subset variables that belong to GCM i   lgm_gcm_i <- lgm_chelsa[[grepl(i, names(lgm_chelsa))]]   #Rename variables   names(lgm_gcm_i) <- c(\"bio1\", \"bio7\", \"bio12\", \"bio15\")   return(lgm_gcm_i) }) names(lgm_by_gcm) <- gcms # Check units of variables in present #> minmax(present_chelsa[[c(\"bio1\", \"bio7\", \"bio12\", \"bio15\")]]) #>         bio1     bio7    bio12    bio15 #> min 12.87700 10.11300 1211.605 10.32925 #> max 24.70025 21.06125 3063.049 70.45125  # Check units of variables in LGM (CCSM4) minmax(lgm_by_gcm$CCSM4) #>         bio1     bio7    bio12    bio15 #> min 2822.425 173.5125 10710.62  86.9125 #> max 2946.758 219.7700 23159.20 659.0025 lgm_fixed_units <- lapply(lgm_by_gcm, function(x){   x$bio1 <- (x$bio1/10) - 273 #Divide by 10 and subtracts -273 to convert to ºC   x$bio7 <- x$bio7/10 #Divide by 10   x$bio12 <- x$bio12/10 #Divide by 10   x$bio15 <- x$bio15/10 #Divide by 10   return(x) })  #Check units minmax(lgm_fixed_units$CCSM4) #>         bio1     bio7    bio12    bio15 #> min  9.24250 17.35125 1071.062  8.69125 #> max 21.67575 21.97700 2315.920 65.90025 # Create directory to save processed lgm variables dir_lgm <- file.path(tempdir(), \"LGM_CHELSA\") dir.create(dir_lgm)  # In each iteration, i is one of the GCMs  lapply(names(lgm_fixed_units), function(i){   #Subset spatraster from GCM i   r_i <- lgm_fixed_units[[i]]   #Name the file with the Period (LGM) and GCM (i)   filename_i <- paste0(\"CHELSA_LGM_\", i, \".tif\")    #Write Raster   writeRaster(r_i,                filename = file.path(dir_lgm, filename_i)) })"},{"path":"https://marlonecobos.github.io/kuenm2/articles/projections_chelsa.html","id":"organize-and-structure-variables","dir":"Articles","previous_headings":"","what":"Organize and structure variables","title":"8. Example of Projections Using CHELSA Data","text":"points must : variables stored TIF files, one file per scenario. names raster variables containing patterns identify time periods, GCMS SSPs (case future scenarios). variables names (bio1, bio2, etc) scenarios names units current variables used calibrate model. Now, can use function organize_for_projection() organize files specific hierarchical manner compatible kuenm2. function requires: present_file, past_files future_files: character vectors full path variables scenarios interest (present, past future, respectively). listing files, important set full.names = TRUE list.files(). past_period /future_period: character vectors specifing time periods past future, respectively. Examples past periods LGM MID. Examples future periods includes “2061-2080” “2100”. Remember periods needs part filenames. past_gcm /future_gcm: character vectors specifing specific GCMS past /future, respectively. Examples GCMS “CCSM4”, “CNRM-CM5”, “FGOALS-g2”. Remember GCMs needs part filenames. future_pscen: character vectors specifing specific emission scenarios future, respectively. Examples future_pscen “ssp126” “ssp585. Remember scenarios needs part filenames. variables_names models: character variables names fitted_model object (variables used model extracted). function also allows use spatial object (SpatVector, SpatRaster, SpatExtent) mask variables, append static variables (.e., topographic variables) climatic variables. Let’s use organize_for_projection() function organize past variables representing LGM conditions. first step using list.files(path, full.names = TRUE list path variables representing present LGM conditions. Remembering, save processed current variables present_dir directory LGM variables dir_lgm: Let’s check listed files ensure storing full path variables scenario: check files, ready organize : can check files structured hierarchically nested folders using dir_tree() function fs package: organizing variables, next step create prepared_projection object. , let’s import fitted_model provided data example kuenm2 package, calibrated using current variables CHELSA used example. information data preparation, model calibration model exploration, consult respective vignetts.","code":"present_list <- list.files(path = dir_current,                          pattern = \"Current_CHELSA\",                         full.names = TRUE) lgm_list <- list.files(path = dir_lgm,                          pattern = \"LGM\",                         full.names = TRUE) present_list #The paths in your computer will be different #> \"Local\\\\Temp\\\\Current_CHELSA.tif\"  lgm_list #The paths in your computer will be different #> [1] \"Local\\\\Temp\\\\CHELSA_LGM_CCSM4.tif\"        #> [2] \"Local\\\\Temp\\\\CHELSA_LGM_CNRM-CM5.tif\"     #> [3] \"Local\\\\Temp\\\\CHELSA_LGM_FGOALS-g2.tif\"    #> [4] \"Local\\\\Temp\\\\CHELSA_LGM_IPSL-CM5A-LR.tif\" #> [5] \"Local\\\\Temp\\\\CHELSA_LGM_MIROC-ESM.tif\"    #> [6] \"Local\\\\Temp\\\\CHELSA_LGM_MPI-ESM-P.tif\"    #> [7] \"Local\\\\Temp\\\\CHELSA_LGM_MRI-CGCM3.tif\" # Create a directory to save the variables #Here, in a temporary directory. Change to your work directory in your computer out_dir <- file.path(tempdir(), \"Projection_variables\")  organize_for_projection(output_dir = out_dir,                          variable_names = c(\"bio1\", \"bio7\", \"bio12\", \"bio15\"),                          present_file = present_list,                          past_files = lgm_list,                          past_period = \"LGM\",                          past_gcm = c(\"CCSM4\", \"CNRM-CM5\", \"FGOALS-g2\",                                       \"IPSL-CM5A-LR\", \"MIROC-ESM\", \"MPI-ESM-P\",                                      \"MRI-CGCM3\"),                         resample_to_present = TRUE,                         overwrite = TRUE) #>  #> Variables successfully organized in directory: #> /tmp/Rtmp1ppdrR/Projection_variables #Install package if necessary if(!require(\"fs\")){   install.packages(\"fs\") } dir_tree(out_dir) #> Local\\Temp\\Projection_variables #> ├── Past #> │   └── LGM #> │       ├── CCSM4 #> │       │   └── Variables.tif #> │       ├── CNRM-CM5 #> │       │   └── Variables.tif #> │       ├── FGOALS-g2 #> │       │   └── Variables.tif #> │       ├── IPSL-CM5A-LR #> │       │   └── Variables.tif #> │       ├── MIROC-ESM #> │       │   └── Variables.tif #> │       ├── MPI-ESM-P #> │       │   └── Variables.tif #> │       └── MRI-CGCM3 #> │           └── Variables.tif #> └── Present #>     └── Variables.tif"},{"path":"https://marlonecobos.github.io/kuenm2/articles/projections_chelsa.html","id":"preparation-of-data-for-model-projections","dir":"Articles","previous_headings":"","what":"Preparation of data for model projections","title":"8. Example of Projections Using CHELSA Data","text":"Now, let’s prepare data model projections across multiple scenarios, storing paths rasters representing scenario. need paths folders raster files stored. includes variables present time, used calibrate fit models. addition storing paths variables scenario, function also verifies variables used fit final models available across scenarios. perform check, need provide either fitted_models object intend use projection simply variable names. strongly suggest using fitted_models object minimize projection errors. also need define root directory containing scenarios projection (present, past, /future), along additional information regarding time periods, SSPs, GCMs. print projection_data object, summarizes scenarios predict also shows root directory predictor rasters stored:","code":"#Define present_dir and past_dir in_dir_present <- file.path(out_dir, \"Present\") in_dir_past <- file.path(out_dir, \"Past\")  # Prepare projections using fitted models to check variables pr <- prepare_projection(models = fitted_model_chelsa,                          present_dir = in_dir_present, #Directory with present-day variables                          past_dir = in_dir_past,                           past_period = \"LGM\",                           past_gcm = c(\"CCSM4\", \"CNRM-CM5\", \"FGOALS-g2\",                                        \"IPSL-CM5A-LR\", \"MIROC-ESM\", \"MPI-ESM-P\",                                       \"MRI-CGCM3\"),                           future_dir = NULL, #NULL because we won't project to the past                          future_period = NULL, #NULL because we won't project to the past                          future_pscen = NULL, #NULL because we won't project to the past                          future_gcm = NULL) #NULL because we won't project to the past pr #> projection_data object summary #> ============================== #> Variables prepared to project models for Present and Past  #> Past projections contain the following periods and GCMs: #>   - Periods: LGM  #>   - GCMs: CCSM4 | CNRM-CM5 | FGOALS-g2 | IPSL-CM5A-LR | MIROC-ESM | MPI-ESM-P | MRI-CGCM3  #> All variables are located in the following root directory: #> Local/Temp/Projection_variables"},{"path":"https://marlonecobos.github.io/kuenm2/articles/projections_chelsa.html","id":"project-selected-models-to-multiple-scenarios","dir":"Articles","previous_headings":"","what":"Project selected models to multiple scenarios","title":"8. Example of Projections Using CHELSA Data","text":"preparing data, can use project_selected() function predict selected models across multiple scenarios specified prepare_projections. comprehensive detail step, please see Project models multiple scenarios vignette.","code":"## Create a folder to save projection results #Here, in a temporary directory out_dir_projections <- file.path(tempdir(), \"Projection_results/chelsa\") dir.create(out_dir_projections, recursive = TRUE)  ## Project selected models to multiple scenarios p <- project_selected(models = fitted_model_chelsa,                        projection_data = pr,                       out_dir = out_dir_projections,                        write_replicates = TRUE,                       progress_bar = FALSE, overwrite = T) #Do not print progress bar  #Import mean of each projected scenario p_mean <- import_projections(projection = p, consensus = \"mean\") #Plot all scenarios plot(p_mean, cex.main = 0.8)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/projections_chelsa.html","id":"compute-changes","dir":"Articles","previous_headings":"","what":"Compute changes","title":"8. Example of Projections Using CHELSA Data","text":"projecting niche model different temporal scenarios (past future), species’ areas can classified three categories relative current baseline: gain, loss stability. interpretation categories depends temporal direction projection. projecting future scenarios: Gain: Areas currently unsuitable become suitable future. Loss: Areas currently suitable become unsuitable future. Stability: Areas retain current classification future, whether suitable unsuitable. projecting past scenarios: Gain: Areas unsuitable past now suitable present. Loss: Areas suitable past now unsuitable present. Stability: Areas retain past classification present, whether suitable unsuitable. outcomes may vary across different General Circulation Models (GCMs) within time scenario (e.g., various Shared Socioeconomic Pathways (SSPs) period). projection_changes() function summarizes number GCMs predicting gain, loss, stability time scenario. , present example projected changes LGM conditions current period. example changes present future conditions, along details function, see Exploring Model Uncertainty Variability vignette.  example, can see present-day conditions, GCMs indicate species lost much suitable area LGM.","code":"changes <- projection_changes(model_projections = p, consensus = \"mean\",                               output_dir = out_dir_projections,                                write_bin_models = TRUE, # Write individual binarized results                               return_raster = TRUE, overwrite = TRUE)  #Set colors summary_with_colors <- colors_for_changes(changes_projections = changes)  #Plot plot(summary_with_colors$Summary_changes)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"7. Exploring Model Uncertainty and Variability","text":"Introduction Set Colors Change Maps Types os results Importing Results Save changes_projections Importing Results Saving variability_projections Object Distances Basic Simple Combined Towards High/Low Towards High/Low End Handling -Range Values: NA Zero Comparing MOP Results Response Curves Saving Importing MOP Results","code":""},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"7. Exploring Model Uncertainty and Variability","text":"projecting models across multiple scenarios using project_selected() function, several types analyses can performed: Compute changes scenarios using projection_changes() function. Explore variability arising replicates, model parameterizations, General Circulation Models (GCMs) projection_variability(). Assess extrapolation risks analysis projection_mop(). analyses require model_projections object, output project_selected(). object contains predicted scenario data well directory paths projection rasters saved. create object, follow steps outlined described “Project models multiple scenarios” vignette.","code":"#Load packages library(kuenm2) library(terra) #> terra 1.8.86  #Import calib_results_maxnet data(\"fitted_model_maxnet\", package = \"kuenm2\")  # Import path to raster files with future predictors provided as example # The data is located in the \"inst/extdata\" folder. in_dir <- system.file(\"extdata\", package = \"kuenm2\")  # Import raster layers (same used to calibrate and fit final models) var <- rast(system.file(\"extdata\", \"Current_variables.tif\", package = \"kuenm2\")) #Get soilType soiltype <- var$SoilType  # Organize and structure WorldClim files #Create folder to save structured files out_dir_future <- file.path(tempdir(), \"Future_raw\") #Here, in a temporary directory #Organize organize_future_worldclim(input_dir = in_dir, #Path to the raw variables from WorldClim                           output_dir = out_dir_future,                            name_format = \"bio_\", #Name format                           fixed_variables = var$SoilType, #Static variables                           progress_bar = FALSE, overwrite = TRUE) #>  #> Variables successfully organized in directory: #> /tmp/RtmpGOummG/Future_raw  # Create a \"Current_raw\" folder in a temporary directory #and copy the rawvariables there. out_dir_current <- file.path(tempdir(), \"Current_raw\") dir.create(out_dir_current, recursive = TRUE)  # Save current variables in temporary directory terra::writeRaster(var, file.path(out_dir_current, \"Variables.tif\"),                     overwrite = TRUE)  # Prepare projections using fitted models to check variables pr <- prepare_projection(models = fitted_model_maxnet,                          present_dir = out_dir_current, #Directory with present-day variables                          future_dir = out_dir_future, #Directory with future variables                          future_period = c(\"2041-2060\", \"2081-2100\"),                          future_pscen = c(\"ssp126\", \"ssp585\"),                          future_gcm = c(\"ACCESS-CM2\", \"MIROC6\"))  # Project selected models to multiple scenarios ## Create a folder to save projection results #Here, in a temporary directory out_dir <- file.path(tempdir(), \"Projection_results/maxnet\") dir.create(out_dir, recursive = TRUE)  ## Project selected models to multiple scenarios p <- project_selected(models = fitted_model_maxnet,                        projection_data = pr,                       out_dir = out_dir,                       write_replicates = TRUE,                       progress_bar = FALSE, #Do not print progress bar                       overwrite = TRUE)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"compute-changes-between-scenarios","dir":"Articles","previous_headings":"","what":"Compute Changes Between Scenarios","title":"7. Exploring Model Uncertainty and Variability","text":"projecting niche model different temporal scenarios (past future), species’ areas can classified three categories relative current baseline: gain, loss stability. interpretation categories depends temporal direction projection. projecting future scenarios: Gain: Areas currently unsuitable become suitable future. Loss: Areas currently suitable become unsuitable future. Stability: Areas retain current classification future, whether suitable unsuitable. projecting past scenarios: Gain: Areas unsuitable past now suitable present. Loss: Areas suitable past now unsuitable present. Stability: Areas retain past classification present, whether suitable unsuitable. outcomes may vary across different General Circulation Models (GCMs) within time scenario (e.g., various Shared Socioeconomic Pathways (SSPs) period). projection_changes() function summarizes number GCMs predicting gain, loss, stability time scenario. default, function writes summary results disk (unless write_results set FALSE), save binarized results individual GCMs. example , demonstrate configure function return resulting rasters write binarized results disk.","code":"changes <- projection_changes(model_projections = p,                                output_dir = out_dir,                                write_bin_models = TRUE, # Write individual binarized results                               return_raster = TRUE)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"set-colors-for-change-maps","dir":"Articles","previous_headings":"Compute Changes Between Scenarios","what":"Set Colors for Change Maps","title":"7. Exploring Model Uncertainty and Variability","text":"plotting results, can use colors_for_changes() function assign custom colors areas gain, loss, stability. default, function uses ‘teal green’ gains, ‘orange-red’ losses, ‘Oxford blue’ areas remain suitable, ‘grey’ areas remain unsuitable. However, can customize colors needed. opacity color automatically adjusted based number GCMs: highest (defined max_alpha) GCMs agree prediction, decreases progressively (min_alpha) fewer GCMs support outcome. function returns changes_projections object, color tables embedded SpatRasters. colors automatically applied visualizing data using plot().","code":"#Set colors for change maps changes_col <- colors_for_changes(changes)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"types-os-results","dir":"Articles","previous_headings":"Compute Changes Between Scenarios","what":"Types os results","title":"7. Exploring Model Uncertainty and Variability","text":"projection_changes() function returns four main types results: binarized models, results GCM, results change, general summary considering GCMs: Binarized models GCM: suitable/unsuitable maps binarized individual GCM. default, binarization applies omission error threshold used selecting best models (e.g., 10%). can specify different threshold using user_threshold argument.  Results gcm: provides computed changes (gain, loss, stability) GCM individually.  Results change: list SpatRaster represents specific type change (e.g., gain, loss, stability) across GCMs given scenario.  Summary changes: provides general summary indicating many GCMs project gain, loss, stability scenario.","code":"plot(changes_col$Binarized, cex.main = 0.8) plot(changes_col$Results_by_gcm, cex.main = 0.8) # Results by change for the scenario of 2041-2060 (ssp126) plot(changes_col$Results_by_change$`Future_2041-2060_ssp126`) plot(changes_col$Summary_changes,       plg=list(cex=0.75)) #Decrease size of legend text"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"importing-results","dir":"Articles","previous_headings":"Compute Changes Between Scenarios","what":"Importing Results","title":"7. Exploring Model Uncertainty and Variability","text":"return_raster = TRUE set, resulting SpatRaster objects returned within changes object. default, however, return_raster = FALSE object contains directory path results saved. case, saved results can imported using import_projections() function. can specify type computed changes import, along target period emission scenario. changes_projections object imported using import_projections() can also used input colors_for_changes() customize colors used plotting. example, import general summary 2041–2060 period SSP5-8.5 scenario:","code":"general_changes <- import_projections(projection = changes,                                        future_period = \"2041-2060\",                                        future_pscen = \"ssp585\",                                       change_types = \"summary\") #Set colors general_changes <- colors_for_changes(general_changes)  #Plot plot(general_changes$Summary, main = names(general_changes$Summary),      plg=list(cex=0.75)) #Decrease size of legend text"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"save-changes_projections","dir":"Articles","previous_headings":"Compute Changes Between Scenarios","what":"Save changes_projections","title":"7. Exploring Model Uncertainty and Variability","text":"changes_projections object list contains resulting SpatRaster objects (return_raster = TRUE) directory path results saved (write_results = TRUE). results written disk initial run, can save SpatRaster objects afterward using writeRaster() function. example, save general summary raster: results saved disk, changes_projections object automatically stored folder named Projection_changes inside specified output_dir. can load back R using readRDS(): loading, object can used import specific results import_projections() function.","code":"writeRaster(changes$Summary_changes,             file.path(out_dir, \"Summary_changes.tif\")) changes <- readRDS(file.path(out_dir, \"Projection_changes/changes_projections.rds\"))"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"explore-variance","dir":"Articles","previous_headings":"","what":"Explore Variance","title":"7. Exploring Model Uncertainty and Variability","text":"projecting niche models, predictions can vary across different replicates, selected models, GCMs. projection_variability() function quantifies spatializes sources variability, offering valuable insights prediction uncertainty. function requires model_projections object, generated project_selected() function. default, projection_variability() uses median consensus summarize variance across selected models GCMs. Alternatively, users can specify summary statistics mean, range, sd (standard deviation). analyze variability originating replicates, ensure write_replicates = TRUE set running project_selected(). , demonstrate calculate variance different sources (replicates, models, GCMs) save results designated out_dir directory. output variability_projections object, list containing SpatRaster layers represent variance attributed replicates, models, GCMs scenario, including present time. results highlight areas prediction uncertainty higher. example, present time scenario, variance mainly arises differences among replicates  pessimistic scenario (SSP5-8.5) 2081–2100 period, slight variance observed, primarily arising replicates different GCMs used.","code":"# Create a directory to save results v <- projection_variability(model_projections = p, write_files = TRUE,                             output_dir = out_dir,                             verbose = FALSE, overwrite = T) # Variance for the present time plot(v$Present, range = c(0, 0.15)) plot(v$`Future_2081-2100_ssp585`, range = c(0, 0.1))"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"importing-results-1","dir":"Articles","previous_headings":"Explore Variance","what":"Importing Results","title":"7. Exploring Model Uncertainty and Variability","text":"write_files = TRUE set, variability_projections object includes file path results saved. can use path import_projections() function load results whenever needed. example, import results 2041–2060 period SSP1-2.6 scenario. scenario, variability mainly originates differences among selected models.","code":"# See the folder where the results were saved v$root_directory #> [1] \"Temp/Projection_results/maxnet/variance\" v_2041_2060_ssp126 <- import_projections(projection = v,                                           present = FALSE, #Do not import results from the present time                                          future_period = \"2041-2060\",                                           future_pscen = \"ssp126\") # Plot plot(v_2041_2060_ssp126, range = c(0, 0.1))"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"saving-the-variability_projections-object","dir":"Articles","previous_headings":"Explore Variance","what":"Saving the variability_projections Object","title":"7. Exploring Model Uncertainty and Variability","text":"variability_projections object list contains resulting SpatRaster layers (return_raster = TRUE) directory path results saved (write_files = TRUE). results saved disk initial run, can save SpatRaster layers afterward using writeRaster() function. example, save variability map one future scenarios: results saved disk, variability_projections object automatically stored folder named variance within specified output_dir. can reload R using readRDS() function: object can used import results import_projections() function.","code":"writeRaster(v$`Future_2081-2100_ssp585`,              file.path(out_dir, \"Future_2081-2100_ssp585.tif\")) v <- readRDS(file.path(out_dir, \"variance/variability_projections.rds\"))"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"analyze-extrapolation-risks-using-the-mop-metric","dir":"Articles","previous_headings":"","what":"Analyze Extrapolation Risks Using the MOP Metric","title":"7. Exploring Model Uncertainty and Variability","text":"projecting model predictions new regions time periods, common encounter non-analogous conditions, environmental conditions present calibration area. example, maximum temperature (bio_1) model’s calibration area 22.7∘C22.7^\\circ C: However, future scenarios, conditions projected become warmer, temperatures may reach higher values. illustrate , let’s import environmental variables one GCMs (ACCESS-CM2) future scenario SSP5-8.5 examine maximum temperature:  Note projected area, temperatures expected exceed current maximum temperature. projection_mop() function performs Mobility-Oriented Parity (MOP) analysis (Owens et al. 2013), identifies areas non-analogous environmental conditions pose extrapolation risks. also quantifies dissimilar conditions projection area relative calibration data. MOP analysis requires following inputs: wish perform MOP variables used selected models, set subset_variables = TRUE provide fitted_models object. object class projection_data returned prepare_projection() function, contains paths raster layers representing environmental conditions projected scenarios. default, projection_mop() performs basic MOP, highlights regions non-analogous conditions relative calibration data. Alternatively, can set: - type = \"simple\" compute number variables non-analogous conditions per location. - type = \"detailed\" identify exactly variables exhibit non-analogous conditions. , perform detailed MOP identify areas extrapolation risk future scenarios predictions made: function returns mop_projections object, contains paths directories results saved. object can used import_projections() function load results.","code":"max(fitted_model_maxnet$calibration_data$bio_1) #> [1] 22.6858 #Import variables from the 2081-2100 period (SSP585, GCM MIROC6) future_ACCESS_CM2 <- rast(file.path(out_dir_future,                                 \"2081-2100/ssp585/ACCESS-CM2/Variables.tif\")) minmax(future_ACCESS_CM2$bio_1) #>     bio_1 #> min  17.8 #> max  29.6  #Plot plot(future_ACCESS_CM2$bio_1,       breaks = c(-Inf, 22.7, Inf)) #Highlight regions with temperature above 22.7ºC # Create a folder to save MOP results out_dir_mop <- file.path(tempdir(), \"MOPresults\") dir.create(out_dir_mop, recursive = TRUE)  # Run MOP kmop <- projection_mop(data = fitted_model_maxnet, projection_data = pr,                         subset_variables = TRUE,                        calculate_distance = TRUE,                        out_dir = out_dir_mop,                         type = \"detailed\",                         overwrite = TRUE, progress_bar = FALSE)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"mop-types","dir":"Articles","previous_headings":"Analyze Extrapolation Risks Using the MOP Metric","what":"MOP types","title":"7. Exploring Model Uncertainty and Variability","text":"results MOP analysis provide multiple perspectives extrapolation risks. component mop_projections object captures different aspect dissimilarity environmental conditions calibration projection areas. importing results, can specify scenarios (e.g., “2081-2100” “ssp585”) well type MOP results load. default, available MOP types imported, including: basic simple towards_high_combined towards_low_combined towards_high_end towards_low_end , examine MOP results SSP5-8.5 scenario 2081–2100 period:","code":"mop_ssp585_2100 <- import_projections(projection = kmop,                                       future_period = \"2081-2100\",                                        future_pscen = \"ssp585\") #See types of results names(mop_ssp585_2100) #> [1] \"distances\"             \"simple\"                \"basic\"                 #> [4] \"towards_high_combined\" \"towards_low_combined\"  \"towards_high_end\"      #> [7] \"towards_low_end\""},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"distances","dir":"Articles","previous_headings":"Analyze Extrapolation Risks Using the MOP Metric > MOP types","what":"Distances","title":"7. Exploring Model Uncertainty and Variability","text":"distances result represents Euclidean Mahalanobis distance projected environmental conditions (G) calibration dataset (M). Higher distance values indicate greater dissimilarity calibration conditions, highlighting areas increased extrapolation risk.","code":"plot(mop_ssp585_2100$distances)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"basic","dir":"Articles","previous_headings":"Analyze Extrapolation Risks Using the MOP Metric > MOP types","what":"Basic","title":"7. Exploring Model Uncertainty and Variability","text":"basic result identifies areas least one environmental variable differs reference (calibration) conditions. value ‘1’ indicates presence non-analogous conditions specific area scenario.","code":"plot(mop_ssp585_2100$basic)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"simple","dir":"Articles","previous_headings":"Analyze Extrapolation Risks Using the MOP Metric > MOP types","what":"Simple","title":"7. Exploring Model Uncertainty and Variability","text":"simple result quantifies number environmental variables projected area non-analogous calibration data.","code":"plot(mop_ssp585_2100$simple)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"combined-towards-highlow","dir":"Articles","previous_headings":"Analyze Extrapolation Risks Using the MOP Metric > MOP types","what":"Combined Towards High/Low","title":"7. Exploring Model Uncertainty and Variability","text":"towards_high_combined towards_low_combined results identify variables exhibit non-analogous conditions. Specifically, towards_high_combined highlights variables values exceeding observed calibration data, towards_low_combined highlights variables values calibration range.","code":"# Non-analogous conditions towards high values plot(mop_ssp585_2100$towards_high_combined) # Non-analogous conditions towards low values plot(mop_ssp585_2100$towards_low_combined)"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"towards-highlow-end","dir":"Articles","previous_headings":"Analyze Extrapolation Risks Using the MOP Metric > MOP types","what":"Towards High/Low End","title":"7. Exploring Model Uncertainty and Variability","text":"towards_high_end towards_low_end results similar “combined” counterparts provide individual SpatRaster layers variable.","code":"# Non-analogous conditions towards high values in the ACCESS-CM2 scenario plot(mop_ssp585_2100$towards_high_end$`Future_2081-2100_ssp585_ACCESS-CM2`) # Non-analogous conditions towards low values in the MIROC6 scenario plot(mop_ssp585_2100$towards_low_end$`Future_2081-2100_ssp585_ACCESS-CM2`,      main = names(mop_ssp585_2100$towards_low_end$`Future_2081-2100_ssp585_ACCESS-CM2`))"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"handling-in-range-values-na-or-zero","dir":"Articles","previous_headings":"Analyze Extrapolation Risks Using the MOP Metric","what":"Handling In-Range Values: NA or Zero","title":"7. Exploring Model Uncertainty and Variability","text":"default, projection_mop() function assigns NA cells whose values fall within range calibration data, indicating considered analogous. Alternatively, can assign value 0 cells setting na_in_range = FALSE running projection_mop(). Let’s explore setting affects simple detailed MOP outputs:","code":"# Create a folder to save MOP results, now assigning 0 to cells within the range out_dir_mop_zero <- file.path(tempdir(), \"MOPresults_zero\") dir.create(out_dir_mop_zero, recursive = TRUE)  # Run MOP kmop_zero <- projection_mop(data = fitted_model_maxnet, projection_data = pr,                              subset_variables = TRUE,                              na_in_range = FALSE, #Assign 0 to cells within range                             calculate_distance = TRUE,                             out_dir = out_dir_mop_zero,                              type = \"detailed\",                              overwrite = TRUE, progress_bar = FALSE) # Import MOP for the scenario ssp585 in 2081-2100 mop_ssp585_2100_zero <- import_projections(projection = kmop_zero,                                            future_period = \"2081-2100\",                                             future_pscen = \"ssp585\")  # Compare with the MOP that assigns NA to cells within the calibration range # Simple MOP plot(c(mop_ssp585_2100$simple$`Future_2081-2100_ssp585_ACCESS-CM2`,         mop_ssp585_2100_zero$simple$`Future_2081-2100_ssp585_ACCESS-CM2`),      main = c(\"Within range as NA\", \"Within range as 0\")) # Detailed MOP plot(c(mop_ssp585_2100$towards_high_combined$`Future_2081-2100_ssp585_ACCESS-CM2`,         mop_ssp585_2100_zero$towards_high_combined$`Future_2081-2100_ssp585_ACCESS-CM2`),      main = c(\"Within range as NA\", \"Within range as 0\"),      plg=list(cex=0.6))"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"comparing-mop-results-with-response-curves","dir":"Articles","previous_headings":"Analyze Extrapolation Risks Using the MOP Metric","what":"Comparing MOP Results with Response Curves","title":"7. Exploring Model Uncertainty and Variability","text":"projection_mop() function identifies areas non-analogous environmental conditions, actual risk extrapolation depends strongly additional factors, especially response curves environmental variables. example, consider future scenario predicted GCM (e.g., ACCESS-CM2 SSP5-8.5 2081–2100), values bio_1 (Annual Mean Temperature), bio_12 (Annual Precipitation) bio_15 (Precipitation Seasonality) exceed upper limits observed calibration data.  Now, let’s examine response curves variables. better visualize model responds range values projected future scenario, can set plotting limits using scenario’s variable values new_data:  response curves bio_1, bio_12, bio_15, higher values correspond lower suitability, reaching zero near upper limit calibration data (indicated dashed line). Beyond upper limit, suitability remains close zero. Given pattern, unlikely suitability increase suddenly even higher values, provides greater confidence model’s extrapolation variables. Next, let’s investigate variables values lower limit calibration data:  regions projected scenario, bio_7 (Temperature Annual Range) exhibits values lower limit calibration data. response curve indicates , extrapolating lower values, suitability continues increase. situation represents higher risk extrapolation substantial uncertainty, don’t know whether suitability truly continues rise low bio_7 values () might eventually decline. example highlights strongly recommend interpreting MOP results alongside response curves.","code":"# Non-analogous conditions towards high values in the MIROC6 scenario plot(mop_ssp585_2100$towards_high_combined$`Future_2081-2100_ssp585_ACCESS-CM2`) par(mfrow = c(1,3)) #Set plot grid response_curve(models = fitted_model_maxnet, variable = \"bio_1\",                 new_data = future_ACCESS_CM2) response_curve(models = fitted_model_maxnet, variable = \"bio_12\",                 new_data = future_ACCESS_CM2) response_curve(models = fitted_model_maxnet, variable = \"bio_15\",                 new_data = future_ACCESS_CM2) #Reinitiate grids on.exit() # Non-analogous conditions towards low values in the MIROC6 scenario par(mfrow = c(1,2)) #Set grid plot(mop_ssp585_2100$towards_low_combined$`Future_2081-2100_ssp585_ACCESS-CM2`) ## It's bio 7. Plot response curve: response_curve(models = fitted_model_maxnet, variable = \"bio_7\",                 new_data = future_ACCESS_CM2) #Reinitiate grid on.exit()"},{"path":"https://marlonecobos.github.io/kuenm2/articles/variability_and_uncertainty.html","id":"saving-and-importing-mop-results","dir":"Articles","previous_headings":"Analyze Extrapolation Risks Using the MOP Metric","what":"Saving and Importing MOP Results","title":"7. Exploring Model Uncertainty and Variability","text":"projection_mop() run, automatically saves mop_projections object RDS file specified out_dir. can reload object R time using readRDS() function:","code":"# Check for RDS files in the directory where we saved the MOP results list.files(path = out_dir_mop, pattern = \"rds\") #> [1] \"mop_projections.rds\"  # Import the mop_projections file kmop <- readRDS(file.path(out_dir_mop, \"mop_projections.rds\"))"},{"path":"https://marlonecobos.github.io/kuenm2/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Weverton C. F. Trindade. Author, maintainer. Luis F. Arias-Giraldo. Author. Luis Osorio-Olvera. Author. . Townsend Peterson. Author. Marlon E. Cobos. Author.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Trindade W, Arias-Giraldo L, Osorio-Olvera L, Peterson , Cobos M (2025). kuenm2: Detailed Development Ecological Niche Models. R package version 0.0.11, https://marlonecobos.github.io/kuenm2/.","code":"@Manual{,   title = {kuenm2: Detailed Development of Ecological Niche Models},   author = {Weverton C. F. Trindade and Luis F. Arias-Giraldo and Luis Osorio-Olvera and A. Townsend Peterson and Marlon E. Cobos},   year = {2025},   note = {R package version 0.0.11},   url = {https://marlonecobos.github.io/kuenm2/}, }"},{"path":"https://marlonecobos.github.io/kuenm2/index.html","id":"kuenm2-detailed-development-of-ecological-niche-models","dir":"","previous_headings":"","what":"Detailed Development of Ecological Niche Models","title":"Detailed Development of Ecological Niche Models","text":"Weverton C. F. Trindade, Luis F. Arias-Giraldo, Luis Osorio-Olvera, . Townsend Peterson, Marlon E. Cobos Package description Installing package Pre-modeling steps Model development Post-modeling analysis Checking vignettes","code":""},{"path":"https://marlonecobos.github.io/kuenm2/index.html","id":"package-description","dir":"","previous_headings":"","what":"Package description","title":"Detailed Development of Ecological Niche Models","text":"kuenm2 new version kuenm Cobos et al. 2019, R package designed make process ecological niche modeling (ENM) easier, faster, reproducible, time robust. aim package facilitate crucial steps ENM process: data preparation, model calibration, selected model exploration, model projections, analyses uncertainty variability. new version package reduces dependency strictly organized working directory (now required projections multiple scenarios needed). Instead, kuenm2 functions generate specific R objects store necessary information subsequent steps. kuenm2 fits maximum entropy (Maxnet) models logistic generalized linear models (GLMs). Maxnet models created described Phillips et al. (2017), GLMs constructed Cobos Peterson (2023). ENM workflow requires minimum data.frame containing occurrence record coordinates (longitude latitude) SpatRaster object predictor variables. Users also use data.frame containing column indicating presence background records (0/1), columns predictor variable values.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/index.html","id":"installing-the-package","dir":"","previous_headings":"","what":"Installing the package","title":"Detailed Development of Ecological Niche Models","text":"Note: Internet connection required install package. install latest release kuenm2 use following line code:  development version kuenm2 can installed using code .  problems? problems installation development version GitHub, restart R session, close RStudio sessions may open, try . installation asked update packages, don’t need specific version one packages installed. packages gives error updating, please install alone using install.packages(), try installing kuenm2 .  load package use:","code":"# Installing from CRAN  #install.packages(\"kuenm2\")  # in progress # Installing and loading packages if(!require(remotes)){   install.packages(\"remotes\") }  # To install the package use remotes::install_github(\"marlonecobos/kuenm2\")  # To install the package and its vignettes use (if needed use: force = TRUE)   remotes::install_github(\"marlonecobos/kuenm2\", build_vignettes = TRUE) # Load kuenm2 library(kuenm2)"},{"path":"https://marlonecobos.github.io/kuenm2/index.html","id":"workflow-in-kuenm2","dir":"","previous_headings":"","what":"Workflow in kuenm2","title":"Detailed Development of Ecological Niche Models","text":"kuenm2 package facilitates following steps ENM process: basic data cleaning, data preparation, model calibration, model exploration, model projections, projection comparisons, exploration variability uncertainty. figure shows schematic view package works. brief description steps can performed package presented . complete description demonstration steps, see package vignettes listed section Checking vignettes. Figure 1. Overview kuenm2 workflow ecological niche modeling.","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/index.html","id":"basic-data-cleaning","dir":"","previous_headings":"Workflow in kuenm2 > Pre-modeling steps","what":"Basic data cleaning","title":"Detailed Development of Ecological Niche Models","text":"kuenm2 automates essential pre-processing steps ecological niche modeling. Data cleaning tools help sort columns, remove missing values duplicates (including duplicates within raster cells), exclude coordinates (0,0), filter based coordinate decimal precision, adjust occurrences fall just outside valid raster boundaries.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/index.html","id":"data-preparation","dir":"","previous_headings":"Workflow in kuenm2 > Pre-modeling steps","what":"Data preparation","title":"Detailed Development of Ecological Niche Models","text":"data preparation, users can input raw occurrence records raster layers provide pre-processed data frame. Two primary functions, prepare_data() prepare_user_data(), guide users process choosing modeling algorithms, defining parameter sets (e.g., feature classes, regularization multipliers, predictor variables), selecting data partitioning strategies. Additional tools allow users visualize environmental conditions calibration data assess similarities among training testing partitions, exclusive feature kuenm2 among ENM tools.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/index.html","id":"data-exploration","dir":"","previous_headings":"Workflow in kuenm2 > Pre-modeling steps","what":"Data exploration","title":"Detailed Development of Ecological Niche Models","text":"data prepared can explored understand looks like environmental geographic spaces. can give users idea expect models, whether changes way data prepared necessary. main functions step workflow explore_calibration_hist(), plot_calibration_hist(), explore_partition_geo(), explore_partition_extrapolation(), plot_explore_partition().","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/index.html","id":"model-calibration","dir":"","previous_headings":"Workflow in kuenm2 > Model development","what":"Model calibration","title":"Detailed Development of Ecological Niche Models","text":"Model calibration kuenm2 computationally intensive, involving training testing candidate models using various partitioning strategies k-fold cross-validation, subsampling, bootstrapping. Custom partitions generated using external tools (e.g., block checkerboard methods) can also imported. Models evaluated using multiple performance criteria emphasize sensitivity due frequent lack true absence data. criteria include: bimodality response curves, partial ROC statistical significance, omission rates predictive performance, Akaike Information Criterion (AIC) model complexity. Model calibration executed using calibration() function. support deeper understanding calibration process, step explore extrapolation risks process training testing models can performed kuenm2. function partition_response_curves() helps produce visualizations response curves training data overlapped environmental conditions testing sites selected models (another option exclusive kuenm2).","code":""},{"path":"https://marlonecobos.github.io/kuenm2/index.html","id":"model-explorations","dir":"","previous_headings":"Workflow in kuenm2 > Model development","what":"Model explorations","title":"Detailed Development of Ecological Niche Models","text":"best models selected, must fitted using fit_selected() analyze characteristics behavior. Fitted models can explored assess variable_importance() examine response curves (response_curve() all_response_curves()). offers insights predictor influence models ecological interpretation. independent set records (included model calibration) available, can used re-evaluate predictive performance selected models using independent_evaluation(). Partial ROC omission rates calculated new records measure model performance, extrapolation risks also assessed points using MOP metric (exclusive kuenm2). Results evaluation can help filter selected models inform decision incorporate independent records calibration dataset.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/index.html","id":"model-projections","dir":"","previous_headings":"Workflow in kuenm2 > Model development","what":"Model projections","title":"Detailed Development of Ecological Niche Models","text":"Fitted models can projected new environmental scenarios, including larger geographic areas different temporal conditions (e.g., past future climates). single scenario projections, use predict_selected(); multiple scenarios, use project_selected(). projecting complex future scenarios (e.g., multiple time periods, emission pathways, GCMs), users first prepare organize data using functions kuenm2 facilitate process (e.g., organize_future_worldclim()).","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/index.html","id":"projection-comparisons","dir":"","previous_headings":"Workflow in kuenm2 > Post-modeling analysis","what":"Projection comparisons","title":"Detailed Development of Ecological Niche Models","text":"projecting multiple scenarios, kuenm2 provides tools quantify characterize differences using projection_changes(). function evaluates spatial changes agreement levels among projections, aiding interpretation temporal scenario-based shifts suitability.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/index.html","id":"variability-and-uncertainty","dir":"","previous_headings":"Workflow in kuenm2 > Post-modeling analysis","what":"Variability and uncertainty","title":"Detailed Development of Ecological Niche Models","text":"assess consistency reliability model outputs, users can explore projection variability (projection_variability()), accounts differences arising model replicates, parameter sets, climate models. MOP metric (projection_mop()) users can evaluate extrapolation risks identifying novel environmental conditions projection areas, offering spatially explicit representation model uncertainty.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/index.html","id":"checking-the-vignettes","dir":"","previous_headings":"","what":"Checking the vignettes","title":"Detailed Development of Ecological Niche Models","text":"Users can check kuenm2 vignettes full explanation package functionality. vignettes can checked online kuenm2 site menu Articles. build vignettes installing package, installing form GitHub, make sure use argument build_vignettes = TRUE. Check vignettes code :","code":"# Guide to basic data cleaning before the ENM process vignette(\"basic_data_cleaning\")  # Guide to prepare data for the ENM process vignette(\"prepare_data\")  # Guide to train and evaluate candidate models, and select based on performance vignette(\"model_calibration\")   # Guide to explore selected models, variable importance, response curves vignette(\"model_exploration\")  # Guide to predict models in geographic space (single scenarios) vignette(\"model_predictions\")  # Guide to project models in geographic space (multiple scenarios) vignette(\"model_projections\")   # Guide to explore variability and uncertainty in projections (multiple scenarios) vignette(\"variability_and_uncertainty\")  # Guide to make projections to multiple scenarios with layers from CHELSA Climate vignette(\"projections_chelsa\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/advanced_cleaning.html","id":null,"dir":"Reference","previous_headings":"","what":"Advanced occurrence data cleaning — advanced_cleaning","title":"Advanced occurrence data cleaning — advanced_cleaning","text":"Advanced processes data cleaning involving duplicate removal movement records.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/advanced_cleaning.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Advanced occurrence data cleaning — advanced_cleaning","text":"","code":"advanced_cleaning(data, x, y, raster_layer, cell_duplicates = TRUE,                   move_points_inside = FALSE, move_limit_distance = NULL,                   verbose = TRUE)  remove_cell_duplicates(data, x, y,                        raster_layer)  move_2closest_cell(data, x, y, raster_layer,                    move_limit_distance, verbose = TRUE)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/advanced_cleaning.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Advanced occurrence data cleaning — advanced_cleaning","text":"data data.frame occurrence records. Rows NA values omitted. x (character) name column data containing longitude values. y (character) name column data containing latitude values. raster_layer raster layer (object class SpatRaster). cell_duplicates (logical) whether remove duplicate coordinates considering raster cells. Default = TRUE. move_points_inside (logical) whether move records outside raster cells valid values closest cell values. Default = FALSE. move_limit_distance maximum distance move records outside cells valid values. Default = NULL. Must defined move_points_inside = TRUE. verbose (logical) whether print messages progress. Default = TRUE.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/advanced_cleaning.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Advanced occurrence data cleaning — advanced_cleaning","text":"data.frame occurrence records resulting advanced cleaning procedures. columns added describe changes made original data.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/advanced_cleaning.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Advanced occurrence data cleaning — advanced_cleaning","text":"Data used functions gone initial processes cleaning filtering.","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/advanced_cleaning.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Advanced occurrence data cleaning — advanced_cleaning","text":"","code":"# Import occurrences data(occ_data_noclean, package = \"kuenm2\")  # Import raster layers var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                    package = \"kuenm2\"))  # Keep only one layer var <- var$bio_1  # all basic cleaning steps clean_init <- initial_cleaning(data = occ_data_noclean, species = \"species\",                                x = \"x\", y = \"y\", remove_na = TRUE,                                remove_empty = TRUE, remove_duplicates = TRUE,                                by_decimal_precision = TRUE,                                decimal_precision = 2)  # Advanced cleaning steps # exclude duplicates based on raster cell (pixel) celldup <- remove_cell_duplicates(data = clean_init, x = \"x\", y = \"y\",                                   raster_layer = var)  # move records to valid pixels moved <- move_2closest_cell(data = celldup, x = \"x\", y = \"y\",                             raster_layer = var, move_limit_distance = 10) #> Moving occurrences to closest pixels...  # the steps at a time clean_data <- advanced_cleaning(data = clean_init, x = \"x\", y = \"y\",                                 raster_layer = var, cell_duplicates = TRUE,                                 move_points_inside = TRUE,                                 move_limit_distance = 10) #> Moving occurrences to closest pixels..."},{"path":"https://marlonecobos.github.io/kuenm2/reference/bias.html","id":null,"dir":"Reference","previous_headings":"","what":"Example Bias File — bias","title":"Example Bias File — bias","text":"SpatRaster object representing bias layer used extracting background points prepare_data() function.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/bias.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Example Bias File — bias","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/bias.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Example Bias File — bias","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/bias.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Example Bias File — bias","text":"","code":"bias <- terra::rast(system.file(\"extdata\", \"bias_file.tif\",                                 package = \"kuenm2\"))  terra::plot(bias)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/binarize_changes.html","id":null,"dir":"Reference","previous_headings":"","what":"Binarize changes based on the agreement among GCMs — binarize_changes","title":"Binarize changes based on the agreement among GCMs — binarize_changes","text":"Highlights areas specified number Global Climate Models (GCMs) agree given outcome projected scenario. function can identify areas classified suitable, unsuitable, stable-suitable, stable-unsuitable, gained, lost.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/binarize_changes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Binarize changes based on the agreement among GCMs — binarize_changes","text":"","code":"binarize_changes(changes_projections, outcome = \"suitable\", n_gcms)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/binarize_changes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Binarize changes based on the agreement among GCMs — binarize_changes","text":"changes_projections object class changes_projections, generated projection_changes() imported using import_projections(), containing $Summary_changes element. outcome (character) outcome binarize. Available options \"suitable\", \"unsuitable\", \"stable-suitable\", \"stable-unsuitable\", \"gain\" \"loss\". Default \"suitable\". See details. n_gcms (numeric) minimum number GCMs must agree specified outcome cell included category.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/binarize_changes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Binarize changes based on the agreement among GCMs — binarize_changes","text":"SpatRaster list SpatRaster objects (one per scenario) binarized outcomes. example, outcome = \"suitable\" n_gcms = 3, cells value 1 indicate areas three GCMs agree area suitable species scenario.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/binarize_changes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Binarize changes based on the agreement among GCMs — binarize_changes","text":"interpretation outcomes depends temporal direction projection. projecting future scenarios: suitable: Areas remain suitable (stable-suitable) become suitable (gain) future. unsuitable: Areas remain unsuitable (stable-unsuitable) become unsuitable (loss) future. gain: Areas currently unsuitable become suitable future. loss: Areas currently suitable become unsuitable future. stable-suitable stable-unsuitable: Areas retain current classification future, whether suitable unsuitable. projecting past scenarios: suitable: Areas remain suitable (stable-suitable) become unsuitable (loss) present. unsuitable: Areas remain unsuitable (stable-unsuitable) become suitable (gain) present gain: Areas unsuitable past now suitable present. loss: Areas suitable past now unsuitable present. stable-suitable stable-unsuitable: Areas retain current classification future, whether suitable unsuitable.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/binarize_changes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Binarize changes based on the agreement among GCMs — binarize_changes","text":"","code":"# Step 1: Organize variables for current projection ## Import current variables (used to fit models) var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  ## Create a folder in a temporary directory to copy the variables out_dir_current <- file.path(tempdir(), \"Current_raw_bin\") dir.create(out_dir_current, recursive = TRUE)  ## Save current variables in temporary directory terra::writeRaster(var, file.path(out_dir_current, \"Variables.tif\"))   # Step 2: Organize future climate variables (example with WorldClim) ## Directory containing the downloaded future climate variables (example) in_dir <- system.file(\"extdata\", package = \"kuenm2\")  ## Create a folder in a temporary directory to copy the future variables out_dir_future <- file.path(tempdir(), \"Future_raw_bin\")  ## Organize and rename the future climate data (structured by year and GCM) ### 'SoilType' will be appended as a static variable in each scenario organize_future_worldclim(input_dir = in_dir, output_dir = out_dir_future,                           name_format = \"bio_\", fixed_variables = var$SoilType) #>    |                                                                               |                                                                      |   0%   |                                                                               |=========                                                             |  12%   |                                                                               |==================                                                    |  25%   |                                                                               |==========================                                            |  38%   |                                                                               |===================================                                   |  50%   |                                                                               |============================================                          |  62%   |                                                                               |====================================================                  |  75%   |                                                                               |=============================================================         |  88%   |                                                                               |======================================================================| 100% #>  #> Variables successfully organized in directory: #> /tmp/Rtmp3eUeuI/Future_raw_bin  # Step 3: Prepare data to run multiple projections ## An example with maxnet models ## Import example of fitted_models (output of fit_selected()) data(fitted_model_maxnet, package = \"kuenm2\")  ## Prepare projection data using fitted models to check variables pr <- prepare_projection(models = fitted_model_maxnet,                          present_dir = out_dir_current,                          future_dir = out_dir_future,                          future_period = c(\"2081-2100\"),                          future_pscen = c(\"ssp585\"),                          future_gcm = c(\"ACCESS-CM2\", \"MIROC6\"),                          raster_pattern = \".tif*\")  # Step 4: Run multiple model projections ## A folder to save projection results out_dir <- file.path(tempdir(), \"Projection_results/maxnet_bin\") dir.create(out_dir, recursive = TRUE)  ## Project selected models to multiple scenarios p <- project_selected(models = fitted_model_maxnet, projection_data = pr,                       out_dir = out_dir) #>    |                                                                               |                                                                      |   0%   |                                                                               |=======================                                               |  33%   |                                                                               |===============================================                       |  67%   |                                                                               |======================================================================| 100%  # Step 5: Identify areas of change in projections ## Contraction, expansion and stability changes <- projection_changes(model_projections = p, write_results = FALSE,                               return_raster = TRUE)  # Step 6: Binarize changes future_suitable <- binarize_changes(changes_projections = changes,                                     outcome = \"suitable\",                                     n_gcms = 1) terra::plot(future_suitable)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/bivariate_response.html","id":null,"dir":"Reference","previous_headings":"","what":"Bivariate response plot for fitted models — bivariate_response","title":"Bivariate response plot for fitted models — bivariate_response","text":"plot suitability prediction two-dimensional environmental space.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/bivariate_response.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bivariate response plot for fitted models — bivariate_response","text":"","code":"bivariate_response(models, variable1 , variable2, modelID = NULL, n = 500,                    new_data = NULL, extrapolate = TRUE, add_bar = TRUE ,                    add_limits = TRUE, color_palette  = NULL,                    xlab = NULL, ylab = NULL, ...)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/bivariate_response.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bivariate response plot for fitted models — bivariate_response","text":"models object class fitted_models returned fit_selected() function. variable1 (character) name variable plotted x axis. variable2 (character) name variable plotted y axis. modelID (character) name ModelID presents fitted object. Default = NULL. n (numeric) number breaks plotting grid. Default = 500 new_data SpatRaster, data.frame,  matrix variables representing area interest. Default = NULL. extrapolate (logical) whether allow extrapolation study behavior response outside calibration limits. Ignored new_data defined. Default = TRUE. add_bar (logical) whether add bar legend. Default = TRUE. add_limits (logical) whether add calibration limits extrapolate = TRUE. Default = TRUE. color_palette (function) color palette function used assign colors plot. default, NULL uses rev(hcl.colors(n, \"terrain\")). xlab (character) label x axis. default, NULL, uses name defined variable1. ylab (character) label y axis. default, NULL, uses name defined variable2. ... additional arguments passed image.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/bivariate_response.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bivariate response plot for fitted models — bivariate_response","text":"bivariate plot considering variable1 variable2.","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/bivariate_response.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bivariate response plot for fitted models — bivariate_response","text":"","code":"# Example with glmnet # Import example of fitted_models (output of fit_selected()) data(fitted_model_maxnet, package = \"kuenm2\")  # Response curve (notice response affected by covariance) bivariate_response(models = fitted_model_maxnet, modelID = \"Model_219\",                    variable1 = \"bio_1\", variable2 = \"bio_12\")   # Example with glm # Import example of fitted_models (output of fit_selected()) data(fitted_model_glm, package = \"kuenm2\")  # Response curve bivariate_response(models = fitted_model_glm, modelID = \"Model_85\",                    variable1 = \"bio_1\", variable2 = \"bio_7\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/calib_results_glm.html","id":null,"dir":"Reference","previous_headings":"","what":"Calibration Results (glm) — calib_results_glm","title":"Calibration Results (glm) — calib_results_glm","text":"calibration_results object resulted calibration() using maxnet algorithm","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/calib_results_glm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calibration Results (glm) — calib_results_glm","text":"","code":"data(\"calib_results_glm\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/calib_results_glm.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Calibration Results (glm) — calib_results_glm","text":"calibration_results following elements: species Species names calibration_data data.frame variables extracted presence background points formula_grid data.frame ID, formulas, regularization multipliers candidate model part_data list partition data, element corresponds replicate contains indices test points replicate partition_method character indicating partition method n_replicates numeric value indicating number replicates k-folds train_proportion numeric value indicating proportion occurrences used train points partition method 'subsample' 'boostrap' data_xy data.frame coordinates occurrence bakground points continuous_variables character indicating names continuous variables categorical_variables character indicating names categorical variables weights numeric value specifying weights occurrence records. NULL, meaning set weights. pca prcomp object storing PCA information. NULL, meaning PCA performed algorithm character indicanting algorithm (glm) calibration_results list containing evaluation metrics candidate model omission_rate numeric value indicating omission rate used evaluate models (10%) addsamplestobackground logical value indicating whether add background presence sample already . selected_models data.frame formulas evaluation metrics selected model summary list number ID models removed selected selection procedure","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/calib_results_maxnet.html","id":null,"dir":"Reference","previous_headings":"","what":"Calibration Results (Maxnet) — calib_results_maxnet","title":"Calibration Results (Maxnet) — calib_results_maxnet","text":"calibration_results object resulted calibration() using maxnet algorithm","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/calib_results_maxnet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calibration Results (Maxnet) — calib_results_maxnet","text":"","code":"data(\"calib_results_maxnet\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/calib_results_maxnet.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Calibration Results (Maxnet) — calib_results_maxnet","text":"calibration_results following elements: species Species names calibration_data data.frame variables extracted presence background points formula_grid data.frame ID, formulas, regularization multipliers candidate model part_data list partition data, element corresponds replicate contains indices test points replicate partition_method character indicating partition method n_replicates numeric value indicating number replicates k-folds train_proportion numeric value indicating proportion occurrences used train points partition method 'subsample' 'boostrap' data_xy data.frame coordinates occurrence bakground points continuous_variables character indicating names continuous variables categorical_variables character indicating names categorical variables weights numeric value specifying weights occurrence records. NULL, meaning set weights. pca prcomp object storing PCA information. NULL, meaning PCA performed algorithm character indicanting algorithm (maxnet) calibration_results list containing evaluation metrics candidate model omission_rate numeric value indicating omission rate used evaluate models (10%) addsamplestobackground logical value indicating whether add background presence sample already . selected_models data.frame formulas evaluation metrics selected model summary list number ID models removed selected selection procedure","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/calibration.html","id":null,"dir":"Reference","previous_headings":"","what":"Fitting and evaluation of models, and selection of the best ones — calibration","title":"Fitting and evaluation of models, and selection of the best ones — calibration","text":"function fits evaluates candidate models using data grid formulas prepared prepare_data(). supports algorithms glm maxnet. function selects best models based unimodality (optional), partial ROC, omission rate, AIC values.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/calibration.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fitting and evaluation of models, and selection of the best ones — calibration","text":"","code":"calibration(data, error_considered, remove_concave = FALSE,             proc_for_all = FALSE, omission_rate = NULL, delta_aic = 2,             allow_tolerance = TRUE, tolerance = 0.01,             addsamplestobackground = TRUE, use_weights = NULL,             write_summary = FALSE, output_directory = NULL,             skip_existing_models = FALSE, return_all_results = TRUE,             parallel = FALSE, ncores = NULL, progress_bar = TRUE,             verbose = TRUE)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/calibration.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fitting and evaluation of models, and selection of the best ones — calibration","text":"data object class prepared_data returned prepare_data() function. contains calibration data, formulas grid, kfolds, model type. error_considered (numeric) values 0 100 representing percentage potential error due source uncertainty data. value used calculate omission rates partial ROC. See details. remove_concave (logical) whether remove candidate models presenting concave curves. Default FALSE. proc_for_all (logical) whether apply partial ROC tests candidate models selected models. Default FALSE, meaning tests applied selected models. omission_rate (numeric) values 0 - 100, maximum omission rate candidate model can considered potentially selected model. default, NULL, uses value error_considered. one value used error_considered, omission_rate must defined. delta_aic (numeric) value delta AIC used threshold select models. Default 2. allow_tolerance (logical) whether allow selection models minimum values omission rates even omission rate surpasses omission_rate. applicable candidate models omission rates higher omission_rate. Default TRUE. tolerance (numeric) value added minimum omission rate exceeds omission_rate. allow_tolerance = TRUE, selected models omission rate equal less minimum rate plus tolerance. Default 0.01. addsamplestobackground (logical) whether add background presence sample already . Default TRUE. use_weights (logical) whether apply weights present data. default, NULL, uses weights provided data. present data, NULL weights 1 presences 100 background. turned FALSE, uses NULL weights even present data. write_summary (logical) whether save evaluation results candidate model disk. Default FALSE. output_directory (character) file name, without path, saving evaluation results candidate model. applicable write_summary = TRUE. skip_existing_models (logical) whether check skip candidate models already fitted saved output_directory. applicable write_summary = TRUE. Default FALSE. return_all_results (logical) whether return evaluation results replicate. Default TRUE, meaning evaluation results replicate returned. parallel (logical) whether fit candidate models parallel. Default FALSE. ncores (numeric) number cores use parallel processing. Default NULL uses available cores - 1. applicable parallel = TRUE. progress_bar (logical) whether display progress bar processing. Default TRUE. verbose (logical) whether display messages processing. Default TRUE.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/calibration.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fitting and evaluation of models, and selection of the best ones — calibration","text":"object class 'calibration_results' containing following elements: species: character string name species. calibration data: data.frame containing column (pr_bg) identifies occurrence points (1) background points (0), along corresponding values predictor variables point. formula_grid: data frame containing calibration grid possible formulas parameters. kfolds: list vectors row indices corresponding fold. data_xy: data.frame occurrence background coordinates. continuous_variables: character indicating continuous variables. categorical_variables: character, categorical variable names (used). weights: numeric vector specifying weights data_xy (used). pca: principal component analysis performed variables, list class \"prcomp\". See prcomp() details. algorithm: model type (glm maxnet) calibration_results: list containing data frame evaluation metrics partitions (return_all_results = TRUE) summary evaluation metrics candidate model. omission_rate: omission rate used select models. addsamplestobackground: logical value indicating whether presence sample already background added. selected_models: data frame ID summary evaluation metrics selected models. summary: list containing delta AIC values model selection, ID values models failed fit, concave curves, non-significant pROC values, omission rates threshold, delta AIC values threshold, selected models.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/calibration.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fitting and evaluation of models, and selection of the best ones — calibration","text":"Partial ROC calculated using values defined error_considered following Peterson et al. (2008). Omission rates calculated using separate testing data subsets. Users can specify multiple values error_considered calculate metric (e.g., c(5, 10)), one can used omission rate model selection. Model fitting complexity (AICc) assessed using models generated complete set occurrences. AICc values computed proposed Warren Seifert (2011).","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/calibration.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Fitting and evaluation of models, and selection of the best ones — calibration","text":"Ninomiya, Yoshiyuki, Shuichi Kawano. \"AIC Lasso generalized linear models.\" (2016): 2537-2560. Warren, D. L., & Seifert, S. N. (2011). Ecological niche modeling Maxent: importance model complexity performance model selection criteria. Ecological applications, 21(2), 335-342.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/calibration.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fitting and evaluation of models, and selection of the best ones — calibration","text":"","code":"# Import occurrences data(occ_data, package = \"kuenm2\")  # Import raster layers var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\")) # Use only continuous variables var <- var[[c(\"bio_1\", \"bio_7\", \"bio_12\", \"bio_15\")]]  # Prepare data for maxnet model sp_swd <- prepare_data(algorithm = \"maxnet\", occ = occ_data,                        x = \"x\", y = \"y\",                        raster_variables = var,                        species = occ_data[1, 1],                        n_background = 100,                        features = c(\"l\", \"lq\"),                        r_multiplier = 1,                        partition_method = \"kfolds\") # Model calibration (maxnet) m <- calibration(data = sp_swd, error_considered = 10) #> Task 1/1: fitting and evaluating models: #>    |                                                                               |                                                                      |   0%   |                                                                               |===                                                                   |   5%   |                                                                               |======                                                                |   9%   |                                                                               |==========                                                            |  14%   |                                                                               |=============                                                         |  18%   |                                                                               |================                                                      |  23%   |                                                                               |===================                                                   |  27%   |                                                                               |======================                                                |  32%   |                                                                               |=========================                                             |  36%   |                                                                               |=============================                                         |  41%   |                                                                               |================================                                      |  45%   |                                                                               |===================================                                   |  50%   |                                                                               |======================================                                |  55%   |                                                                               |=========================================                             |  59%   |                                                                               |=============================================                         |  64%   |                                                                               |================================================                      |  68%   |                                                                               |===================================================                   |  73%   |                                                                               |======================================================                |  77%   |                                                                               |=========================================================             |  82%   |                                                                               |============================================================          |  86%   |                                                                               |================================================================      |  91%   |                                                                               |===================================================================   |  95%   |                                                                               |======================================================================| 100% #>  #>  #> Model selection step: #> Selecting best among 22 models. #> Calculating pROC... #>  #> Filtering 22 models. #> Removing 0 model(s) because they failed to fit. #> 7 model(s) were selected with omission rate below 10%. #> Selecting 1 final model(s) with delta AIC <2. #> Validating pROC of selected models... #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #>  #> All selected models have significant pROC values. m #> calibration_results object summary (maxnet) #> ============================================================= #> Species: Myrcia hatschbachii  #> Number of candidate models: 22  #>   - Models removed because they failed to fit: 0  #>   - Models identified with concave curves: 0  #>   - Model with concave curves not removed  #>   - Models removed with non-significant values of pROC: 0  #>   - Models removed with omission error > 10%: 15  #>   - Models removed with delta AIC > 2: 6  #> Selected models: 1  #>   - Up to 5 printed here: #>   ID                   Formulas Features R_multiplier pval_pROC_at_10.mean #> 8  8 ~bio_1 + bio_7 + bio_15 -1        l            1                    0 #>   Omission_rate_at_10.mean dAIC Parameters #> 8                   0.0978    0          3  # Prepare data for glm model sp_swd_glm <- prepare_data(algorithm = \"glm\", occ = occ_data,                            x = \"x\", y = \"y\",                            raster_variables = var,                            species = occ_data[1, 1],                            n_background = 100,                            features = c(\"l\", \"lq\"),                            partition_method = \"kfolds\") m_glm <- calibration(data = sp_swd_glm, error_considered = 10) #> Task 1/1: fitting and evaluating models: #>    |                                                                               |                                                                      |   0%   |                                                                               |===                                                                   |   5%   |                                                                               |======                                                                |   9%   |                                                                               |==========                                                            |  14%   |                                                                               |=============                                                         |  18%   |                                                                               |================                                                      |  23%   |                                                                               |===================                                                   |  27%   |                                                                               |======================                                                |  32%   |                                                                               |=========================                                             |  36%   |                                                                               |=============================                                         |  41%   |                                                                               |================================                                      |  45%   |                                                                               |===================================                                   |  50%   |                                                                               |======================================                                |  55%   |                                                                               |=========================================                             |  59%   |                                                                               |=============================================                         |  64%   |                                                                               |================================================                      |  68%   |                                                                               |===================================================                   |  73%   |                                                                               |======================================================                |  77%   |                                                                               |=========================================================             |  82%   |                                                                               |============================================================          |  86%   |                                                                               |================================================================      |  91%   |                                                                               |===================================================================   |  95%   |                                                                               |======================================================================| 100% #>  #>  #> Model selection step: #> Selecting best among 22 models. #> Calculating pROC... #>  #> Filtering 22 models. #> Removing 0 model(s) because they failed to fit. #> 13 model(s) were selected with omission rate below 10%. #> Selecting 2 final model(s) with delta AIC <2. #> Validating pROC of selected models... #>    |                                                                               |                                                                      |   0%   |                                                                               |===================================                                   |  50%   |                                                                               |======================================================================| 100% #>  #> All selected models have significant pROC values. m_glm #> calibration_results object summary (glm) #> ============================================================= #> Species: Myrcia hatschbachii  #> Number of candidate models: 22  #>   - Models removed because they failed to fit: 0  #>   - Models identified with concave curves: 1  #>   - Model with concave curves not removed  #>   - Models removed with non-significant values of pROC: 0  #>   - Models removed with omission error > 10%: 9  #>   - Models removed with delta AIC > 2: 11  #> Selected models: 2  #>   - Up to 5 printed here: #>    ID                                                        Formulas Features #> 12 12                        ~bio_1 + bio_7 + I(bio_1^2) + I(bio_7^2)       lq #> 18 18 ~bio_1 + bio_7 + bio_12 + I(bio_1^2) + I(bio_7^2) + I(bio_12^2)       lq #>    pval_pROC_at_10.mean Omission_rate_at_10.mean       dAIC Parameters #> 12                    0                   0.0962 0.02361168          4 #> 18                    0                   0.0769 0.00000000          6"},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_current.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing present-day Conditions (CHELSA) — chelsa_current","title":"SpatRaster Representing present-day Conditions (CHELSA) — chelsa_current","text":"Raster layer containing bioclimatic variables representing present-day climatic conditions. variables resampled 10 arc-minute resolution masked using m region provided package. Data sourced CHELSA: https://chelsa-climate.org/","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_current.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing present-day Conditions (CHELSA) — chelsa_current","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_current.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing present-day Conditions (CHELSA) — chelsa_current","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_current.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing present-day Conditions (CHELSA) — chelsa_current","text":"","code":"chelsa_current <- terra::rast(system.file(\"extdata\",                                            \"Current_CHELSA.tif\",                                            package = \"kuenm2\")) terra::plot(chelsa_current)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_ccsm4.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing LGM Conditions (GCM: CCSM4) — chelsa_lgm_ccsm4","title":"SpatRaster Representing LGM Conditions (GCM: CCSM4) — chelsa_lgm_ccsm4","text":"Raster layer containing bioclimatic variables representing Last Glacial Maximum (LGM) climatic conditions based CCSM4 General Circulation Model (GCM). variables resampled 10arc-minutes masked using m provided package. Data sourced CHELSA: https://chelsa-climate.org/last-glacial-maximum-climate/","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_ccsm4.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing LGM Conditions (GCM: CCSM4) — chelsa_lgm_ccsm4","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_ccsm4.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing LGM Conditions (GCM: CCSM4) — chelsa_lgm_ccsm4","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_ccsm4.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing LGM Conditions (GCM: CCSM4) — chelsa_lgm_ccsm4","text":"","code":"chelsa_lgm_ccsm4 <- terra::rast(system.file(\"extdata\",                                            \"CHELSA_LGM_CCSM4.tif\",                                             package = \"kuenm2\"))  terra::plot(chelsa_lgm_ccsm4)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_cnrm_cm5.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing LGM Conditions (GCM: CNRM-CM5) — chelsa_lgm_cnrm_cm5","title":"SpatRaster Representing LGM Conditions (GCM: CNRM-CM5) — chelsa_lgm_cnrm_cm5","text":"Raster layer containing bioclimatic variables representing Last Glacial Maximum (LGM) climatic conditions based CNRM-CM5 General Circulation Model. variables resampled 10arc-minutes masked using m provided package. Data sourced CHELSA: https://chelsa-climate.org/last-glacial-maximum-climate/","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_cnrm_cm5.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing LGM Conditions (GCM: CNRM-CM5) — chelsa_lgm_cnrm_cm5","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_cnrm_cm5.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing LGM Conditions (GCM: CNRM-CM5) — chelsa_lgm_cnrm_cm5","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_cnrm_cm5.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing LGM Conditions (GCM: CNRM-CM5) — chelsa_lgm_cnrm_cm5","text":"","code":"chelsa_lgm_cnrm_cm5 <- terra::rast(system.file(\"extdata\",                                                \"CHELSA_LGM_CNRM-CM5.tif\",                                                package = \"kuenm2\")) terra::plot(chelsa_lgm_cnrm_cm5)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_fgoals_g2.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing LGM Conditions (GCM: FGOALS-g2) — chelsa_lgm_fgoals_g2","title":"SpatRaster Representing LGM Conditions (GCM: FGOALS-g2) — chelsa_lgm_fgoals_g2","text":"Raster layer containing bioclimatic variables representing Last Glacial Maximum (LGM) climatic conditions based FGOALS-g2 General Circulation Model. variables resampled 10arc-minutes masked using m provided package. Data sourced CHELSA: https://chelsa-climate.org/last-glacial-maximum-climate/","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_fgoals_g2.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing LGM Conditions (GCM: FGOALS-g2) — chelsa_lgm_fgoals_g2","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_fgoals_g2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing LGM Conditions (GCM: FGOALS-g2) — chelsa_lgm_fgoals_g2","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_fgoals_g2.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing LGM Conditions (GCM: FGOALS-g2) — chelsa_lgm_fgoals_g2","text":"","code":"chelsa_lgm_fgoals_g2 <- terra::rast(system.file(\"extdata\",                                                \"CHELSA_LGM_FGOALS-g2.tif\",                                                package = \"kuenm2\")) terra::plot(chelsa_lgm_fgoals_g2)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_ipsl.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing LGM Conditions (GCM: IPSL-CM5A-LR) — chelsa_lgm_ipsl","title":"SpatRaster Representing LGM Conditions (GCM: IPSL-CM5A-LR) — chelsa_lgm_ipsl","text":"Raster layer containing bioclimatic variables representing Last Glacial Maximum (LGM) climatic conditions based IPSL-CM5A-LR General Circulation Model. variables resampled 10arc-minutes masked using m provided package. Data sourced CHELSA: https://chelsa-climate.org/last-glacial-maximum-climate/","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_ipsl.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing LGM Conditions (GCM: IPSL-CM5A-LR) — chelsa_lgm_ipsl","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_ipsl.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing LGM Conditions (GCM: IPSL-CM5A-LR) — chelsa_lgm_ipsl","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_ipsl.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing LGM Conditions (GCM: IPSL-CM5A-LR) — chelsa_lgm_ipsl","text":"","code":"chelsa_lgm_ipsl <- terra::rast(system.file(\"extdata\",                                            \"CHELSA_LGM_IPSL-CM5A-LR.tif\",                                            package = \"kuenm2\")) terra::plot(chelsa_lgm_ipsl)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_miroc.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing LGM Conditions (GCM: MIROC-ESM) — chelsa_lgm_miroc","title":"SpatRaster Representing LGM Conditions (GCM: MIROC-ESM) — chelsa_lgm_miroc","text":"Raster layer containing bioclimatic variables representing Last Glacial Maximum (LGM) climatic conditions based MIROC-ESM General Circulation Model. variables resampled 10arc-minutes masked using m provided package. Data sourced CHELSA: https://chelsa-climate.org/last-glacial-maximum-climate/","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_miroc.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing LGM Conditions (GCM: MIROC-ESM) — chelsa_lgm_miroc","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_miroc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing LGM Conditions (GCM: MIROC-ESM) — chelsa_lgm_miroc","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_miroc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing LGM Conditions (GCM: MIROC-ESM) — chelsa_lgm_miroc","text":"","code":"chelsa_lgm_miroc <- terra::rast(system.file(\"extdata\",                                            \"CHELSA_LGM_MIROC-ESM.tif\",                                            package = \"kuenm2\")) terra::plot(chelsa_lgm_miroc)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_mpi.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing LGM Conditions (GCM: MPI-ESM-P) — chelsa_lgm_mpi","title":"SpatRaster Representing LGM Conditions (GCM: MPI-ESM-P) — chelsa_lgm_mpi","text":"Raster layer containing bioclimatic variables representing Last Glacial Maximum (LGM) climatic conditions based MPI-ESM-P General Circulation Model. variables resampled 10arc-minutes masked using m provided package. Data sourced CHELSA: https://chelsa-climate.org/last-glacial-maximum-climate/","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_mpi.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing LGM Conditions (GCM: MPI-ESM-P) — chelsa_lgm_mpi","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_mpi.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing LGM Conditions (GCM: MPI-ESM-P) — chelsa_lgm_mpi","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_mpi.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing LGM Conditions (GCM: MPI-ESM-P) — chelsa_lgm_mpi","text":"","code":"chelsa_lgm_mpi <- terra::rast(system.file(\"extdata\",                                            \"CHELSA_LGM_MPI-ESM-P.tif\",                                            package = \"kuenm2\")) terra::plot(chelsa_lgm_mpi)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_mri.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing LGM Conditions (GCM: MRI-CGCM3) — chelsa_lgm_mri","title":"SpatRaster Representing LGM Conditions (GCM: MRI-CGCM3) — chelsa_lgm_mri","text":"Raster layer containing bioclimatic variables representing Last Glacial Maximum (LGM) climatic conditions based MRI-CGCM3 General Circulation Model. variables resampled 10arc-minutes masked using m provided package. Data sourced CHELSA: https://chelsa-climate.org/last-glacial-maximum-climate/","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_mri.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing LGM Conditions (GCM: MRI-CGCM3) — chelsa_lgm_mri","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_mri.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing LGM Conditions (GCM: MRI-CGCM3) — chelsa_lgm_mri","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/chelsa_lgm_mri.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing LGM Conditions (GCM: MRI-CGCM3) — chelsa_lgm_mri","text":"","code":"chelsa_lgm_mri <- terra::rast(system.file(\"extdata\",                                            \"CHELSA_LGM_MRI-CGCM3.tif\",                                            package = \"kuenm2\")) terra::plot(chelsa_lgm_mri)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/colors_for_changes.html","id":null,"dir":"Reference","previous_headings":"","what":"Set Colors for Change Maps — colors_for_changes","title":"Set Colors for Change Maps — colors_for_changes","text":"functions sets color tables associated SpatRaster object resulting projection_changes(). Color tables used associate specific colors raster values using plot(). function defines custom colors areas gain, loss, stability across scenarios.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/colors_for_changes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set Colors for Change Maps — colors_for_changes","text":"","code":"colors_for_changes(   changes_projections,   gain_color = \"#009E73\",   loss_color = \"#D55E00\",   stable_suitable = \"#0072B2\",   stable_unsuitable = \"grey\",   max_alpha = 1,   min_alpha = 0.25 )"},{"path":"https://marlonecobos.github.io/kuenm2/reference/colors_for_changes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set Colors for Change Maps — colors_for_changes","text":"changes_projections object class changes_projections, generated projection_changes() imported using import_projections(), containing $Summary_changes element. gain_color (character) color used define pallete representing gains. Default \"#009E73\" (teal green). loss_color (character) color used define pallete representing losses. Default \"#D55E00\" (orange-red). stable_suitable (character) color used representing areas remain suitable across scenarios. Default \"#0072B2\" (oxford blue). stable_unsuitable (character) color used representing areas remain unsuitable across scenarios. Default \"grey\". max_alpha (numeric) opacity value (0 1) areas GCMs agree change (gain, loss, stability). Default 1. min_alpha (numeric) opacity value (0 1) areas one GCM predicts given change.  Default 0.25","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/colors_for_changes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set Colors for Change Maps — colors_for_changes","text":"object class changes_projections structure SpatRasters input changes_projections, color tables embedded SpatRasters. colors used automatically visualizing data plot().","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/colors_for_changes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set Colors for Change Maps — colors_for_changes","text":"","code":"# Step 1: Organize variables for current projection ## Import current variables (used to fit models) var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  ## Create a folder in a temporary directory to copy the variables out_dir_current <- file.path(tempdir(), \"Current_raw_color_example\") dir.create(out_dir_current, recursive = TRUE)  ## Save current variables in temporary directory terra::writeRaster(var, file.path(out_dir_current, \"Variables.tif\"))   # Step 2: Organize future climate variables (example with WorldClim) ## Directory containing the downloaded future climate variables (example) in_dir <- system.file(\"extdata\", package = \"kuenm2\")  ## Create a folder in a temporary directory to copy the future variables out_dir_future <- file.path(tempdir(), \"Future_raw_color_example\")  ## Organize and rename the future climate data (structured by year and GCM) ### 'SoilType' will be appended as a static variable in each scenario organize_future_worldclim(input_dir = in_dir, output_dir = out_dir_future,                           name_format = \"bio_\",                           fixed_variables = var$SoilType) #>    |                                                                               |                                                                      |   0%   |                                                                               |=========                                                             |  12%   |                                                                               |==================                                                    |  25%   |                                                                               |==========================                                            |  38%   |                                                                               |===================================                                   |  50%   |                                                                               |============================================                          |  62%   |                                                                               |====================================================                  |  75%   |                                                                               |=============================================================         |  88%   |                                                                               |======================================================================| 100% #>  #> Variables successfully organized in directory: #> /tmp/Rtmp3eUeuI/Future_raw_color_example  # Step 3: Prepare data to run multiple projections ## An example with maxnet models ## Import example of fitted_models (output of fit_selected()) data(fitted_model_maxnet, package = \"kuenm2\")  ## Prepare projection data using fitted models to check variables pr <- prepare_projection(models = fitted_model_maxnet,                          present_dir = out_dir_current,                          future_dir = out_dir_future,                          future_period = c(\"2081-2100\"),                          future_pscen = c(\"ssp126\", \"ssp585\"),                          future_gcm = c(\"ACCESS-CM2\", \"MIROC6\"),                          raster_pattern = \".tif*\")  # Step 4: Run multiple model projections ## A folder to save projection results out_dir <- file.path(tempdir(), \"Projection_results/maxnet_color_example\") dir.create(out_dir, recursive = TRUE)  ## Project selected models to multiple scenarios p <- project_selected(models = fitted_model_maxnet, projection_data = pr,                       out_dir = out_dir) #>    |                                                                               |                                                                      |   0%   |                                                                               |==============                                                        |  20%   |                                                                               |============================                                          |  40%   |                                                                               |==========================================                            |  60%   |                                                                               |========================================================              |  80%   |                                                                               |======================================================================| 100%  # Step 5: Identify areas of change in projections ## Contraction, expansion and stability changes <- projection_changes(model_projections = p, by_gcm = TRUE,                               by_change = TRUE, write_results = FALSE,                               return_raster = TRUE)  #Step 6: Set Colors for Change Maps changes_with_colors <- colors_for_changes(changes_projections = changes) terra::plot(changes_with_colors$Summary_changes)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/detect_concave.html","id":null,"dir":"Reference","previous_headings":"","what":"Detect concave curves in GLM and GLMNET models — detect_concave","title":"Detect concave curves in GLM and GLMNET models — detect_concave","text":"Identifies presence concave response curves within calibration range GLM GLMNET models.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/detect_concave.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Detect concave curves in GLM and GLMNET models — detect_concave","text":"","code":"detect_concave(model, calib_data, extrapolation_factor = 0.1,                averages_from = \"pr\", var_limits = NULL, plot = FALSE,                mfrow = NULL, legend = FALSE)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/detect_concave.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Detect concave curves in GLM and GLMNET models — detect_concave","text":"model object class glmnet_mx glm. calib_data data.frame matrix data used model calibration. extrapolation_factor (numeric) multiplier used calculate extrapolation range. Larger values allow broader extrapolation beyond observed data range. Default 0.1. averages_from (character) specifies averages modes variables calculated. Available options \"pr\" (calculate averages presence localities) \"pr_bg\" (use combined set presence background localities). Default \"pr\". See details. var_limits (list) named list specifying lower /upper limits variables. first value represents lower limit, second value represents upper limit. Default NULL, meaning specific limits applied, range calculated using extrapolation_factor. See details. plot (logical) whether plot response curve variables. Default FALSE. mfrow (numeric) vector form c(number rows, number columns) specifying layout plots. Default c(1, 1), meaning one plot per window. legend (logical) whether include legend plot. legend indicates whether response curve convex, concave outside range limits, concave within range limits. Default FALSE.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/detect_concave.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Detect concave curves in GLM and GLMNET models — detect_concave","text":"list following elements variable: is_concave (logical): indicates whether response curve variable concave within limit range. occurs quadratic term's coefficient positive vertex lies x_min x_max, vertex (numeric): vertex parabola, representing point curve changes direction. b2 (numeric): coefficient quadratic term variable. Positive values indicate concave curve. x_min x_max (numeric): range limits identify concave curves, calculated observed data range multiplied extrapolation factor. real_x_min real_x_max (numeric) actual range data, excluding extrapolation factor.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/detect_concave.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Detect concave curves in GLM and GLMNET models — detect_concave","text":"Concave curves identified analyzing beta coefficients quadratic terms within variable's range. range extrapolation calculated difference variable's maximum minimum values model, multiplied extrapolation factor. concave curve detected beta coefficient positive, vertex (curve changes direction) lies lower upper limits variable. Users can specify lower upper limits certain variables using var_limits. example, var_limits = list(\"bio12\" = c(0, NA), \"bio15\" = c(0, 100)), lower limit bio12 0, upper limit calculated using extrapolation factor. Similarly, lower upper limits bio15 0 100, respectively. calculating vertex position, response curve given variable generated variables set mean values (mode categorical variables). values calculated either presence localities (averages_from = \"pr\") combined set presence background localities (averages_from = \"pr_bg\").","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/detect_concave.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Detect concave curves in GLM and GLMNET models — detect_concave","text":"","code":"# Import example of a fitted_model (output of fit_selected()) that have # concave curves data(\"fitted_model_concave\", package = \"kuenm2\")  #Response curves ccurves <- detect_concave(model = fitted_model_concave$Models$Model_798$Full_model,                           calib_data = fitted_model_concave$calibration_data,                           extrapolation_factor = 0.2,                           var_limits = list(\"bio_2\" = c(0, NA),                                             \"sand\" = c(0, NA),                                             \"clay\" = c(0, NA)),                           plot = TRUE, mfrow = c(2, 3), legend = TRUE)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/enmeval_block.html","id":null,"dir":"Reference","previous_headings":"","what":"Spatial Blocks from ENMeval — enmeval_block","title":"Spatial Blocks from ENMeval — enmeval_block","text":"list resulting ENMeval::get.block() partition occurrence background localities bins training validation (, evaluation calibration). object used \"Prepare Data Model Calibration\" vignette demonstrate implement custom data partitions generated ENMeval kuenm2.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/enmeval_block.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Spatial Blocks from ENMeval — enmeval_block","text":"","code":"data(\"enmeval_block\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/enmeval_block.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Spatial Blocks from ENMeval — enmeval_block","text":"list following elements: occs.grp numeric vector indicating spatial group occurrence belongs bg.grp numeric vector indicating spatial group background point belongs","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_calibration_hist.html","id":null,"dir":"Reference","previous_headings":"","what":"Explore variable distribution for occurrence and background points — explore_calibration_hist","title":"Explore variable distribution for occurrence and background points — explore_calibration_hist","text":"function prepares data generate overlaid histograms visualize distribution predictor variables occurrence (presence) background points.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_calibration_hist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Explore variable distribution for occurrence and background points — explore_calibration_hist","text":"","code":"explore_calibration_hist(data, include_m = FALSE, raster_variables = NULL,                          magnify_occurrences = 2, breaks = 15)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_calibration_hist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Explore variable distribution for occurrence and background points — explore_calibration_hist","text":"data object class prepared_data returned prepare_data() function. include_m (logical) whether include data plotting histogram entire area background points sampled. Default FALSE, meaning background presence information plotted. raster_variables (SpatRaster) predictor variables used prepared data prepared_data. applicable include_m TRUE. magnify_occurrences (numeric) factor frequency occurrences magnified better visualization. Default 2, meaning occurrence frequencies plot doubled. breaks (numeric) single number giving desired number intervals histogram.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_calibration_hist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Explore variable distribution for occurrence and background points — explore_calibration_hist","text":"list information plot informative histograms explore data used modeling process. Histogram plots can plotted function plot_calibration_hist().","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_calibration_hist.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Explore variable distribution for occurrence and background points — explore_calibration_hist","text":"","code":"# Import raster layers var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  # Import occurrences data(sp_swd, package = \"kuenm2\")  # Explore calibration data calib_hist <- explore_calibration_hist(data = sp_swd,                                        raster_variables = var,                                        include_m = TRUE)  # To visualize results use the function plot_calibration_hist()"},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_partition_env.html","id":null,"dir":"Reference","previous_headings":"","what":"Explore the Distribution of Partitions in Environmental Space — explore_partition_env","title":"Explore the Distribution of Partitions in Environmental Space — explore_partition_env","text":"Plots training testing data (presences backgrounds) two-dimensional environmental space. space can defined either performing PCA environmental variables specifying two environmental variables manually.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_partition_env.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Explore the Distribution of Partitions in Environmental Space — explore_partition_env","text":"","code":"explore_partition_env(data, show_unused_data = FALSE,                              raster_variables = NULL, mask = NULL,                              variables = NULL, type_of_plot = \"combined\",                              use_pca = TRUE, pcs = c(\"PC1\", \"PC2\"),                              partition_palette = \"cols25\",                              custom_partition_palette = NULL,                              include_test_background = TRUE,                              pr_train_col = \"#009E73\",                              pr_test_col = \"#D55E00\",                              bg_train_col = \"grey\",                              bg_test_col = \"#56B4E9\", pr_transparency = 0.75,                              bg_transparency = 0.4, pch = 19, cex_plot = 1.2,                              size_text_legend = 1, ...)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_partition_env.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Explore the Distribution of Partitions in Environmental Space — explore_partition_env","text":"data object class prepared_data returned prepare_data() function show_unused_data (logical) whether plot distribution environmental conditions represented background points. set TRUE, raster_variables must provided. applicable type_of_plot = \"combined\". raster_variables SpatRaster object representing predictor variables used calibrate models. Preferably object used prepare_data. Required show_unused_data = TRUE. Default NULL. mask (SpatRaster, SpatVector, SpatExtent) spatial object used mask raster_variables area model calibrated. Preferably object used prepare_data (applicable). used show_unused_data = TRUE. Default NULL. variables (character) names variables data define two-dimensional environmental space. use_pca = TRUE, variables used perform PCA. use_pca = FALSE, must character vector exactly two variable names (e.g., c(\"bio_1\", \"bio_12\")). Default NULL, meaning variables data used. type_of_plot (character) type plot. Options \"combined\" \"individual\". See details. Default \"combined\". use_pca (logical) whether use PCA variables define environmental space. TRUE, PCA performed variables, unless data already includes PCA object using prepare_data(do_pca = TRUE). Default TRUE. pcs (character) two PCA axes use define two-dimensional environmental space. Default c(\"PC1\", \"PC2\"), meaning first two axes used. applicable use_pca = TRUE. partition_palette (character) color palette used color different partitions. See ?kuenm2_discrete_palettes check available options. Default \"cols25\". custom_partition_palette (character) character vector defining custom colors different partitions. number values must match number partitions data. Default NULL, meaning palette defined partition_palette used. include_test_background (logical) whether include background points used training plotting individual partition plots. Default TRUE. pr_train_col (character) color used train records individual plots. Default \"009E73\". pr_test_col (character) color used test records individual plots. Default \"D55E00\". bg_train_col (character) color used train backgrounds individual plots. Default \"56B4E9\". bg_test_col (character) color used test backgrounds individual plots. Default \"gray\". applicable include_test_background = TRUE. pr_transparency (numeric) value 0 (fully transparent) 1 (fully opaque) defining transparency points representing presences. Default 0.75. bg_transparency (numeric) value 0 (fully transparent) 1 (fully opaque) defining transparency points representing background points. Default 0.4. pch (numeric) value 1 25 specify point shape. See ?pch details. Default 19 (solid circle). cex_plot (numeric) specify size points plot. Default 1.2. size_text_legend (numeric) specify size text legend. Default 1. ... additional arguments passed plot.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_partition_env.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Explore the Distribution of Partitions in Environmental Space — explore_partition_env","text":"Plots showing training testing data two-dimensional environmental space.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_partition_env.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Explore the Distribution of Partitions in Environmental Space — explore_partition_env","text":"function provides two types plots: combined: two plots side side, one showing presences another showing background points. colors points represent partitions. default option. individual: one plot per partition. plot, colors points represent used train records, test records, train background, test background (.e., used training specified partition). obtain types plots, set: type_of_plot = c(\"combined\", \"individual\").","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_partition_env.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Explore the Distribution of Partitions in Environmental Space — explore_partition_env","text":"","code":"# Prepare data # Import occurrences data(occ_data, package = \"kuenm2\")  # Import raster layers var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  # Prepare data for maxnet model sp_swd <- prepare_data(algorithm = \"maxnet\", occ = occ_data,                        x = \"x\", y = \"y\",                        raster_variables = var,                        species = occ_data[1, 1],                        n_background = 100,                        categorical_variables = \"SoilType\",                        features = c(\"l\", \"lq\"),                        r_multiplier = 1,                        partition_method = \"kfolds\") #> Warning: 3 rows were excluded from database because NAs were found.  # Explore the Distribution of Partitions in Environmental Space explore_partition_env(data = sp_swd, show_unused_data = TRUE,                       raster_variables = var,                       type_of_plot = c(\"combined\", \"individual\"))"},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_partition_extrapolation.html","id":null,"dir":"Reference","previous_headings":"","what":"Analysis of extrapolation risks in partitions using the MOP metric — explore_partition_extrapolation","title":"Analysis of extrapolation risks in partitions using the MOP metric — explore_partition_extrapolation","text":"function calculates environmental dissimilarities identifies non-analogous conditions comparing training data test data partition, using MOP (Mobility-Oriented Parity) metric.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_partition_extrapolation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Analysis of extrapolation risks in partitions using the MOP metric — explore_partition_extrapolation","text":"","code":"explore_partition_extrapolation(data, include_train_background = TRUE,                                        include_test_background = FALSE,                                        variables = NULL,                                        mop_type = \"detailed\",                                        calculate_distance = TRUE,                                        where_distance = \"all\",                                        progress_bar = FALSE, ...)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_partition_extrapolation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Analysis of extrapolation risks in partitions using the MOP metric — explore_partition_extrapolation","text":"data object class prepared_data returned prepare_data() function. include_train_background (logical) whether include background points used training define environmental range training data. set FALSE, environmental conditions training presence records considered. Default TRUE, meaning presence background points used. include_test_background (logical) whether compute MOP test presence records background points used training. Default FALSE, meaning MOP calculated test presences. variables (character) names variables used MOP calculation. Default NULL, meaning variables data used. mop_type (character) type MOP analysis performed. Options available \"basic\", \"simple\" \"detailed\". Default 'simples'. See projection_mop() details. calculate_distance (logical) whether calculate distances (dissimilarities) train test data. Default TRUE. where_distance (character) specifies values train data used calculate distances. Options : \"in_range\" (conditions within train range), \"out_range\" (conditions outside train range), \"\" (conditions). Default \"\". progress_bar (logical) whether display progress bar processing. Default FALSE. ... additional arguments passed mop().","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_partition_extrapolation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Analysis of extrapolation risks in partitions using the MOP metric — explore_partition_extrapolation","text":"data.frame containing: MOP distances (calculate_distance = TRUE); indicator whether environmental conditions test record fall within training range; number variables outside training range; names variables values lower higher training range; prepared_data object includes categorical variables, also contain columns indicating values testing data present training data.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_partition_extrapolation.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Analysis of extrapolation risks in partitions using the MOP metric — explore_partition_extrapolation","text":"","code":"#Prepare data # Import occurrences data(occ_data, package = \"kuenm2\")  # Import raster layers var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  # Prepare data for maxnet model sp_swd <- prepare_data(algorithm = \"maxnet\", occ = occ_data,                        x = \"x\", y = \"y\",                        raster_variables = var,                        species = occ_data[1, 1],                        n_background = 100,                        categorical_variables = \"SoilType\",                        features = c(\"l\", \"lq\"),                        r_multiplier = 1,                        partition_method = \"kfolds\") #> Warning: 3 rows were excluded from database because NAs were found.  # Analysis of extrapolation risks in partitions res <- explore_partition_extrapolation(data = sp_swd)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_partition_geo.html","id":null,"dir":"Reference","previous_headings":"","what":"Explore the spatial distribution of partitions for occurrence and background points — explore_partition_geo","title":"Explore the spatial distribution of partitions for occurrence and background points — explore_partition_geo","text":"Explore spatial distribution partitions occurrence background points","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_partition_geo.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Explore the spatial distribution of partitions for occurrence and background points — explore_partition_geo","text":"","code":"explore_partition_geo(data, raster_variables, mask = NULL,                       show_partitions = TRUE, partition_palette = \"cols25\",                       custom_partition_palette = NULL, pr_col = \"#D55E00\",                       bg_col = \"#0072B2\", pr_bg_col = \"#CC79A7\",                       calibration_area_col = \"gray80\", ...)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_partition_geo.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Explore the spatial distribution of partitions for occurrence and background points — explore_partition_geo","text":"data object class prepared_data returned prepare_data() function. raster_variables (SpatRaster) predictor variables used model calibration. mask (SpatRaster, SpatVector, SpatExtent) spatial object used mask raster_variables area model calibrated. Preferably object used prepare_data (applicable). Default NULL. show_partitions (logical) whether return SpatRaster showing spatial distribution partition presence background points. Default TRUE. partition_palette (character) color palette used color different partitions. See ?kuenm2_discrete_palettes check available options. Default \"cols25\". applicable show_partitions = TRUE. custom_partition_palette (character) character vector defining custom colors different partitions. number values must match number partitions data. Default NULL, meaning palette defined partition_palette used. pr_col (character) color used cells presence records. Default \"#D55E00\". bg_col (character) color used cells background points. Default \"#0072B2\". pr_bg_col (character) color used cells presences background points. Default \"#CC79A7\". calibration_area_col (character) color used cells without presences background points. Default \"gray80\". ... additional arguments passed terra::plot().","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_partition_geo.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Explore the spatial distribution of partitions for occurrence and background points — explore_partition_geo","text":"categorical SpatRaster four factor values representing: 1 - Background cells  2 - Presence cells  3 - Cells presence background  4 - Non-used cells  show_partitions = TRUE, also returns SpatRaster showing spatial distribution partition presence background points.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/explore_partition_geo.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Explore the spatial distribution of partitions for occurrence and background points — explore_partition_geo","text":"","code":"# Import raster layers var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  # Import prepared_data data(sp_swd, package = \"kuenm2\")  # Explore partitions in the geographic space pbg <- explore_partition_geo(data = sp_swd, raster_variables = var[[1]]) terra::plot(pbg)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/extract_occurrence_variables.html","id":null,"dir":"Reference","previous_headings":"","what":"Extracts Environmental Variables for Occurrences — extract_occurrence_variables","title":"Extracts Environmental Variables for Occurrences — extract_occurrence_variables","text":"function extracts values environmental predictor variables (SpatRaster) georeferenced occurrence points. also adds column indicating presence points(pr_bg = 1).","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/extract_occurrence_variables.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extracts Environmental Variables for Occurrences — extract_occurrence_variables","text":"","code":"extract_occurrence_variables(occ, x, y, raster_variables)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/extract_occurrence_variables.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extracts Environmental Variables for Occurrences — extract_occurrence_variables","text":"occ data.frame containing occurrence data. must include columns longitude (x) latitude (y) coordinates. x (character) string specifying name column occ contains longitude values. y (character) string specifying name column occ contains latitude values. raster_variables (SpatRaster) predictor variables used calibrate models.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/extract_occurrence_variables.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extracts Environmental Variables for Occurrences — extract_occurrence_variables","text":"data.frame containing original x y coordinates occurrence points (x y), values variables extracted raster_variables, new column pr_bg value 1 occurrences.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/extract_occurrence_variables.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extracts Environmental Variables for Occurrences — extract_occurrence_variables","text":"","code":"# Import occurrences data(occ_data, package = \"kuenm2\")  # Import raster layers var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  # Extracts environmental variables for occurrences occ_var <- extract_occurrence_variables(occ = occ_data, x = \"x\", y = \"y\",                                         raster_variables = var)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/extract_var_from_formulas.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract predictor names from formulas — extract_var_from_formulas","title":"Extract predictor names from formulas — extract_var_from_formulas","text":"Extract predictor names formulas","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/extract_var_from_formulas.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract predictor names from formulas — extract_var_from_formulas","text":"","code":"extract_var_from_formulas(formulas, ...)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/extract_var_from_formulas.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract predictor names from formulas — extract_var_from_formulas","text":"formulas (character formula) model formulas. ... Arguments pass .vars()","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/extract_var_from_formulas.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract predictor names from formulas — extract_var_from_formulas","text":"character vector list length formulas, containing names predictors formula.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/extract_var_from_formulas.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract predictor names from formulas — extract_var_from_formulas","text":"","code":"# Import an example of calibration results data(calib_results_maxnet, package = \"kuenm2\")  # Extract predictor names vars <- extract_var_from_formulas(calib_results_maxnet$formula_grid$Formulas)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/fit_selected.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit models selected after calibration — fit_selected","title":"Fit models selected after calibration — fit_selected","text":"function fits models selected model calibration().","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/fit_selected.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit models selected after calibration — fit_selected","text":"","code":"fit_selected(calibration_results, replicate_method = \"kfolds\",              n_replicates = 1, sample_proportion = 0.7, type = \"cloglog\",              write_models = FALSE,              file_name = NULL, parallel = FALSE, ncores = NULL,              progress_bar = TRUE, verbose = TRUE, seed = 1)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/fit_selected.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit models selected after calibration — fit_selected","text":"calibration_results object class calibration_results returned calibration() function. replicate_method (character) method used producing replicates. Available options \"kfolds\", \"subsample\", \"bootstrap\". See Details information. n_replicates (numeric) number replicates folds generate. replicate_method \"subsample\" \"bootstrap\", defines number replicates. \"kfolds\", specifies number folds. Default 4. sample_proportion (numeric) proportion occurrence background points used fit model replicates. applicable replicate_method \"subsample\" \"bootstrap\". Default 0.7 (.e., 70% data). type (character) format prediction values computing thresholds. maxnet models, valid options \"raw\", \"cumulative\", \"logistic\", \"cloglog\". glm models, valid options \"cloglog\", \"response\" \"raw\". Default \"cloglog\". write_models (logical) whether save final fitted models disk. Default FALSE. file_name (character) file name, without path, saving final models. applicable write_models = TRUE. parallel (logical) whether fit final models parallel. Default FALSE. ncores (numeric) number cores use parallel processing. Default NULL uses available cores - 1. applicable parallel = TRUE. progress_bar (logical) whether display progress bar processing. Default TRUE. verbose (logical) whether display detailed messages processing. Default TRUE. seed (numeric) integer value used specify initial seed split data. Default 1.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/fit_selected.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit models selected after calibration — fit_selected","text":"object class 'fitted_models' containing following elements: species character string name species. Models list fitted models, including replicates (fitted part data) full models (fitted data). calibration_data data.frame containing column (pr_bg) identifies occurrence points (1) background points (0), along corresponding values predictor variables point. selected_models data frame ID summary evaluation metrics selected models. weights numeric vector specifying weights predictor variables (used). pca list class prcomp representing result principal component analysis (performed). addsamplestobackground logical value indicating whether presence sample already background added. omission_rate omission rate determined calibration step. thresholds thresholds binarize replicate consensus (mean median), calculated based omission rate set calibration().","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/fit_selected.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fit models selected after calibration — fit_selected","text":"function also computes model consensus (mean median), thresholds binarize model predictions based omission rate set model calibration select models.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/fit_selected.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit models selected after calibration — fit_selected","text":"","code":"# An example with maxnet models data(calib_results_maxnet, package = \"kuenm2\")  # Fit models using calibration results fm <- fit_selected(calibration_results = calib_results_maxnet,                    n_replicates = 4) #> Fitting replicates... #>    |                                                                               |                                                                      |   0%   |                                                                               |=========                                                             |  12%   |                                                                               |==================                                                    |  25%   |                                                                               |==========================                                            |  38%   |                                                                               |===================================                                   |  50%   |                                                                               |============================================                          |  62%   |                                                                               |====================================================                  |  75%   |                                                                               |=============================================================         |  88%   |                                                                               |======================================================================| 100% #>  #> Fitting full models... #>    |                                                                               |                                                                      |   0%   |                                                                               |===================================                                   |  50%   |                                                                               |======================================================================| 100%  # Output the fitted models fm #> fitted_models object summary #> ============================ #> Species: Myrcia hatschbachii  #> Algortihm: maxnet  #> Number of fitted models: 2  #> Models fitted with 4 replicates  # An example with GLMs data(calib_results_glm, package = \"kuenm2\")  # Fit models using calibration results fm_glm <- fit_selected(calibration_results = calib_results_glm,                        replicate_method = \"subsample\",                        n_replicates = 5) #> Fitting replicates... #>    |                                                                               |                                                                      |   0%   |                                                                               |==============                                                        |  20%   |                                                                               |============================                                          |  40%   |                                                                               |==========================================                            |  60%   |                                                                               |========================================================              |  80%   |                                                                               |======================================================================| 100% #>  #> Fitting full models... #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100%  # Output the fitted models fm_glm #> fitted_models object summary #> ============================ #> Species: Myrcia hatschbachii  #> Algortihm: glm  #> Number of fitted models: 1  #> Models fitted with 5 replicates"},{"path":"https://marlonecobos.github.io/kuenm2/reference/fitted_model_chelsa.html","id":null,"dir":"Reference","previous_headings":"","what":"Fitted model with CHELSA variables — fitted_model_chelsa","title":"Fitted model with CHELSA variables — fitted_model_chelsa","text":"fitted_models object resulting fit_selected() using calibration data based CHELSA variables.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/fitted_model_chelsa.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fitted model with CHELSA variables — fitted_model_chelsa","text":"","code":"data(\"fitted_model_chelsa\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/fitted_model_chelsa.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Fitted model with CHELSA variables — fitted_model_chelsa","text":"fitted_models following elements: species Species names Models list fitted maxnet models (replicates full models) calibration_data data.frame containing variables extracted presence background points continuous_variables character indicating names continuous variables categorical_variables character indicating names categorical variables selected_models data.frame formulas evaluation metrics selected model weights numeric vector specifying weights occurrence records. NULL weights set. pca prcomp object containing PCA results. NULL PCA performed. addsamplestobackground logical value indicating whether add presence point already included background. omission_rate numeric value indicating omission rate used evaluate models. thresholds numeric vector thresholds used binarize replicate consensus (mean median), calculated based omission rate defined calibration(). algorithm character string indicating algorithm used (maxnet). partition_method character string indicating partitioning method used. n_replicates numeric value indicating number replicates folds. train_proportion numeric value indicating proportion occurrences used training partition method 'subsample' 'bootstrap'.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/fitted_model_concave.html","id":null,"dir":"Reference","previous_headings":"","what":"Fitted model with concave curves — fitted_model_concave","title":"Fitted model with concave curves — fitted_model_concave","text":"maxnet fitted_models object resulting fit_selected() model presenting concave curves.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/fitted_model_concave.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fitted model with concave curves — fitted_model_concave","text":"","code":"data(\"fitted_model_concave\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/fitted_model_concave.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Fitted model with concave curves — fitted_model_concave","text":"fitted_models following elements: species Species names Models list fitted maxnet models (replicates full models) calibration_data data.frame containing variables extracted presence background points continuous_variables character indicating names continuous variables categorical_variables character indicating names categorical variables selected_models data.frame formulas evaluation metrics selected model weights numeric vector specifying weights occurrence records. NULL weights set. pca prcomp object containing PCA results. NULL PCA performed. addsamplestobackground logical value indicating whether add presence point already included background. omission_rate numeric value indicating omission rate used evaluate models. thresholds numeric vector thresholds used binarize replicate consensus (mean median), calculated based omission rate defined calibration(). algorithm character string indicating algorithm used (maxnet). partition_method character string indicating partitioning method used. n_replicates numeric value indicating number replicates folds.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/fitted_model_glm.html","id":null,"dir":"Reference","previous_headings":"","what":"Fitted model with glm algorithm — fitted_model_glm","title":"Fitted model with glm algorithm — fitted_model_glm","text":"glm fitted_models object resulting fit_selected() using calibration data based WorldCLim variables.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/fitted_model_glm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fitted model with glm algorithm — fitted_model_glm","text":"","code":"data(\"fitted_model_glm\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/fitted_model_glm.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Fitted model with glm algorithm — fitted_model_glm","text":"fitted_models following elements: species Species names Models list fitted maxnet models (replicates full models) calibration_data data.frame containing variables extracted presence background points continuous_variables character indicating names continuous variables categorical_variables character indicating names categorical variables selected_models data.frame formulas evaluation metrics selected model weights numeric vector specifying weights occurrence records. NULL weights set. pca prcomp object containing PCA results. NULL PCA performed. addsamplestobackground logical value indicating whether add presence point already included background. omission_rate numeric value indicating omission rate used evaluate models. thresholds numeric vector thresholds used binarize replicate consensus (mean median), calculated based omission rate defined calibration(). algorithm character string indicating algorithm used (glm). partition_method character string indicating partitioning method used. n_replicates numeric value indicating number replicates folds. train_proportion numeric value indicating proportion occurrences used training partition method 'subsample' 'bootstrap'.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/fitted_model_maxnet.html","id":null,"dir":"Reference","previous_headings":"","what":"Fitted model with maxnet algorithm — fitted_model_maxnet","title":"Fitted model with maxnet algorithm — fitted_model_maxnet","text":"maxnet fitted_models object resulting fit_selected() using calibration data based WorldCLim variables.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/fitted_model_maxnet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fitted model with maxnet algorithm — fitted_model_maxnet","text":"","code":"data(\"fitted_model_maxnet\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/fitted_model_maxnet.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Fitted model with maxnet algorithm — fitted_model_maxnet","text":"fitted_models following elements: species Species names Models list fitted maxnet models (replicates full models) calibration_data data.frame containing variables extracted presence background points continuous_variables character indicating names continuous variables categorical_variables character indicating names categorical variables selected_models data.frame formulas evaluation metrics selected model weights numeric vector specifying weights occurrence records. NULL weights set. pca prcomp object containing PCA results. NULL PCA performed. addsamplestobackground logical value indicating whether add presence point already included background. omission_rate numeric value indicating omission rate used evaluate models. thresholds numeric vector thresholds used binarize replicate consensus (mean median), calculated based omission rate defined calibration(). algorithm character string indicating algorithm used (maxnet). partition_method character string indicating partitioning method used. n_replicates numeric value indicating number replicates folds. train_proportion numeric value indicating proportion occurrences used training partition method 'subsample' 'bootstrap'.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/flexsdm_block.html","id":null,"dir":"Reference","previous_headings":"","what":"Spatial Blocks from flexsdm — flexsdm_block","title":"Spatial Blocks from flexsdm — flexsdm_block","text":"list resulting flexsdm::part_sblock(), used partition occurrence background localities bins training evaluation. object used \"Prepare Data Model Calibration\" vignette demonstrate implement custom data partitions generated flexsdm kuenm2","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/flexsdm_block.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Spatial Blocks from flexsdm — flexsdm_block","text":"","code":"data(\"enmeval_block\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/flexsdm_block.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Spatial Blocks from flexsdm — flexsdm_block","text":"list following elements: part tibble object information used 'data' arguments additional column .part partition group. best_part_info tibble information best partition.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp126_access.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing Future Conditions (2041-2060, SSP126, GCM: ACCESS-CM2) — future_2050_ssp126_access","title":"SpatRaster Representing Future Conditions (2041-2060, SSP126, GCM: ACCESS-CM2) — future_2050_ssp126_access","text":"raster layer containing bioclimatic variables representing future climatic conditions (2041-2060) based ACCESS-CM2 General Circulation Model SSP126 scenario. variables obtained 10 arc-minute resolution masked using m region provided package. Data sourced WorldClim: https://worldclim.org/data/cmip6/cmip6climate.html","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp126_access.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing Future Conditions (2041-2060, SSP126, GCM: ACCESS-CM2) — future_2050_ssp126_access","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp126_access.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing Future Conditions (2041-2060, SSP126, GCM: ACCESS-CM2) — future_2050_ssp126_access","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp126_access.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing Future Conditions (2041-2060, SSP126, GCM: ACCESS-CM2) — future_2050_ssp126_access","text":"","code":"future_2050_ssp126_access <- terra::rast(system.file(\"extdata\",                                     \"wc2.1_10m_bioc_ACCESS-CM2_ssp126_2041-2060.tif\",                                      package = \"kuenm2\")) terra::plot(future_2050_ssp126_access)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp126_miroc.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing Future Conditions (2041-2060, SSP126, GCM: MIROC6) — future_2050_ssp126_miroc","title":"SpatRaster Representing Future Conditions (2041-2060, SSP126, GCM: MIROC6) — future_2050_ssp126_miroc","text":"raster layer containing bioclimatic variables representing future climatic conditions (2041-2060) based MIROC6 General Circulation Model SSP126 scenario. variables obtained 10 arc-minute resolution masked using m region provided package. Data sourced WorldClim: https://worldclim.org/data/cmip6/cmip6climate.html","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp126_miroc.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing Future Conditions (2041-2060, SSP126, GCM: MIROC6) — future_2050_ssp126_miroc","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp126_miroc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing Future Conditions (2041-2060, SSP126, GCM: MIROC6) — future_2050_ssp126_miroc","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp126_miroc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing Future Conditions (2041-2060, SSP126, GCM: MIROC6) — future_2050_ssp126_miroc","text":"","code":"future_2050_ssp126_miroc <- terra::rast(system.file(\"extdata\",                                     \"wc2.1_10m_bioc_MIROC6_ssp126_2041-2060.tif\",                                      package = \"kuenm2\")) terra::plot(future_2050_ssp126_miroc)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp585_access.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing Future Conditions (2041-2060, SSP585, GCM: ACCESS-CM2) — future_2050_ssp585_access","title":"SpatRaster Representing Future Conditions (2041-2060, SSP585, GCM: ACCESS-CM2) — future_2050_ssp585_access","text":"raster layer containing bioclimatic variables representing future climatic conditions (2041-2060) based ACCESS-CM2 General Circulation Model SSP585 scenario. variables obtained 10 arc-minute resolution masked using m region provided package. Data sourced WorldClim: https://worldclim.org/data/cmip6/cmip6climate.html","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp585_access.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing Future Conditions (2041-2060, SSP585, GCM: ACCESS-CM2) — future_2050_ssp585_access","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp585_access.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing Future Conditions (2041-2060, SSP585, GCM: ACCESS-CM2) — future_2050_ssp585_access","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp585_access.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing Future Conditions (2041-2060, SSP585, GCM: ACCESS-CM2) — future_2050_ssp585_access","text":"","code":"future_2050_ssp585_access <- terra::rast(system.file(\"extdata\",                                     \"wc2.1_10m_bioc_ACCESS-CM2_ssp585_2041-2060.tif\",                                      package = \"kuenm2\")) terra::plot(future_2050_ssp585_access)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp585_miroc.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing Future Conditions (2041-2060, SSP585, GCM: MIROC6) — future_2050_ssp585_miroc","title":"SpatRaster Representing Future Conditions (2041-2060, SSP585, GCM: MIROC6) — future_2050_ssp585_miroc","text":"raster layer containing bioclimatic variables representing future climatic conditions (2041-2060) based MIROC6 General Circulation Model SSP585 scenario. variables obtained 10 arc-minute resolution masked using m region provided package. Data sourced WorldClim: https://worldclim.org/data/cmip6/cmip6climate.html","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp585_miroc.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing Future Conditions (2041-2060, SSP585, GCM: MIROC6) — future_2050_ssp585_miroc","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp585_miroc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing Future Conditions (2041-2060, SSP585, GCM: MIROC6) — future_2050_ssp585_miroc","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2050_ssp585_miroc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing Future Conditions (2041-2060, SSP585, GCM: MIROC6) — future_2050_ssp585_miroc","text":"","code":"future_2050_ssp585_miroc <- terra::rast(system.file(\"extdata\",                                     \"wc2.1_10m_bioc_MIROC6_ssp585_2041-2060.tif\",                                      package = \"kuenm2\")) terra::plot(future_2050_ssp585_miroc)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp126_access.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing Future Conditions (2081-2100, SSP126, GCM: ACCESS-CM2) — future_2100_ssp126_access","title":"SpatRaster Representing Future Conditions (2081-2100, SSP126, GCM: ACCESS-CM2) — future_2100_ssp126_access","text":"raster layer containing bioclimatic variables representing future climatic conditions (2081-2100) based ACCESS-CM2 General Circulation Model SSP126 scenario. variables obtained 10 arc-minute resolution masked using m region provided package. Data sourced WorldClim: https://worldclim.org/data/cmip6/cmip6climate.html","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp126_access.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing Future Conditions (2081-2100, SSP126, GCM: ACCESS-CM2) — future_2100_ssp126_access","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp126_access.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing Future Conditions (2081-2100, SSP126, GCM: ACCESS-CM2) — future_2100_ssp126_access","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp126_access.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing Future Conditions (2081-2100, SSP126, GCM: ACCESS-CM2) — future_2100_ssp126_access","text":"","code":"future_2100_ssp126_access <- terra::rast(system.file(\"extdata\",                                     \"wc2.1_10m_bioc_ACCESS-CM2_ssp126_2081-2100.tif\",                                      package = \"kuenm2\")) terra::plot(future_2100_ssp126_access)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp126_miroc.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing Future Conditions (2081-2100, SSP126, GCM: MIROC6) — future_2100_ssp126_miroc","title":"SpatRaster Representing Future Conditions (2081-2100, SSP126, GCM: MIROC6) — future_2100_ssp126_miroc","text":"raster layer containing bioclimatic variables representing future climatic conditions (2081-2100) based MIROC6 General Circulation Model SSP126 scenario. variables obtained 10 arc-minute resolution masked using m region provided package. Data sourced WorldClim: https://worldclim.org/data/cmip6/cmip6climate.html","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp126_miroc.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing Future Conditions (2081-2100, SSP126, GCM: MIROC6) — future_2100_ssp126_miroc","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp126_miroc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing Future Conditions (2081-2100, SSP126, GCM: MIROC6) — future_2100_ssp126_miroc","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp126_miroc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing Future Conditions (2081-2100, SSP126, GCM: MIROC6) — future_2100_ssp126_miroc","text":"","code":"future_2100_ssp126_miroc <- terra::rast(system.file(\"extdata\",                                     \"wc2.1_10m_bioc_MIROC6_ssp126_2081-2100.tif\",                                      package = \"kuenm2\")) terra::plot(future_2100_ssp126_miroc)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp585_access.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing Future Conditions (2081-2100, SSP585, GCM: ACCESS-CM2) — future_2100_ssp585_access","title":"SpatRaster Representing Future Conditions (2081-2100, SSP585, GCM: ACCESS-CM2) — future_2100_ssp585_access","text":"raster layer containing bioclimatic variables representing future climatic conditions (2081-2100) based ACCESS-CM2 General Circulation Model SSP585 scenario. variables obtained 10 arc-minute resolution masked using m region provided package. Data sourced WorldClim: https://worldclim.org/data/cmip6/cmip6climate.html","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp585_access.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing Future Conditions (2081-2100, SSP585, GCM: ACCESS-CM2) — future_2100_ssp585_access","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp585_access.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing Future Conditions (2081-2100, SSP585, GCM: ACCESS-CM2) — future_2100_ssp585_access","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp585_access.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing Future Conditions (2081-2100, SSP585, GCM: ACCESS-CM2) — future_2100_ssp585_access","text":"","code":"future_2100_ssp585_access <- terra::rast(system.file(\"extdata\",                                     \"wc2.1_10m_bioc_ACCESS-CM2_ssp585_2081-2100.tif\",                                      package = \"kuenm2\")) terra::plot(future_2100_ssp585_access)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp585_miroc.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing Future Conditions (2081-2100, SSP585, GCM: MIROC6) — future_2100_ssp585_miroc","title":"SpatRaster Representing Future Conditions (2081-2100, SSP585, GCM: MIROC6) — future_2100_ssp585_miroc","text":"raster layer containing bioclimatic variables representing future climatic conditions (2081-2100) based MIROC6 General Circulation Model SSP585 scenario. variables obtained 10 arc-minute resolution masked using m region provided package. Data sourced WorldClim: https://worldclim.org/data/cmip6/cmip6climate.html","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp585_miroc.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing Future Conditions (2081-2100, SSP585, GCM: MIROC6) — future_2100_ssp585_miroc","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp585_miroc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing Future Conditions (2081-2100, SSP585, GCM: MIROC6) — future_2100_ssp585_miroc","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/future_2100_ssp585_miroc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing Future Conditions (2081-2100, SSP585, GCM: MIROC6) — future_2100_ssp585_miroc","text":"","code":"future_2100_ssp585_miroc <- terra::rast(system.file(\"extdata\",                                     \"wc2.1_10m_bioc_MIROC6_ssp585_2081-2100.tif\",                                      package = \"kuenm2\")) terra::plot(future_2100_ssp585_miroc)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/glm_mx.html","id":null,"dir":"Reference","previous_headings":"","what":"Maxent-like Generalized Linear Models (GLM) — glm_mx","title":"Maxent-like Generalized Linear Models (GLM) — glm_mx","text":"function fits Generalized Linear Model (GLM) binary presence-background data. allows specification custom weights, default presences weight 1 background 100.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/glm_mx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Maxent-like Generalized Linear Models (GLM) — glm_mx","text":"","code":"glm_mx(formula, family = binomial(link = \"cloglog\"), data,        weights = NULL, ...)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/glm_mx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Maxent-like Generalized Linear Models (GLM) — glm_mx","text":"formula formula specifying model fitted, format used glm. family description error distribution link function used model. Defaults binomial(link = \"cloglog\"), commonly used presence-background data. data data.frame containing variables model. Must include column named pr_bg indicates whether record presence (1) background (0), least another column independent variable (predictor). weights Optional. numeric vector weights observation. provided, default weights 1 presences 100 background used. ... Additional arguments passed glm.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/glm_mx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Maxent-like Generalized Linear Models (GLM) — glm_mx","text":"fitted glm object. model object includes minimum maximum values non-factor variables dataset, stored model$varmin model$varmax.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/glm_mx.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Maxent-like Generalized Linear Models (GLM) — glm_mx","text":"details glms using presence background emulating Maxent , see Fithian Hastie (2013) doi:10.1214/13-AOAS667.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/glmnet_mx.html","id":null,"dir":"Reference","previous_headings":"","what":"Maxent-like glmnet models — glmnet_mx","title":"Maxent-like glmnet models — glmnet_mx","text":"function fits Maxent-like models using glmnet package, designed presence-background data.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/glmnet_mx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Maxent-like glmnet models — glmnet_mx","text":"","code":"glmnet_mx(p, data, f, regmult = 1.0, regfun = maxnet.default.regularization,           addsamplestobackground = TRUE, weights = NULL, ...)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/glmnet_mx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Maxent-like glmnet models — glmnet_mx","text":"p vector binary presence-background labels, 1 indicates presence 0 indicates background. data data.frame containing predictor variables model. must include number rows length p. f formula specifying model fitted, format used model.matrix. regmult (numeric) Regularization multiplier, default 1.0. regfun function calculates regularization penalties. Default maxnet.default.regularization. addsamplestobackground (logical) Whether add presence points background background data. Default TRUE. weights (numeric) numeric vector weights observation. Default NULL, sets weights 1 presence points 100 background points. ... Additional arguments pass glmnet.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/glmnet_mx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Maxent-like glmnet models — glmnet_mx","text":"fitted Maxent-like model object class glmnet_mx, includes model coefficients, AIC (requested), elements feature mins maxes, sample means, entropy.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/glmnet_mx.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Maxent-like glmnet models — glmnet_mx","text":"function modified package maxnet fits Maxent-like model using regularization avoid -fitting. Regularization weights computed using provided function (can changed) can multiplied regularization multiplier (regmult). function also includes option calculate AIC.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/import_projections.html","id":null,"dir":"Reference","previous_headings":"","what":"Import rasters resulting from projection functions — import_projections","title":"Import rasters resulting from projection functions — import_projections","text":"function facilitates import rasters generated written disk project_selected(), projection_changes(), variability_projections(), projection_mop() functions. Users can select specific periods (past/future), emission scenarios, General Circulation Models (GCMs), result types import.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/import_projections.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Import rasters resulting from projection functions — import_projections","text":"","code":"import_projections(   projection,   consensus = c(\"median\", \"range\", \"mean\", \"stdev\"),   present = TRUE,   past_period = NULL,   past_gcm = NULL,   future_period = NULL,   future_pscen = NULL,   future_gcm = NULL,   change_types = c(\"summary\", \"by_gcm\", \"by_change\"),   mop_types = c(\"distances\", \"simple\", \"basic\", \"towards_high_combined\",     \"towards_low_combined\", \"towards_high_end\", \"towards_low_end\") )"},{"path":"https://marlonecobos.github.io/kuenm2/reference/import_projections.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Import rasters resulting from projection functions — import_projections","text":"projection object class model_projections, changes_projections, variability_projections, mop_projections. object direct output one projection functions listed description. consensus (character) consensus measures import. Available options : 'median', 'range', 'mean' 'stdev' (standard deviation). Default c(\"median\", \"range\", \"mean\", \"stdev\"), imports options. applicable projection model_projections object. present (logical) wheter import present-day projections. Default TRUE. applicable projection  changes_projections object. past_period (character) names specific past periods (e.g., 'LGM' 'MID') import. Default NULL, meaning available past periods imported. past_gcm (character) names specific General Circulation Models (GCMs) past import. Default NULL, meaning available past GCMs imported. future_period (character) names specific future periods (e.g., '2041-2060' '2081-2100') import. Default NULL, meaning available future periods imported. future_pscen (character) names specific future emission scenarios (e.g., 'ssp126' 'ssp585') import. Default NULL, meaning available future scenarios imported. future_gcm (character) names specific General Circulation Models (GCMs) future import. Default NULL, meaning available future GCMs imported. change_types (character) names type computed changes import. Available options : 'summary', 'by_gcm', 'by_change' 'binarized'. Default c(\"summary\", \"by_gcm\", \"by_change\"), importing types. applicable projection changes_projections object. mop_types (character) type(s) MOP import. Available options : 'basic', 'simple', 'towards_high_combined', 'towards_low_combined', towards_high_end', 'towards_low_end'. Default NULL, meaning available MOPs imported. applicable projection mop_projections object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/import_projections.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Import rasters resulting from projection functions — import_projections","text":"SpatRaster list SpatRasters, structured according input projection class: projection model_projections: stacked SpatRaster containing selected projections. projection changes_projections: list SpatRasters, organized selected change_types (e.g., 'summary', 'by_gcm', /'by_change'). projection mop_projections: list SpatRasters, organized selected mop_types (e.g., 'simple' 'basic'). projection variability_projections: list SpatRasters, containing computed variability.","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/import_projections.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Import rasters resulting from projection functions — import_projections","text":"","code":"# Load packages library(terra) #> terra 1.8.86 # Step 1: Organize variables for current projection ## Import current variables (used to fit models) var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  ## Create a folder in a temporary directory to copy the variables out_dir_current <- file.path(tempdir(), \"Current_raw2\") dir.create(out_dir_current, recursive = TRUE)  ## Save current variables in temporary directory terra::writeRaster(var, file.path(out_dir_current, \"Variables.tif\"))   # Step 2: Organize future climate variables (example with WorldClim) ## Directory containing the downloaded future climate variables (example) in_dir <- system.file(\"extdata\", package = \"kuenm2\")  ## Create a folder in a temporary directory to copy the future variables out_dir_future <- file.path(tempdir(), \"Future_raw2\")  ## Organize and rename the future climate data (structured by year and GCM) ### 'SoilType' will be appended as a static variable in each scenario organize_future_worldclim(input_dir = in_dir, output_dir = out_dir_future,                           name_format = \"bio_\", fixed_variables = var$SoilType) #>    |                                                                               |                                                                      |   0%   |                                                                               |=========                                                             |  12%   |                                                                               |==================                                                    |  25%   |                                                                               |==========================                                            |  38%   |                                                                               |===================================                                   |  50%   |                                                                               |============================================                          |  62%   |                                                                               |====================================================                  |  75%   |                                                                               |=============================================================         |  88%   |                                                                               |======================================================================| 100% #>  #> Variables successfully organized in directory: #> /tmp/Rtmp3eUeuI/Future_raw2  # Step 3: Prepare data to run multiple projections ## An example with maxnet models ## Import example of fitted_models (output of fit_selected()) data(fitted_model_maxnet, package = \"kuenm2\")  ## Prepare projection data using fitted models to check variables pr <- prepare_projection(models = fitted_model_maxnet,                          present_dir = out_dir_current,                          future_dir = out_dir_future,                          future_period = \"2041-2060\",                          future_pscen = c(\"ssp126\", \"ssp585\"),                          future_gcm = c(\"ACCESS-CM2\", \"MIROC6\"),                          raster_pattern = \".tif*\")  # Step 4: Run multiple model projections ## A folder to save projection results out_dir <- file.path(tempdir(), \"Projection_results/maxnet\") dir.create(out_dir, recursive = TRUE)  ## Project selected models to multiple scenarios p <- project_selected(models = fitted_model_maxnet, projection_data = pr,                       out_dir = out_dir) #>    |                                                                               |                                                                      |   0%   |                                                                               |==============                                                        |  20%   |                                                                               |============================                                          |  40%   |                                                                               |==========================================                            |  60%   |                                                                               |========================================================              |  80%   |                                                                               |======================================================================| 100%  # Use import_projections to import results: raster_p <- import_projections(projection = p, consensus = \"mean\") plot(raster_p)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/independent_evaluation.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluate models with independent data — independent_evaluation","title":"Evaluate models with independent data — independent_evaluation","text":"function evaluates selected models using independent data (.e., data used model calibration). function computes omission rate pROC, optionally assesses whether environmental conditions independent data analogous (.e., within range) calibration data.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/independent_evaluation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluate models with independent data — independent_evaluation","text":"","code":"independent_evaluation(fitted_models, new_data,                               consensus = c(\"mean\", \"median\"),                               type = \"cloglog\", extrapolation_type = \"E\",                               var_to_clamp = NULL, perform_mop = TRUE,                               mop_type = \"detailed\",                               calculate_distance = TRUE,                               where_distance = \"all\",                               return_predictions = TRUE,                               return_binary = TRUE,                               progress_bar = FALSE, ...)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/independent_evaluation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluate models with independent data — independent_evaluation","text":"fitted_models object class fitted_models returned fit_selected() function. new_data data.frame containing environmental variables independent test records. column names must correspond exactly environmental variables used fit selected models, row individual test record. consensus (character) vector specifying types consensus use. Available options \"median\" \"mean\". Default c(\"median\", \"mean\"). type (character) format prediction values. maxnet models, valid options \"raw\", \"cumulative\", \"logistic\", \"cloglog\". glm models, valid options \"response\" \"cloglog\". Default \"cloglog\". extrapolation_type (character) extrapolation type model. Models can transferred three options: free extrapolation ('E'), extrapolation clamping ('EC'), extrapolation ('NE'). Default = 'E'. See details. var_to_clamp (character) vector specifying variables clamp extrapolate. applicable extrapolation_type \"EC\" \"NE\". Default NULL, meaning variables clamped extrapolated. perform_mop (logical) whether execute Mobility-Oriented Parity (MOP) analysis. analysis assesses environmental conditions new_data analogous (within ranges) calibration data. Defaults TRUE. mop_type (character) type MOP analysis performed. Options available \"basic\", \"simple\" \"detailed\". Default 'simples'. See projection_mop() details. calculate_distance (logical) whether calculate distances (dissimilarities) new_data calibration data. Default TRUE. where_distance (character) specifies values new_data used calculate distances. Options : \"in_range\" (conditions within calibration range), \"out_range\" (conditions outside calibration range), \"\" (conditions). Default \"\". return_predictions (logical) whether return continuous predictions locations independent records new_data. Default TRUE. return_binary (logical) whether return binary predictions locations independent records new_data. predictions binarized using respective thresholds stores fitted_models. Default TRUE. progress_bar (logical) whether display progress bar mop processing. Default FALSE. ... additional arguments passed mop().","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/independent_evaluation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evaluate models with independent data — independent_evaluation","text":"list containing following elements: evaluation: data.frame omission rate pROC values selected model consensus. mop_results: (perform_mop = TRUE) object class mop_results, metrics environmental similarity calibration independent data. predictions: (return_predictions = TRUE) list data.frames containing continuous binary predictions independent record locations, along MOP distances, indicator whether environmental conditions location fall within calibration range, identity variables lower higher values calibration range. fitted_models object includes categorical variables, returned data.frame also contain columns indicating values new_data present calibration data.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/independent_evaluation.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Evaluate models with independent data — independent_evaluation","text":"","code":"# Example with maxnet # Import example of fitted_models (output of fit_selected()) data(\"fitted_model_maxnet\", package = \"kuenm2\")  # Import independent records to evaluate the models data(\"new_occ\", package = \"kuenm2\")  # Import raster layers var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  #Extract variables to occurrences new_data <- extract_occurrence_variables(occ = new_occ, x = \"x\", y = \"y\",                                          raster_variables = var)  #Add some fake data beyond the limits of calibration ranges fake_data <- data.frame(\"pr_bg\" = c(1, 1, 1),                         \"x\" = c(NA, NA, NA),                         \"y\" = c(NA, NA, NA),                         \"bio_1\" = c(10, 15, 23),                         \"bio_7\" = c(12, 16, 20),                         \"bio_12\" = c(2300, 2000, 1000),                         \"bio_15\" = c(30, 40, 50),                         \"SoilType\" = c(1, 1, 1)) new_data <- rbind(new_data, fake_data)   # Evaluate models with independent data res_ind <- independent_evaluation(fitted_models = fitted_model_maxnet,                                   new_data = new_data)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/initial_cleaning.html","id":null,"dir":"Reference","previous_headings":"","what":"Initial occurrence data cleaning steps — initial_cleaning","title":"Initial occurrence data cleaning steps — initial_cleaning","text":"Simple occurrence data cleaning procedures.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/initial_cleaning.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Initial occurrence data cleaning steps — initial_cleaning","text":"","code":"initial_cleaning(data, species, x, y,                  other_columns = NULL, keep_all_columns = TRUE,                  sort_columns = TRUE, remove_na = TRUE, remove_empty = TRUE,                  remove_duplicates = TRUE, by_decimal_precision = FALSE,                  decimal_precision = 0, longitude_precision = NULL,                  latitude_precision = NULL)  sort_columns(data, species, x, y, keep_all_columns = FALSE)  remove_missing(data, columns = NULL, remove_na = TRUE,                remove_empty = TRUE, keep_all_columns = TRUE)  remove_duplicates(data, columns = NULL, keep_all_columns = TRUE)  remove_corrdinates_00(data, x, y)  filter_decimal_precision(data, x,                          y, decimal_precision = 0,                          longitude_precision = NULL,                          latitude_precision = NULL)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/initial_cleaning.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Initial occurrence data cleaning steps — initial_cleaning","text":"data data.frame occurrence records. species (character) name column data containing species name. x (character) name column data containing longitude values. y (character) name column data containing latitude values. other_columns (character) vector column name(s) data considered performing cleaning steps, default = NULL. keep_all_columns (logical) whether keep columns data. Default = TRUE. sort_columns (logical) whether sort species, longitude, latitude columns data. Default = TRUE. remove_na (logical) whether remove NA values columns considered. Default = TRUE. remove_empty (logical) whether remove empty (missing) values columns considered. Default = TRUE. remove_duplicates (logical) whether remove duplicates columns considered. Default = TRUE. by_decimal_precision (logical) whether remove certain records coordinate precision lower following three parameters. Default = FALSE decimal_precision (numeric) decimal precision threshold coordinates. Default = 0. Ignored following two parameters defined. longitude_precision (numeric) decimal precision threshold longitude. Default = NULL. latitude_precision (numeric) decimal precision threshold latitude. Default = NULL. columns (character) vector additional column name(s) data considered removing missing duplicate records, default = NULL.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/initial_cleaning.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Initial occurrence data cleaning steps — initial_cleaning","text":"data.frame resulting occurrence records.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/initial_cleaning.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Initial occurrence data cleaning steps — initial_cleaning","text":"Function initial_cleaning helps perform simple steps data cleaning.","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/initial_cleaning.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Initial occurrence data cleaning steps — initial_cleaning","text":"","code":"# Import occurrences data(occ_data_noclean, package = \"kuenm2\")  # remove missing data mis <- remove_missing(data = occ_data_noclean, columns = NULL, remove_na = TRUE,                       remove_empty = TRUE)  # remove exact duplicates mis_dup <- remove_duplicates(data = mis, columns = NULL, keep_all_columns = TRUE)  # remove records with 0 for x and y coordinates mis_dup_00 <- remove_corrdinates_00(data = mis_dup, x = \"x\", y = \"y\")  # remove coordinates with low decimal precision. mis_dup_00_dec <- filter_decimal_precision(data = mis_dup_00, x = \"x\", y = \"y\",                                            decimal_precision = 2)  # all basic cleaning steps clean_init <- initial_cleaning(data = occ_data_noclean, species = \"species\",                                x = \"x\", y = \"y\", remove_na = TRUE,                                remove_empty = TRUE, remove_duplicates = TRUE,                                by_decimal_precision = TRUE,                                decimal_precision = 2)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/kuenm2-package.html","id":null,"dir":"Reference","previous_headings":"","what":"kuenm2: Detailed Development of Ecological Niche Models — kuenm2-package","title":"kuenm2: Detailed Development of Ecological Niche Models — kuenm2-package","text":"kuenm2 new set tools help development detailed ecological niche models using multiple algorithms, moment Maxnet GLM. Pre-modeling analyses explorations can done prepare data. Model calibration (model selection) can done creating testing several candidate models, later selected based multicriteria approach. Handy options producing final models transfers included. tools assess extrapolation risks variability model transfers also available.","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/kuenm2-package.html","id":"pre-modeling-steps","dir":"Reference","previous_headings":"","what":"Pre-modeling steps","title":"kuenm2: Detailed Development of Ecological Niche Models — kuenm2-package","text":"Data preparation: initial_cleaning(), advanced_cleaning(), prepare_data(), prepare_user_data() Data exploration: explore_calibration_hist(), explore_partition_env(), explore_partition_geo(), explore_partition_extrapolation(), plot_calibration_hist(), plot_explore_partition()","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/kuenm2-package.html","id":"modeling-process","dir":"Reference","previous_headings":"","what":"Modeling process","title":"kuenm2: Detailed Development of Ecological Niche Models — kuenm2-package","text":"Model calibration: calibration(), select_models() Model exploration: fit_selected(), variable_importance(), plot_importance(), response_curve(), all_response_curves(), bivariate_response(), partition_response_curves() Model projection: predict_selected(), organize_for_projection(), organize_future_worldclim(), prepare_projection(), project_selected()","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/kuenm2-package.html","id":"post-modeling-analysis","dir":"Reference","previous_headings":"","what":"Post-modeling analysis","title":"kuenm2: Detailed Development of Ecological Niche Models — kuenm2-package","text":"Variability: projection_changes(), projection_variability() Uncertainty: projection_mop()","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/kuenm2-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"kuenm2: Detailed Development of Ecological Niche Models — kuenm2-package","text":"Maintainer: Weverton C. F. Trindade wevertonf1993@gmail.com (ORCID) Authors: Luis F. Arias-Giraldo lfarias.giraldo@gmail.com (ORCID) Luis Osorio-Olvera luismurao@gmail.com (ORCID) . Townsend Peterson town@ku.edu (ORCID) Marlon E. Cobos manubio13@gmail.com (ORCID)","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/kuenm2_discrete_palletes.html","id":null,"dir":"Reference","previous_headings":"","what":"Discrete palettes based on pals R package — kuenm2_discrete_palletes","title":"Discrete palettes based on pals R package — kuenm2_discrete_palletes","text":"Color palettes designed discrete, categorical data. Palettes retrived pals R package","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/kuenm2_discrete_palletes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Discrete palettes based on pals R package — kuenm2_discrete_palletes","text":"","code":"data(\"kuenm2_discrete_palletes\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/kuenm2_discrete_palletes.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Discrete palettes based on pals R package — kuenm2_discrete_palletes","text":"list following color palettes: \"alphabet\", \"alphabet2\", \"cols25\", \"glasbey\", \"kelly\", \"polychrome\", \"stepped\", \"stepped2\", \"stepped3\", \"okabe\", \"tableau20\", \"tol\", \"tol.groundcover\", \"trubetskoy\", \"watlington\"","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/kuenm2_discrete_palletes.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Discrete palettes based on pals R package — kuenm2_discrete_palletes","text":"Wright K (2023). pals: Color Palettes, Colormaps, Tools Evaluate Them_. R package version 1.8, https://CRAN.R-project.org/package=pals.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/m.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatVector Representing Calibration Area for Myrcia hatschbachii — m","title":"SpatVector Representing Calibration Area for Myrcia hatschbachii — m","text":"spatial vector defining calibration area used extract background points fitting models Myrcia hatschbachii. area generated creating minimum convex polygon around presence records (occ_data), applying 300 km buffer.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/m.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatVector Representing Calibration Area for Myrcia hatschbachii — m","text":"Spatvector object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/m.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatVector Representing Calibration Area for Myrcia hatschbachii — m","text":"return value. Used function vect bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/m.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatVector Representing Calibration Area for Myrcia hatschbachii — m","text":"","code":"m <- terra::vect(system.file(\"extdata\",                              \"m.gpkg\",                               package = \"kuenm2\")) terra::plot(m)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/new_occ.html","id":null,"dir":"Reference","previous_headings":"","what":"Independent Species Occurrence — new_occ","title":"Independent Species Occurrence — new_occ","text":"data.frame containing coordinates 82 occurrences Myrcia hatschbachii (tree endemic southern Brazil). valid occurrences sourced NeotropicTree (Oliveira-Filho, 2017) used independent data test models fitted occ_data.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/new_occ.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Independent Species Occurrence — new_occ","text":"","code":"data(\"new_occ\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/new_occ.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Independent Species Occurrence — new_occ","text":"data.frame following columns: species species name. x Longitude. y Latitude.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/new_occ.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Independent Species Occurrence — new_occ","text":"Oliveira_Filho, .T. 2017. NeoTropTree, Flora arbórea da Região Neotropical: Um banco de dados envolvendo biogeografia, diversidade e conservação. Universidade Federal de Minas Gerais. (http://www.neotroptree,info).","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/occ_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Species Occurrence — occ_data","title":"Species Occurrence — occ_data","text":"data.frame containing coordinates 51 valid occurrences Myrcia hatschbachii (tree endemic southern Brazil). valid occurrences sourced Trindade & Marques (2024) contains records retrieved GBIF SpeciesLink.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/occ_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Species Occurrence — occ_data","text":"","code":"data(\"occ_data\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/occ_data.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Species Occurrence — occ_data","text":"data.frame following columns: species species name. x Longitude. y Latitude.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/occ_data.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Species Occurrence — occ_data","text":"Trindade, W.C.F., Marques, M.C.M., 2023. Invisible Species: Big Data Unveil Coverage Gaps Atlantic Forest Hotspot. Diversity Distributions 30, e13931. https://doi.org/10.1111/ddi.13931","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/occ_data_noclean.html","id":null,"dir":"Reference","previous_headings":"","what":"Species Occurrence with Erroneous Records — occ_data_noclean","title":"Species Occurrence with Erroneous Records — occ_data_noclean","text":"data.frame containing coordinates 51 valid occurrences Myrcia hatschbachii (tree endemic southern Brazil), along set erroneous records used demonstrate data cleaning procedures. valid occurrences sourced Trindade & Marques (2024).","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/occ_data_noclean.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Species Occurrence with Erroneous Records — occ_data_noclean","text":"","code":"data(\"occ_data_noclean\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/occ_data_noclean.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Species Occurrence with Erroneous Records — occ_data_noclean","text":"data.frame following columns: species species name. x Longitude. y Latitude.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/occ_data_noclean.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Species Occurrence with Erroneous Records — occ_data_noclean","text":"Trindade, W.C.F., Marques, M.C.M., 2023. Invisible Species: Big Data Unveil Coverage Gaps Atlantic Forest Hotspot. Diversity Distributions 30, e13931. https://doi.org/10.1111/ddi.13931","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/organize_for_projection.html","id":null,"dir":"Reference","previous_headings":"","what":"Organize and structure variables for past and future projections — organize_for_projection","title":"Organize and structure variables for past and future projections — organize_for_projection","text":"function helpts organize climate variable files past future scenarios folders categorized time period (\"Past\" \"Future\"), specific period (e.g., \"LGM\" \"2081–2100\"), emission scenario (e.g., \"ssp585\"), GCMs. structure simplifies preparation climate data ensures compatibility prepare_projection() function, making variables properly organized modeling projections. See Details information.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/organize_for_projection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Organize and structure variables for past and future projections — organize_for_projection","text":"","code":"organize_for_projection(output_dir, models = NULL,                                variable_names = NULL,                                categorical_variables = NULL,                                present_file = NULL,                                past_files = NULL, past_period = NULL,                                past_gcm = NULL, future_files = NULL,                                future_period = NULL, future_pscen = NULL,                                future_gcm = NULL, fixed_variables = NULL,                                check_extent = TRUE,                                resample_to_present = TRUE, mask = NULL,                                overwrite = FALSE)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/organize_for_projection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Organize and structure variables for past and future projections — organize_for_projection","text":"output_dir (character) path folder organized data saved. models object class fitted_models returned fit_selected() function. Default NULL. variable_names (character) names variables used fit model PCA prepare_data() function. applicable 'models' argument provided. Default NULL. categorical_variables (character) names variables categorical. Default NULL. present_file (character) full paths variables present scenario. Default NULL. past_files (character) full paths variables past scenario(s). Default NULL. past_period (character) names subfolders within 'past_files', representing specific time periods (e.g., 'LGM' 'MID'). applicable 'past_files' provided. Default NULL. past_gcm (character) names subfolders within 'past_files', representing specific General Circulation Models (GCMs). applicable 'past_files' provided. Default NULL. future_files (character) full paths variables future scenario(s). Default NULL. future_period (character) names subfolders within 'future_files', representing specific time periods (e.g., '2041-2060' '2081-2100'). applicable 'future_files' provided. Default NULL. future_pscen (character) names subfolders within 'future_files', representing specific emission scenarios (e.g., 'ssp126' 'ssp585'). applicable 'future_files' provided. Default NULL. future_gcm (character) names subfolders within 'future_files', representing specific General Circulation Models (GCMs). applicable 'future_files' provided. Default NULL. fixed_variables (SpatRaster) optional static variables (.e., soil type) used model, remain unchanged past future scenarios. variable included scenario. Default NULL. check_extent (logical) whether ensure 'fixed_variables' spatial extent bioclimatic variables. Applicable 'fixed_variables' provided. Default TRUE. resample_to_present (logical) whether resample past future variables match extent present variables. used 'present_file' provided. Default TRUE. mask (SpatRaster, SpatVector, SpatExtent) spatial object used mask variables (optional). Default NULL. overwrite whether overwrite existing files output directory. Default FALSE.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/organize_for_projection.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Organize and structure variables for past and future projections — organize_for_projection","text":"message indicating variables successfully organized 'output_dir' directory.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/organize_for_projection.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Organize and structure variables for past and future projections — organize_for_projection","text":"listed input rasters must stored .tif files, one file per scenario. Filenames include identifiable patterns time period, GCM, (future scenarios) emission scenario (SSP). example: file representing \"Past\" conditions \"LGM\" period using \"MIROC6\" GCM named: \"Past_LGM_MIROC6.tif\" file representing \"Future\" conditions period \"2081–2100\" emission scenario \"ssp585\" GCM \"ACCESS-CM2\" named: \"Future_2081-2100_ssp585_ACCESS-CM2.tif\" scenario files must contain variable names (e.g., bio1, bio2, etc.) units used model calibration present-day data. Tip: listing files, use list.files(path, full.names = TRUE) obtain full file paths required function.","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/organize_for_projection.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Organize and structure variables for past and future projections — organize_for_projection","text":"","code":"# Set the input directory containing the climate variables. # In this example, we use present and LGM variables from CHELSA # located in the \"inst/extdata\" folder of the package. present_lgm_dir <- system.file(\"extdata\", package = \"kuenm2\")  # Define an output directory (here, using a temporary folder) # Replace with your own working directory if needed. out_dir <- file.path(tempdir(), \"Projection_variables\")  # List files for present-day conditions present_list <- list.files(path = present_lgm_dir,                            pattern = \"Current_CHELSA\", # Select only CHELSA present-day files                            full.names = TRUE)  # List files for LGM conditions lgm_list <- list.files(path = present_lgm_dir,                        pattern = \"LGM\", # Select only LGM files                        full.names = TRUE)  # Organize variables for projection organize_for_projection(output_dir = out_dir,                         variable_names = c(\"bio1\", \"bio7\", \"bio12\", \"bio15\"),                         present_file = present_list,                         past_files = lgm_list,                         past_period = \"LGM\",                         past_gcm = c(\"CCSM4\", \"CNRM-CM5\", \"FGOALS-g2\",                                      \"IPSL-CM5A-LR\", \"MIROC-ESM\", \"MPI-ESM-P\",                                      \"MRI-CGCM3\"),                         resample_to_present = TRUE,                         overwrite = TRUE) #>  #> Variables successfully organized in directory: #> /tmp/Rtmp3eUeuI/Projection_variables"},{"path":"https://marlonecobos.github.io/kuenm2/reference/organize_future_worldclim.html","id":null,"dir":"Reference","previous_headings":"","what":"Organize and structure future climate variables from WorldClim — organize_future_worldclim","title":"Organize and structure future climate variables from WorldClim — organize_future_worldclim","text":"function imports future climate variables downloaded WorldClim, renames files, organizes folders categorized year, emission scenario (SSP) General Circulation Model (GCM). simplifies preparation climate data, making compatible prepare_projection() function, ensuring required variables properly structured modeling projections.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/organize_future_worldclim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Organize and structure future climate variables from WorldClim — organize_future_worldclim","text":"","code":"organize_future_worldclim(input_dir, output_dir, name_format = \"bio_\",                           variables = NULL, fixed_variables = NULL,                           check_extent = TRUE, mask = NULL,                           progress_bar = TRUE, overwrite = FALSE)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/organize_future_worldclim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Organize and structure future climate variables from WorldClim — organize_future_worldclim","text":"input_dir (character) path folder containing future climate variables downloaded WorldClim. output_dir (character) path folder organized data saved. name_format (character) format renaming variable. Options \"bio_\", \"Bio_\", \"bio_0\", \"Bio_0\". See details information. Default \"bio_\". variables (character) names variables retain. Default NULL, meaning variables kept. fixed_variables (SpatRaster) optional static variables (.e., soil type) used model, remain unchanged future scenarios. variable included future scenario. Default NULL. check_extent (logical) whether ensure fixed_variables spatial extent bioclimatic variables. Applicable fixed_variables provided. Default TRUE. mask (SpatRaster, SpatVector, SpatExtent) spatial object used mask variables (optional). Default NULL. progress_bar (logical) whether display progress bar processing. Default TRUE. overwrite whether overwrite existing files output directory. Default FALSE.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/organize_future_worldclim.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Organize and structure future climate variables from WorldClim — organize_future_worldclim","text":"list paths folders organized climate data saved.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/organize_future_worldclim.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Organize and structure future climate variables from WorldClim — organize_future_worldclim","text":"raw variables downloaded WorldClim named \"Bio01\", \"Bio02\", \"Bio03\", \"Bio10\", etc. name_format parameter controls variables renamed: \"bio_\": variables renamed bio_1, bio_2, bio_3, bio_10, etc. \"bio_0\": variables renamed bio_01, bio_02, bio_03, bio_10, etc \"Bio_\": variables renamed Bio_1, Bio_2, Bio_3, Bio_10, etc. \"Bio_0\": variables renamed Bio_01, Bio_02, Bio_03, Bio_10, etc.","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/organize_future_worldclim.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Organize and structure future climate variables from WorldClim — organize_future_worldclim","text":"","code":"# Import the current variables used to fit the model. # In this case, SoilType will be treated as a static variable (constant # across future scenarios). var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  # Set the input directory containing the raw future climate variables. # For this example, the data is located in the \"inst/extdata\" folder. in_dir <- system.file(\"extdata\", package = \"kuenm2\")  # Create a \"Future_raw\" folder in a temporary directory and copy the raw # variables there. out_dir <- file.path(tempdir(), \"Future_raw\")  # Organize and rename the future climate data, structuring it by year and GCM. # The 'SoilType' variable will be appended as a static variable in each scenario. # The files will be renamed following the \"bio_\" format organize_future_worldclim(input_dir = in_dir, output_dir = out_dir,                           name_format = \"bio_\",                           fixed_variables = var$SoilType) #>    |                                                                               |                                                                      |   0%   |                                                                               |=========                                                             |  12%   |                                                                               |==================                                                    |  25%   |                                                                               |==========================                                            |  38%   |                                                                               |===================================                                   |  50%   |                                                                               |============================================                          |  62%   |                                                                               |====================================================                  |  75%   |                                                                               |=============================================================         |  88%   |                                                                               |======================================================================| 100% #>  #> Variables successfully organized in directory: #> /tmp/Rtmp3eUeuI/Future_raw  # Check files organized dir(out_dir, recursive = TRUE) #> [1] \"2041-2060/ssp126/ACCESS-CM2/Variables.tif\" #> [2] \"2041-2060/ssp126/MIROC6/Variables.tif\"     #> [3] \"2041-2060/ssp585/ACCESS-CM2/Variables.tif\" #> [4] \"2041-2060/ssp585/MIROC6/Variables.tif\"     #> [5] \"2081-2100/ssp126/ACCESS-CM2/Variables.tif\" #> [6] \"2081-2100/ssp126/MIROC6/Variables.tif\"     #> [7] \"2081-2100/ssp585/ACCESS-CM2/Variables.tif\" #> [8] \"2081-2100/ssp585/MIROC6/Variables.tif\""},{"path":"https://marlonecobos.github.io/kuenm2/reference/partial_roc.html","id":null,"dir":"Reference","previous_headings":"","what":"Partial ROC calculation for multiple candidate models — partial_roc","title":"Partial ROC calculation for multiple candidate models — partial_roc","text":"Computes partial ROC tests multiple candidate models.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/partial_roc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Partial ROC calculation for multiple candidate models — partial_roc","text":"","code":"partial_roc(formula_grid, data, omission_rate = 10,             addsamplestobackground = TRUE, weights = NULL,             algorithm = \"maxnet\", parallel = FALSE, ncores = NULL,             progress_bar = TRUE)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/partial_roc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Partial ROC calculation for multiple candidate models — partial_roc","text":"formula_grid data.frame grid formulas defining candidate models test. data object class prepared_data returned prepare_data() function object class calibration_results returned calibration() function. contains calibration data k-folds. omission_rate (numeric) values 0 100 representing percentage potential error due source uncertainty. value used calculate omission rate. Default 10. See details. addsamplestobackground (logical) whether add background presence sample already . Default TRUE. weights (numeric) numeric vector specifying weights occurrence records. Default NULL. algorithm (character) type algorithm, either \"glm\" \"maxnet\". Default \"maxnet\". parallel (logical) whether fit candidate models parallel. Default FALSE. ncores (numeric) number cores use parallel processing. Default NULL uses available cores - 1. applicable parallel = TRUE. progress_bar (logical) whether display progress bar processing. Default TRUE.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/partial_roc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Partial ROC calculation for multiple candidate models — partial_roc","text":"data frame summary statistics AUC ratios significance calculated replicates candidate model. Specifically, includes mean standard deviation metrics model.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/partial_roc.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Partial ROC calculation for multiple candidate models — partial_roc","text":"Partial ROC calculated following Peterson et al. (2008) doi:10.1016/j.ecolmodel.2007.11.008.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/partial_roc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Partial ROC calculation for multiple candidate models — partial_roc","text":"","code":"# Import prepared data to get model formulas data(sp_swd, package = \"kuenm2\")  # Calculate proc for the first 5 candidate models res_proc <- partial_roc(formula_grid = sp_swd$formula_grid[1:2,],                         data = sp_swd, omission_rate = 10,                         algorithm = \"maxnet\") #>    |                                                                               |                                                                      |   0%   |                                                                               |===================================                                   |  50%   |                                                                               |======================================================================| 100%"},{"path":"https://marlonecobos.github.io/kuenm2/reference/partition_response_curves.html","id":null,"dir":"Reference","previous_headings":"","what":"Response curves for selected models according to training/testing partitions — partition_response_curves","title":"Response curves for selected models according to training/testing partitions — partition_response_curves","text":"Variable responses models selected model calibration. Responses based training partitions points testing presence records.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/partition_response_curves.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Response curves for selected models according to training/testing partitions — partition_response_curves","text":"","code":"partition_response_curves(calibration_results, modelID, n = 100,                           averages_from = \"pr_bg\", col = \"darkblue\",                           ylim = NULL, las = 1, parallel = FALSE,                           ncores = NULL, ...)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/partition_response_curves.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Response curves for selected models according to training/testing partitions — partition_response_curves","text":"calibration_results object class calibration_results returned calibration() function. modelID (character numeric) number Model (ID) considered plotting. n (numeric) integer guiding number breaks. Default = 100 averages_from (character) specifies averages modes variables calculated. Available options \"pr\" (calculate averages presence localities) \"pr_bg\" (use combined set presence background localities). Default \"pr_bg\". See details. col (character) color lines. Default = \"darkblue\". ylim (numeric) vector length two indicating minimum maximum limits y axis. default, NULL, uses c(0, 1). las (numeric) stile axis tick labels; options : 0, 1, 2, 3. Default = 1. parallel (logical) whether fit models parallel. Default FALSE. ncores (numeric) number cores use parallel processing. Default NULL uses available cores - 1. applicable parallel = TRUE. ... additional arguments passed plot.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/partition_response_curves.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Response curves for selected models according to training/testing partitions — partition_response_curves","text":"plot response curves variables used selected model corresponding modelID. row plot shows response curves produced training data leaves partition laveled. points represent records left testing.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/partition_response_curves.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Response curves for selected models according to training/testing partitions — partition_response_curves","text":"Response curves generated using training portions data points showed ones left testing. partition labeled plot panels indicates portion left testing. response curves generated variables set mean values (mode categorical variables), calculated either presence localities (averages_from = \"pr\") combined set presence background localities (averages_from = \"pr_bg\"). categorical variables, bar plot generated error bars showing variability across models (multiple models included).","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/partition_response_curves.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Response curves for selected models according to training/testing partitions — partition_response_curves","text":"","code":"# Example with maxnet # Import example of calibration results data(calib_results_maxnet, package = \"kuenm2\")  # Options of models that can be tested calib_results_maxnet$selected_models$ID #> [1] 192 219  # Response curves partition_response_curves(calibration_results = calib_results_maxnet,                           modelID = 192)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/perform_pca.html","id":null,"dir":"Reference","previous_headings":"","what":"Principal Component Analysis for raster layers — perform_pca","title":"Principal Component Analysis for raster layers — perform_pca","text":"function performs principal component analysis (PCA) set raster variables.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/perform_pca.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Principal Component Analysis for raster layers — perform_pca","text":"","code":"perform_pca(raster_variables, exclude_from_pca = NULL, project = FALSE,             projection_data = NULL, out_dir = NULL, overwrite = FALSE,             progress_bar = FALSE, center = TRUE, scale = FALSE,             variance_explained = 95, min_explained = 5)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/perform_pca.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Principal Component Analysis for raster layers — perform_pca","text":"raster_variables (SpatRaster) set predictor variables function summarize set orthogonal, uncorrelated components based PCA. exclude_from_pca (character) variable names within raster_variables included PCA transformation. Instead, variables added directly final set output variables without modified. default NULL, meaning variables used unless specified otherwise. project (logical) whether function project new data different scenarios (e.g. future variables) onto PCA coordinates generated initial analysis. TRUE, argument projection_data needs defined. Default FALSE. projection_data object class prepared_projection returned prepare_projection() function. file contains paths raster files representing scenario. applicable project = TRUE. Default NULL. out_dir (character) path root directory saving raster files projection. Default = NULL. overwrite (logical) whether overwrite SpatRaster already exists projecting. applicable write_files set TRUE. Default FALSE. progress_bar (logical) whether display progress bar processing projections. applicable project = TRUE. Default FALSE center (logical) whether variables zero-centered. Default TRUE. scale (logical) whether variables scaled unit variance analysis takes place. Default FALSE. variance_explained (numeric) cumulative percentage total variance must explained selected principal components. Default 95. min_explained (numeric) minimum percentage total variance principal component must explain retained. Default 5.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/perform_pca.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Principal Component Analysis for raster layers — perform_pca","text":"list containing following elements: env: SpatRaster object contains orthogonal components derived PCA. PCs correspond variables used perform analysis. pca: object class prcomp, containing details PCA analysis. See prcomp(). variance_explained_cum_sum: cumulative percentage total variance explained selected principal components. value indicates much data’s original variability captured PCA transformation. projection_directory: root directory projection files saved. NULL project set TRUE. directory contains projected raster files scenario.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/perform_pca.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Principal Component Analysis for raster layers — perform_pca","text":"","code":"# PCA with current variables # Import raster layers var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  # PCA pca_var <- perform_pca(raster_variables = var, exclude_from_pca = \"SoilType\",                        center = TRUE, scale = TRUE)  pca_var #> $env #> class       : SpatRaster  #> size        : 52, 40, 5  (nrow, ncol, nlyr) #> resolution  : 0.1666667, 0.1666667  (x, y) #> extent      : -53.5, -46.83333, -30.83333, -22.16667  (xmin, xmax, ymin, ymax) #> coord. ref. : lon/lat WGS 84 (EPSG:4326)  #> sources     : memory  (4 layers)  #>               Current_variables.tif   #> varnames    : Current_variables  #>               Current_variables  #> names       :       PC1,       PC2,       PC3,       PC4, SoilType  #> min values  : -3.621362, -2.041276, -3.923471, -1.730859,        1  #> max values  :  2.929786,  3.029667,  1.752452,  1.601162,       23  #>  #> $pca #> Standard deviations (1, .., p=4): #> [1] 1.4574175 0.9290330 0.8020786 0.6078666 #>  #> Rotation (n x k) = (4 x 4): #>               PC1        PC2         PC3         PC4 #> bio_1  -0.5327531 -0.1500983 -0.66580466  0.50034864 #> bio_7   0.3338164 -0.9359166 -0.09775690 -0.05541088 #> bio_12  0.5032916  0.2775767 -0.73525490 -0.35923383 #> bio_15 -0.5928223 -0.1564666 -0.08091963 -0.78583200 #>  #> $variance_explained_cumsum #>       PC1       PC2       PC3       PC4  #>  53.10165  74.67920  90.76246 100.00000  #>  #> $projection_directory #> NULL #>   # Project PCA for new scenarios (future) # First, organize and prepare future variables # Set the input directory containing the raw future climate variables # For this example, the data is located in the \"inst/extdata\" folder. in_dir <- system.file(\"extdata\", package = \"kuenm2\")  # Create a \"Future_raw\" folder in a temporary directory and copy the variables. out_dir_future <- file.path(tempdir(), \"Future_raw1\")  # Organize and rename the future climate data, structuring it by year and GCM. # The 'SoilType' variable will be appended as a static variable in each scenario. # The files will be renamed following the \"bio_\" format organize_future_worldclim(input_dir = in_dir, output_dir = out_dir_future,                           name_format = \"bio_\", fixed_variables = var$SoilType) #>    |                                                                               |                                                                      |   0%   |                                                                               |=========                                                             |  12%   |                                                                               |==================                                                    |  25%   |                                                                               |==========================                                            |  38%   |                                                                               |===================================                                   |  50%   |                                                                               |============================================                          |  62%   |                                                                               |====================================================                  |  75%   |                                                                               |=============================================================         |  88%   |                                                                               |======================================================================| 100% #>  #> Variables successfully organized in directory: #> /tmp/Rtmp3eUeuI/Future_raw1  # Prepare projections pr <- prepare_projection(variable_names = c(\"bio_1\", \"bio_7\", \"bio_12\",                                             \"bio_15\", \"SoilType\"),                          future_dir = out_dir_future,                          future_period = c(\"2041-2060\", \"2081-2100\"),                          future_pscen = c(\"ssp126\", \"ssp585\"),                          future_gcm = c(\"ACCESS-CM2\", \"MIROC6\"),                          raster_pattern = \".tif*\")  # Create folder to save projection results out_dir <- file.path(tempdir(), \"PCA_projections\") dir.create(out_dir, recursive = TRUE)  # Perform and project PCA for new scenarios (future) proj_pca <- perform_pca(raster_variables = var, exclude_from_pca = \"SoilType\",                         project = TRUE, projection_data = pr,                         out_dir = out_dir, center = TRUE, scale = TRUE)  proj_pca$projection_directory  # Directory with projected PCA-variables #> [1] \"/tmp/Rtmp3eUeuI/PCA_projections\""},{"path":"https://marlonecobos.github.io/kuenm2/reference/plot_calibration_hist.html","id":null,"dir":"Reference","previous_headings":"","what":"Histograms to visualize data from explore_calibration objects — plot_calibration_hist","title":"Histograms to visualize data from explore_calibration objects — plot_calibration_hist","text":"Plots histograms visualize data explore_calibration object generated explore_calibration_hist function.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/plot_calibration_hist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Histograms to visualize data from explore_calibration objects — plot_calibration_hist","text":"","code":"plot_calibration_hist(explore_calibration, color_m = \"grey\",                       color_background = \"#56B4E9\",                       color_presence = \"#009E73\", alpha = 0.4,                       lines = FALSE, which_lines = c(\"cl\", \"mean\"),                       lty_range = 1, lty_cl = 2, lty_mean = 3,                       lwd_range = 3, lwd_cl = 2, lwd_mean = 2,                       xlab = NULL, ylab = NULL, mfrow = NULL)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/plot_calibration_hist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Histograms to visualize data from explore_calibration objects — plot_calibration_hist","text":"explore_calibration object class explore_calibration generated explore_calibration_hist function. color_m (character) color used fill histogram bars entire area (M). Default \"grey\". color_background (character) color used fill histogram bars background data. Default \"#56B4E9\". color_presence (character) color used fill histogram bars presence data. Default \"#009E73\". alpha (numeric) opacity factor fill bars, typically range 0-1. Default 0.4. lines (logical) whether add vertical lines plot representing range, confidence interval, mean variables. Default = FALSE. which_lines (character) vector indicating lines plot. Available options \"range\", \"cl\" (confidence interval), \"mean\". Default c(\"range\", \"cl\", \"mean\"). lty_range (numeric) line type plotting ranges variables. Default 1, meaning solid line. lty_cl (numeric) line type plotting confidence interval variables. Default 2, meaning dashed line. lty_mean (numeric) line type plotting mean variables. Default 3, meaning dotted line. lwd_range (numeric) line width line representing range. Default 3. lwd_cl (numeric) line width line representing confidence interval. Default 2. lwd_mean (numeric) line width line representing mean. Default 2. xlab (character) vector names labeling x-axis. must length number variables. Default NULL, meaning labels extracted explore_calibration object. ylab (character) label y-axis. Default NULL, meaning y-axis labeled \"Frequency\". mfrow (numeric) vector specifying number rows columns plot layout, e.g., c(rows, columns). Default NULL, meaning grid arranged automatically based number plots.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/plot_calibration_hist.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Histograms to visualize data from explore_calibration objects — plot_calibration_hist","text":"","code":"# Import raster layers var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  # Import occurrences data(sp_swd, package = \"kuenm2\")  # Explore calibration data calib_hist <- explore_calibration_hist(data = sp_swd,                                        raster_variables = var,                                        include_m = TRUE)  # Plot histograms plot_calibration_hist(explore_calibration = calib_hist)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/plot_explore_partition.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot extrapolation risks for partitions — plot_explore_partition","title":"Plot extrapolation risks for partitions — plot_explore_partition","text":"Visualize data explore_partition object generated explore_partition_extrapolation function.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/plot_explore_partition.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot extrapolation risks for partitions — plot_explore_partition","text":"","code":"plot_explore_partition(   explore_partition,   space = c(\"G\", \"E\"),   type_of_plot = c(\"distance\", \"simple\"),   variables = NULL,   calibration_area = NULL,   show_limits = TRUE,   include_background = FALSE,   distance_palette = NULL,   break_type = \"pretty\",   in_range_color = \"#009E73\",   out_range_color = \"#D55E00\",   calibration_area_col = \"gray90\",   pr_alpha = 1,   bg_alpha = 0.4,   pch_in_range = 21,   pch_out_range = 24,   cex_plot = 1.4,   size_text_legend = 1,   legend.margin = 0.4,   lwd_legend = 12,   ncols = NULL,   ... )"},{"path":"https://marlonecobos.github.io/kuenm2/reference/plot_explore_partition.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot extrapolation risks for partitions — plot_explore_partition","text":"explore_partition object class explore_partition returned explore_partition_extrapolation() function. space (character) vector specifying space plot. Available options  'G' geographical space E' environmental space. Default c(\"G\",\"E\"), meaning spaces plotted. type_of_plot (character) vector specifying type(s) plot. Options \"simple\", shows whether record partition within range partitions, \"distance\", shows Euclidean distance record set conditions partitions. Default c(\"simple\", \"distance\"), meaning plots produced. variables (character) pair variables used define axes environmental space. Default NULL, meaning first two continuous variables available explore_partition used define E space. calibration_area (SpatRaster, SpatVector, SpatExtent) spatial object representing calibration area. Preferably, one raster layers used variables prepare_data. Required type_of_plot = \"G\". default, NULL, uses basic Spatvector world. show_limits (logical) whether draw box representing lower upper limits variables, considering partitions (.e., Partition 1, box represents limits considering Partitions 2, 3, 4. applicable \"E\" included type_of_plot. Default TRUE. include_background (logical) whether plot background points together presence records. applicable explore_partition obtained using presence background points (.e., include_test_background = TRUE explore_partition_extrapolation). Default FALSE. distance_palette (character) vector valid colors used interpolate palette representing distance. Default NULL, meaning built-palette used (green lower distances red higher distances). applicable \"distance\" included type_of_plot. break_type (character) specifies method used define distance breaks. Options \"pretty\" \"quantile\". Default \"pretty\", uses pretty() function set breaks. applicable \"distance\" included type_of_plot. in_range_color (character) color used represent records fall within range partitions. Default \"#009E73\" (Seafoam Green). out_range_color (character) color used represent records fall outside range partitions. Default \"#D55E00\" (reddish-orange). calibration_area_col (character) color used represent calibration area. Default \"gray90\". pr_alpha (numeric) specifies transparency presence records. Default 1, meaning fully opaque. bg_alpha (numeric) specifies transparency background points. Default 0.4. applicable include_background set TRUE. pch_in_range (numeric) specifies symbol used points fall within range partitions. Default 21 (filled circle). See ?pch available options. pch_out_range (numeric) specifies symbol used points fall outside range partitions. Default 24 (filled triangle). See ?pch available options. cex_plot (numeric) specifies size points plot. Default 1.4 size_text_legend (numeric) specifies size text legend. Default  1. legend.margin (numeric) specifies height row layout contains legend. Default 0.4, meaning row 40% height rows layout. lwd_legend (numeric) specifies width legend bar representing distance. Default 12. Applicable \"distance\" included type_of_plot. ncols (numeric) specifies number columns plot layout. Default NULL, meaning number columns determined automatically based number partitions. ... additional arguments passed plot().","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/plot_explore_partition.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot extrapolation risks for partitions — plot_explore_partition","text":"","code":"# Load prepared_data with spatial blocks as the partitioning method (from ENMeval) data(swd_spatial_block, package = \"kuenm2\") # Analyze extrapolation risks across partitions res <- explore_partition_extrapolation(data = swd_spatial_block,                                        include_test_background = TRUE) # Plot partition distribution in Geographic Space (Distance and Simple MOP) plot_explore_partition(explore_partition = res, space = \"G\",                        variables = c(\"bio_7\", \"bio_15\"))    # Plot partition distribution in Environmental Space (Distance and Simple MOP) plot_explore_partition(explore_partition = res, space = \"E\",                        variables = c(\"bio_7\", \"bio_15\"))"},{"path":"https://marlonecobos.github.io/kuenm2/reference/plot_importance.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary plot for variable importance in models — plot_importance","title":"Summary plot for variable importance in models — plot_importance","text":"See details plot_importance","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/plot_importance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary plot for variable importance in models — plot_importance","text":"","code":"plot_importance(x, xlab = NULL, ylab = \"Relative contribution\",                 main = \"Variable importance\", extra_info = TRUE, ...)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/plot_importance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary plot for variable importance in models — plot_importance","text":"x data.frame output variable_importance(). xlab (character) label x axis. ylab (character) label y axis. main (character) main title plot. extra_info (logical) results one model, adds information number models using predictor mean contribution found. ... additional arguments passed barplot boxplot. Value barplot boxplot depending number models considered.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/predict.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict method for glmnet_mx (maxnet) models — predict","title":"Predict method for glmnet_mx (maxnet) models — predict","text":"Predict method glmnet_mx (maxnet) models","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/predict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict method for glmnet_mx (maxnet) models — predict","text":"","code":"predict.glmnet_mx(object, newdata, clamp = FALSE,                   type = c(\"link\", \"exponential\", \"cloglog\", \"logistic\",                   \"cumulative\"))"},{"path":"https://marlonecobos.github.io/kuenm2/reference/predict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict method for glmnet_mx (maxnet) models — predict","text":"object glmnet_mx object. newdata data predict . clamp (logical) whether clamp predictions. Default = FALSE. type (character) type prediction performed. Options : \"link\", \"exponential\", \"cloglog\", \"logistic\", cumulative. Defaults \"link\" defined.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/predict.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict method for glmnet_mx (maxnet) models — predict","text":"glmnet_mx (maxnet) prediction.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/predict_selected.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict selected models for a single scenario — predict_selected","title":"Predict selected models for a single scenario — predict_selected","text":"function predicts selected models single set new data using either maxnet glm provides options save output compute consensus results (mean, median, etc.) across replicates models.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/predict_selected.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict selected models for a single scenario — predict_selected","text":"","code":"predict_selected(models, new_variables, mask = NULL, write_files = FALSE,                  write_replicates = FALSE, out_dir = NULL,                  consensus_per_model = TRUE, consensus_general = TRUE,                  consensus = c(\"median\", \"range\", \"mean\", \"stdev\"),                  extrapolation_type = \"E\", var_to_clamp = NULL,                  type = \"cloglog\", overwrite = FALSE, progress_bar = TRUE)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/predict_selected.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict selected models for a single scenario — predict_selected","text":"models object class fitted_models returned fit_selected() function. new_variables SpatRaster data.frame predictor variables. names variables must match used calibrate models used run PCA do_pca = TRUE prepare_data() function. mask (SpatRaster, SpatVector, SpatExtent) spatial object used mask variables predict. Default NULL. write_files (logical) whether save predictions (SpatRasters data.frame) disk. Default FALSE. write_replicates (logical) whether save predictions replicates disk. applicable write_files TRUE. Default FALSE. out_dir (character) directory path predictions saved. relevant write_files = TRUE. consensus_per_model (logical) whether compute consensus (mean, median, etc.) model across replicates. Default TRUE. consensus_general (logical) whether compute general consensus across models. Default TRUE. consensus (character) vector specifying types consensus calculate across replicates models. Available options \"median\", \"range\", \"mean\", \"stdev\" (standard deviation). Default c(\"median\", \"range\", \"mean\", \"stdev\"). extrapolation_type (character) extrapolation type model. Models can transferred three options: free extrapolation ('E'), extrapolation clamping ('EC'), extrapolation ('NE'). Default = 'E'. See details. var_to_clamp (character) vector specifying variables clamp extrapolate. applicable extrapolation_type \"EC\" \"NE\". Default NULL, meaning variables clamped extrapolated. type (character) format prediction values. maxnet models, valid options \"raw\", \"cumulative\", \"logistic\", \"cloglog\". glm models, valid options \"cloglog\", \"response\", \"raw\", \"cumulative\" \"link\". Default \"cloglog. overwrite (logical) whether overwrite SpatRasters already exist. applicable write_files = TRUE. Default FALSE. progress_bar (logical) whether display progress bar processing. Default TRUE.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/predict_selected.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict selected models for a single scenario — predict_selected","text":"list containing SpatRaster data.frames predictions replicate, long consensus results model overall general consensus.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/predict_selected.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Predict selected models for a single scenario — predict_selected","text":"predicting areas variables beyond lower upper limits calibration data, users can choose free extrapolate predictions (extrapolation_type = \"E\"), extrapolate clamping (extrapolation_type = \"EC\"), extrapolate (extrapolation_type = \"NE\"). clamping, variables set minimum maximum values established maximum minimum values within calibration data. extrapolation approach, cell least one variable listed var_to_clamp falling outside calibration range assigned suitability value 0.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/predict_selected.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict selected models for a single scenario — predict_selected","text":"","code":"# Import variables to predict on var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  # Example with maxnet # Import example of fitted_models (output of fit_selected()) data(\"fitted_model_maxnet\", package = \"kuenm2\")  # Predict to single scenario p <- predict_selected(models = fitted_model_maxnet, new_variables = var) #>    |                                                                               |                                                                      |   0%   |                                                                               |===================================                                   |  50%   |                                                                               |======================================================================| 100%  # Example with GLMs # Import example of fitted_models (output of fit_selected()) without replicates data(\"fitted_model_glm\", package = \"kuenm2\")  # Predict to single scenario p_glm <- predict_selected(models = fitted_model_glm, new_variables = var) #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100%  # Plot predictions terra::plot(c(p$General_consensus$median, p_glm$General_consensus),             col = rev(terrain.colors(240)), main = c(\"MAXNET\", \"GLM\"),             zlim = c(0, 1))"},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare data for model calibration — prepare_data","title":"Prepare data for model calibration — prepare_data","text":"function prepares data model calibration, including optional PCA, background point generation, training/testing partitioning, creation grid parameter combinations, including regularization multiplier values, feature classes, sets environmental variables.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare data for model calibration — prepare_data","text":"","code":"prepare_data(algorithm, occ, x, y, raster_variables, species = NULL,              n_background = 1000, features = c(\"lq\", \"lqp\"),              r_multiplier = c(0.1, 0.5, 1, 2, 3),              user_formulas = NULL,              partition_method = \"kfolds\",              n_partitions = 4, train_proportion = 0.7,              categorical_variables = NULL,              do_pca = FALSE, center = TRUE, scale = TRUE,              exclude_from_pca = NULL, variance_explained = 95,              min_explained = 5, min_number = 2, min_continuous = NULL,              bias_file = NULL, bias_effect = NULL, weights = NULL,              include_xy = TRUE, write_pca = FALSE, pca_directory = NULL,              write_file = FALSE, file_name = NULL, seed = 1)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare data for model calibration — prepare_data","text":"algorithm (character) modeling algorithm, either \"glm\" \"maxnet\". occ (data frame) data.frame containing coordinates (longitude latitude) occurrence records. x (character) string specifying name column occ contains longitude values. y (character) string specifying name column occ contains latitude values. raster_variables (SpatRaster) predictor variables environmental values extracted using occ background sampled. Must correspond geographically area model calibrated. species (character) string specifying species name (optional). Default NULL. n_background (numeric) number points represent background model. Default 1000. features (character) vector feature classes. Default c(\"q\", \"lq\", \"lp\", \"qp\", \"lqp\"). r_multiplier (numeric) vector regularization parameters maxnet. Default c(0.1, 1, 2, 3, 5). user_formulas (character) Optional character vector custom formulas provided user. See Details. Default NULL. partition_method (character) method used data partitioning. Available options \"kfolds\", \"subsample\", \"bootstrap\". See Details information. Default = \"kfolds\". n_partitions (numeric) number partitions generate. partition_method \"subsample\" \"bootstrap\", defines number training testing replicates. \"kfolds\", specifies number folds. Must > 1; default = 4. train_proportion (numeric) proportion occurrence background points used model training partition. applicable partition_method \"subsample\" \"bootstrap\". Default 0.7 (.e., 70% training 30% testing). categorical_variables (character) names variables categorical. Default NULL. do_pca (logical) whether perform principal component analysis (PCA) set variables. Default FALSE. center (logical) whether variables zero-centered. Default TRUE. scale (logical) whether variables scaled unit variance analysis takes place. Default FALSE. exclude_from_pca (character) variable names within raster_variables included PCA transformation. Instead, variables added directly final set output variables without modified. default NULL, meaning variables used unless specified otherwise. variance_explained (numeric) cumulative percentage total variance must explained selected principal components. Default 95. min_explained (numeric) minimum percentage total variance principal component must explain retained. Default 5. min_number (numeric) minimum number variables included model formulas generated. Default = 2. min_continuous (numeric) minimum number continuous variables required combination. Default NULL. bias_file (SpatRaster) raster containing bias values (probability weights) influence selection background points. must extent, resolution, number cells raster variables. Default NULL. bias_effect (character) string specifying values bias_file interpreted. Options \"direct\" \"inverse\". \"direct\", higher values bias file increase likelihood selecting background points. \"inverse\", higher values decrease likelihood. Default = NULL. Must defined bias_file provided. weights (numeric) numeric vector specifying weights occurrence records. default, NULL, uses 1 presence 100 background. include_xy (logical) whether include coordinates (longitude latitude) results preparing data. Columns containing coordinates renamed \"x\" \"y\". Default TRUE. write_pca (logical) whether save PCA-derived raster layers (principal components) disk. Default FALSE. pca_directory (character) path name folder PC raster layers saved. applicable write_pca = TRUE. Default NULL. write_file (logical) whether write resulting prepared_data list local directory. Default FALSE. file_name (character) name file (extension needed) write resulting object local directory. needed write_file = TRUE. Default NULL. seed (numeric) integer value specify initial seed split data extract background. Default 1.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare data for model calibration — prepare_data","text":"object class prepared_data containing elements necessary perform explorations data run model calibration routine.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Prepare data for model calibration — prepare_data","text":"Training testing performed multiple times (.e., number set n_partitions), model selection based average performance models running routine. description available data partitioning methods : \"kfolds\": Splits dataset K subsets (folds) approximately equal size, keeping proportion 0 1 stable compared full set. training/test run, one fold used test set, remaining folds combined form training set. \"bootstrap\": Creates training dataset sampling observations original dataset replacement (.e., observation can selected multiple times). test set consists observations selected specific sampling. \"subsample\": Similar bootstrap, training set created sampling without replacement (.e., observation selected ). user_formulas must character vector model formulas. Supported terms include linear effects, quadratic terms (e.g., (bio_7^2)), products (e.g., bio_1:bio_7), hinge (e.g., hinge(bio_1)), threshold (e.g., thresholds(bio_2)), categorical predictors (e.g., categorical(SoilType)). Example valid formula: ~ bio_1 + bio_7 + (bio_7^2) + bio_1:bio_7 + hinge(bio_1) + thresholds(bio_2) + categorical(SoilType). variables appearing formulas must exist raster supplied raster_variables.","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare data for model calibration — prepare_data","text":"","code":"# Import occurrences data(occ_data, package = \"kuenm2\")  # Import raster layers var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  # Import a bias file bias <- terra::rast(system.file(\"extdata\", \"bias_file.tif\",                                 package = \"kuenm2\"))  # Prepare data for maxnet model sp_swd <- prepare_data(algorithm = \"maxnet\", occ = occ_data,                        x = \"x\", y = \"y\",                        raster_variables = var,                        species = occ_data[1, 1],                        categorical_variables = \"SoilType\",                        n_background = 500, bias_file = bias,                        bias_effect = \"direct\",                        features = c(\"l\", \"q\", \"p\", \"lq\", \"lqp\"),                        r_multiplier = c(0.1, 1, 2, 3, 5)) #> Warning: 27 rows were excluded from database because NAs were found. print(sp_swd) #> prepared_data object summary #> ============================ #> Species: Myrcia hatschbachii  #> Number of Records: 524  #>   - Presence: 51  #>   - Background: 473  #> Partition Method: kfolds  #>   - Number of kfolds: 4  #> Continuous Variables: #>   - bio_1, bio_7, bio_12, bio_15  #> Categorical Variables: #>   - SoilType  #> PCA Information: PCA not performed #> Weights: No weights provided #> Calibration Parameters: #>   - Algorithm: maxnet  #>   - Number of candidate models: 610  #>   - Features classes (responses): l, q, p, lq, lqp  #>   - Regularization multipliers: 0.1, 1, 2, 3, 5   # Prepare data for glm model sp_swd_glm <- prepare_data(algorithm = \"glm\", occ = occ_data,                            x = \"x\", y = \"y\",                            raster_variables = var,                            species = occ_data[1, 1],                            categorical_variables = \"SoilType\",                            n_background = 500, bias_file = bias,                            bias_effect = \"direct\",                            features = c(\"l\", \"q\", \"p\", \"lq\", \"lqp\")) #> Warning: 27 rows were excluded from database because NAs were found. print(sp_swd_glm) #> prepared_data object summary #> ============================ #> Species: Myrcia hatschbachii  #> Number of Records: 524  #>   - Presence: 51  #>   - Background: 473  #> Partition Method: kfolds  #>   - Number of kfolds: 4  #> Continuous Variables: #>   - bio_1, bio_7, bio_12, bio_15  #> Categorical Variables: #>   - SoilType  #> PCA Information: PCA not performed #> Weights: No weights provided #> Calibration Parameters: #>   - Algorithm: glm  #>   - Number of candidate models: 122  #>   - Features classes (responses): l, q, p, lq, lqp"},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_projection.html","id":null,"dir":"Reference","previous_headings":"","what":"Preparation of data for model projections — prepare_projection","title":"Preparation of data for model projections — prepare_projection","text":"function prepared data model projections multiple scenarios, storing paths rasters representing scenario.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_projection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preparation of data for model projections — prepare_projection","text":"","code":"prepare_projection(models = NULL, variable_names = NULL, present_dir = NULL,                    past_dir = NULL, past_period = NULL, past_gcm = NULL,                    future_dir = NULL, future_period = NULL,                    future_pscen = NULL, future_gcm = NULL,                    write_file = FALSE, filename = NULL,                    raster_pattern = \".tif*\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_projection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preparation of data for model projections — prepare_projection","text":"models object class fitted_models returned fit_selected() function. Default NULL. variable_names (character) names variables used fit model PCA prepare_data() function. applicable models argument provided. Default NULL. present_dir (character) path folder containing variables represent current scenario projection. Default NULL. past_dir (character) path folder containing subfolders v ariables representing past scenarios projection. Default NULL. past_period (character) names subfolders within past_dir, representing specific time periods (e.g., 'LGM' 'MID'). past_gcm (character) names subfolders within past_period folders, representing specific General Circulation Models (GCMs). future_dir (character) path folder containing subfolders variables representing future scenarios projection. Default NULL. future_period (character) names subfolders within future_dir, representing specific time periods (e.g., '2041-2060' '2081-2100'). Default NULL. future_pscen (character) names subfolders within future_period, representing specific emission scenarios (e.g., 'ssp126' 'ssp585'). Default NULL. future_gcm (character) names subfolders within future_pscen folders, representing specific General Circulation Models (GCMs). Default NULL. write_file (logical) whether write object containing paths structured folders. object required projecting models across multiple scenarios using project_selected() function. Default FALSE. filename (character) path name folder object saved. applicable write_file = TRUE. Default NULL. raster_pattern (character) pattern used identify format raster files within folders. Default \".tif*\".","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_projection.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preparation of data for model projections — prepare_projection","text":"object class prepared_projection containing following elements: Present, Past, Future: paths variables structured subfolders. Raster_pattern: pattern used identify format raster files within folders. PCA: principal component analysis (PCA) performed set variables prepare_data(), list class \"prcomp\" returned. See ?stats::prcomp() details. variables: names raw predictos variables used project.","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_projection.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preparation of data for model projections — prepare_projection","text":"","code":"# Import example of fitted_models (output of fit_selected()) data(\"fitted_model_maxnet\", package = \"kuenm2\")  # Organize and structure future climate variables from WorldClim # Import the current variables used to fit the model. # In this case, SoilType will be treated as a static variable (constant # across future scenarios). var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  # Create a \"Current_raw\" folder in a temporary directory and copy the raw # variables there. out_dir_current <- file.path(tempdir(), \"Current_raw\") dir.create(out_dir_current, recursive = TRUE)  # Save current variables in temporary directory terra::writeRaster(var, file.path(out_dir_current, \"Variables.tif\"))  # Set the input directory containing the raw future climate variables. # For this example, the data is located in the \"inst/extdata\" folder. in_dir <- system.file(\"extdata\", package = \"kuenm2\")  # Create a \"Future_raw\" folder in a temporary directory and copy the raw # variables there. out_dir_future <- file.path(tempdir(), \"Future_raw\")  # Organize and rename the future climate data, structuring it by year and GCM. # The 'SoilType' variable will be appended as a static variable in each scenario. # The files will be renamed following the \"bio_\" format organize_future_worldclim(input_dir = in_dir,                           output_dir = out_dir_future,                           name_format = \"bio_\", variables = NULL,                           fixed_variables = var$SoilType, mask = NULL,                           overwrite = TRUE) #>    |                                                                               |                                                                      |   0%   |                                                                               |=========                                                             |  12%   |                                                                               |==================                                                    |  25%   |                                                                               |==========================                                            |  38%   |                                                                               |===================================                                   |  50%   |                                                                               |============================================                          |  62%   |                                                                               |====================================================                  |  75%   |                                                                               |=============================================================         |  88%   |                                                                               |======================================================================| 100% #>  #> Variables successfully organized in directory: #> /tmp/Rtmp3eUeuI/Future_raw  # Prepare projections using fitted models to check variables pr <- prepare_projection(models = fitted_model_maxnet,                          present_dir = out_dir_current,                          past_dir = NULL,                          past_period = NULL,                          past_gcm = NULL,                          future_dir = out_dir_future,                          future_period = c(\"2041-2060\", \"2081-2100\"),                          future_pscen = c(\"ssp126\", \"ssp585\"),                          future_gcm = c(\"ACCESS-CM2\", \"MIROC6\"),                          write_file = FALSE,                          filename = NULL,                          raster_pattern = \".tif*\") pr #> projection_data object summary #> ============================= #> Variables prepared to project models for Present and Future  #> Future projections contain the following periods, scenarios and GCMs: #>   - Periods: 2041-2060 | 2081-2100  #>   - Scenarios: ssp126 | ssp585  #>   - GCMs: ACCESS-CM2 | MIROC6  #> All variables are located in the following root directory: #> /tmp/Rtmp3eUeuI  # Prepare projections using variables names pr_b <- prepare_projection(models = NULL,                            variable_names = c(\"bio_1\", \"bio_7\", \"bio_12\"),                            present_dir = out_dir_current,                            past_dir = NULL,                            past_period = NULL,                            past_gcm = NULL,                            future_dir = out_dir_future,                            future_period = c(\"2041-2060\", \"2081-2100\"),                            future_pscen = c(\"ssp126\", \"ssp585\"),                            future_gcm = c(\"ACCESS-CM2\", \"MIROC6\"),                            write_file = FALSE,                            filename = NULL,                            raster_pattern = \".tif*\") pr_b #> projection_data object summary #> ============================= #> Variables prepared to project models for Present and Future  #> Future projections contain the following periods, scenarios and GCMs: #>   - Periods: 2041-2060 | 2081-2100  #>   - Scenarios: ssp126 | ssp585  #>   - GCMs: ACCESS-CM2 | MIROC6  #> All variables are located in the following root directory: #> /tmp/Rtmp3eUeuI"},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_user_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare data for model calibration with user-prepared calibration data — prepare_user_data","title":"Prepare data for model calibration with user-prepared calibration data — prepare_user_data","text":"function prepares data model calibration using user-prepared calibration data. includes optional PCA, training/testing partitioning, creation grid parameter combinations, including distinct regularization multiplier values, various feature classes, different sets environmental variables.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_user_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare data for model calibration with user-prepared calibration data — prepare_user_data","text":"","code":"prepare_user_data(algorithm, user_data, pr_bg, species = NULL, x = NULL,                   y = NULL, features = c(\"lq\", \"lqp\"),                   r_multiplier = c(0.1, 0.5, 1, 2, 3),                   user_formulas = NULL,                   partition_method = \"kfolds\", n_partitions = 4,                   train_proportion = 0.7, user_part = NULL,                   categorical_variables = NULL,                   do_pca = FALSE, center = TRUE, scale = TRUE,                   exclude_from_pca = NULL, variance_explained = 95,                   min_explained = 5, min_number = 2, min_continuous = NULL,                   weights = NULL, include_xy = TRUE, write_pca = FALSE,                   pca_directory = NULL, write_file = FALSE, file_name = NULL,                   seed = 1)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_user_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare data for model calibration with user-prepared calibration data — prepare_user_data","text":"algorithm (character) modeling algorithm, either \"glm\" \"maxnet\". user_data (data frame) data.frame column presence (1) background (0) records, together variable values (one variable per column). See example data(\"user_data\", package = \"kuenm2\"). pr_bg (character) name column user_data contains presence/background records. species (character) string specifying species name (optional). Default NULL. x (character) string specifying name column user_data contains longitude values. Default NULL. Must defined present user_data otherwise considered another predictor variable. y (character) string specifying name column user_data contains latitude values. Default NULL. Must defined present user_data otherwise considered another predictor variable. features (character) vector feature classes. Default c(\"q\", \"lq\", \"lp\", \"qp\", \"lqp\"). r_multiplier (numeric) vector regularization parameters maxnet. Default c(0.1, 1, 2, 3, 5). user_formulas (character) Optional character vector custom formulas provided user. See Details. Default NULL. partition_method (character) method used data partitioning. Available options \"kfolds\", \"subsample\", \"bootstrap\". See Details information. Default = \"kfolds\". n_partitions (numeric) number partitions generate. partition_method \"subsample\" \"bootstrap\", defines number training testing replicates. \"kfolds\", specifies number folds. Must > 1; default = 4. train_proportion (numeric) proportion occurrence background points used model training partition. applicable partition_method \"subsample\" \"bootstrap\". Default 0.7 (.e., 70% training 30% testing). user_part user provided list partitions folds cross-validation used model calibration. element list contain vector indices indicating test points, used split user_data training testing sets. Useful experiments require exactly partition sets. categorical_variables (character) names variables categorical. Default NULL. do_pca (logical) whether perform principal component analysis (PCA) set variables. Default FALSE. center (logical) whether variables zero-centered. Default TRUE. scale (logical) whether variables scaled unit variance analysis takes place. Default FALSE. exclude_from_pca (character) variable names within raster_variables included PCA transformation. Instead, variables added directly final set output variables without modified. default NULL, meaning variables used unless specified otherwise. variance_explained (numeric) cumulative percentage total variance must explained selected principal components. Default 95. min_explained (numeric) minimum percentage total variance principal component must explain retained. Default 5. min_number (numeric) minimum number variables included model formulas generated. min_continuous (numeric) minimum number continuous variables required combination. Default NULL. weights (numeric) numeric vector specifying weights occurrence records. Default NULL. include_xy (logical) whether include coordinates (longitude latitude) results preparing data. Default TRUE. write_pca (logical) whether save PCA-derived raster layers (principal components) disk. Default FALSE. pca_directory (character) path name folder PC raster layers saved. applicable write_pca = TRUE. Default NULL. write_file (logical) whether write resulting prepared_data list local directory. Default FALSE. file_name (character) path name folder resulting list saved. applicable write_file = TRUE. Default NULL. seed (numeric) integer value specify initial seed split data. Default 1.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_user_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare data for model calibration with user-prepared calibration data — prepare_user_data","text":"object class prepared_data containing elements necessary perform explorations data run model calibration routine.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_user_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Prepare data for model calibration with user-prepared calibration data — prepare_user_data","text":"Training testing performed multiple times (.e., number set n_partitions), model selection based average performance models running routine. description available data partitioning methods : \"kfolds\": Splits dataset K subsets (folds) approximately equal size, keeping proportion 0 1 stable compared full set. training/test run, one fold used test set, remaining folds combined form training set. \"bootstrap\": Creates training dataset sampling observations original dataset replacement (.e., observation can selected multiple times). test set consists observations selected specific sampling. \"subsample\": Similar bootstrap, training set created sampling without replacement (.e., observation selected ). user_formulas must character vector model formulas. Supported terms include linear effects, quadratic terms (e.g., (bio_7^2)), products (e.g., bio_1:bio_7), hinge (e.g., hinge(bio_1)), threshold (e.g., thresholds(bio_2)), categorical predictors (e.g., categorical(SoilType)). Example valid formula: ~ bio_1 + bio_7 + (bio_7^2) + bio_1:bio_7 + hinge(bio_1) + thresholds(bio_2) + categorical(SoilType). variables appearing formulas must exist data.frame supplied user_data.","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/prepare_user_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare data for model calibration with user-prepared calibration data — prepare_user_data","text":"","code":"# Import user-prepared data data(\"user_data\", package = \"kuenm2\")  # Prepare data for maxnet model maxnet_swd_user <- prepare_user_data(algorithm = \"maxnet\",                                      user_data = user_data, pr_bg = \"pr_bg\",                                      species = \"Myrcia hatschbachii\",                                      categorical_variables = \"SoilType\",                                      features = c(\"l\", \"q\", \"p\", \"lq\", \"lqp\"),                                      r_multiplier = c(0.1, 1, 2, 3, 5)) maxnet_swd_user #> prepared_data object summary #> ============================ #> Species: Myrcia hatschbachii  #> Number of Records: 527  #>   - Presence: 51  #>   - Background: 476  #> Partition Method: kfolds  #>   - Number of kfolds: 4  #> Continuous Variables: #>   - bio_1, bio_7, bio_12, bio_15  #> Categorical Variables: #>   - SoilType  #> PCA Information: PCA not performed #> Weights: No weights provided #> Calibration Parameters: #>   - Algorithm: maxnet  #>   - Number of candidate models: 610  #>   - Features classes (responses): l, q, p, lq, lqp  #>   - Regularization multipliers: 0.1, 1, 2, 3, 5   # Prepare data for glm model glm_swd_user <- prepare_user_data(algorithm = \"glm\",                                   user_data = user_data, pr_bg = \"pr_bg\",                                   species = \"Myrcia hatschbachii\",                                   categorical_variables = \"SoilType\",                                   features = c(\"l\", \"q\", \"p\", \"lq\", \"lqp\")) glm_swd_user #> prepared_data object summary #> ============================ #> Species: Myrcia hatschbachii  #> Number of Records: 527  #>   - Presence: 51  #>   - Background: 476  #> Partition Method: kfolds  #>   - Number of kfolds: 4  #> Continuous Variables: #>   - bio_1, bio_7, bio_12, bio_15  #> Categorical Variables: #>   - SoilType  #> PCA Information: PCA not performed #> Weights: No weights provided #> Calibration Parameters: #>   - Algorithm: glm  #>   - Number of candidate models: 122  #>   - Features classes (responses): l, q, p, lq, lqp"},{"path":"https://marlonecobos.github.io/kuenm2/reference/print.html","id":null,"dir":"Reference","previous_headings":"","what":"Print method for kuenm2 objects — print","title":"Print method for kuenm2 objects — print","text":"Print method kuenm2 objects","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/print.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print method for kuenm2 objects — print","text":"","code":"# S3 method for class 'prepared_data' print(x, ...)  # S3 method for class 'calibration_results' print(x, ...)  # S3 method for class 'fitted_models' print(x, ...)  # S3 method for class 'projection_data' print(x, ...)  # S3 method for class 'model_projections' print(x, ...)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/print.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print method for kuenm2 objects — print","text":"x object classes: prepared_data, calibration_results, fitted_models, projection_data, model_projections. ... additional arguments affecting summary produced. Ignored functions.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/print.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print method for kuenm2 objects — print","text":"printed version object summarizes main elements contained.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/project_selected.html","id":null,"dir":"Reference","previous_headings":"","what":"Project selected models to multiple sets of new data (scenarios) — project_selected","title":"Project selected models to multiple sets of new data (scenarios) — project_selected","text":"function performs predictions selected models multiple scenarios, specified projection_data object created prepare_projection() function. addition generating predictions replicate, function calculates consensus measures (e.g., mean, median) across replicates models.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/project_selected.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Project selected models to multiple sets of new data (scenarios) — project_selected","text":"","code":"project_selected(models, projection_data, out_dir, mask = NULL,                  consensus_per_model = TRUE, consensus_general = TRUE,                  consensus = c(\"median\", \"range\", \"mean\", \"stdev\"),                  write_replicates = FALSE, extrapolation_type = \"E\",                  var_to_clamp = NULL, type = NULL, overwrite = FALSE,                  parallel = FALSE, ncores = NULL,                  progress_bar = TRUE, verbose = TRUE)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/project_selected.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Project selected models to multiple sets of new data (scenarios) — project_selected","text":"models object class fitted_models returned fit_selected() function. projection_data object class projection_data returned prepare_projection() function. file contains paths rasters representing scenario. out_dir (character) path root directory saving raster file projection. mask (SpatRaster, SpatVector, SpatExtent) spatial object used mask variables predict. Default NULL. consensus_per_model (logical) whether calculate consensus across replicates one replicate per model. Default TRUE. consensus_general (logical) whether calculate consensus across models one selected model. Default TRUE. consensus (character) consensus measures calculate. Options available 'median', 'range', 'mean' 'stdev' (standard deviation). Default c(\"median\", \"range\", \"mean\", \"stdev\"). write_replicates (logical) whether write projections replicate. Default FALSE. extrapolation_type (character) extrapolation type model. Models can transferred three options: free extrapolation ('E'), extrapolation clamping ('EC'), extrapolation ('NE'). Default = 'E'. See details. var_to_clamp (character) vector specifying variables clamp. applicable extrapolation_type \"EC\" \"NE\". Default NULL, meaning variables clamped extrapolated. type (character) format prediction values. maxnet models, valid options \"raw\", \"cumulative\", \"logistic\", \"cloglog\". glm models, valid options \"response\" \"raw\". NULL (default), function uses \"cloglog\" maxnet models \"response\" glm models. overwrite (logical) whether overwrite SpatRaster already exists. applicable write_files set TRUE. Default FALSE. parallel (logical) whether fit candidate models parallel. Default FALSE. ncores (numeric) number cores use parallel processing. Default NULL uses available cores - 1. applicable parallel = TRUE. progress_bar (logical) whether display progress bar processing. Default TRUE. verbose (logical) whether display messages processing. Default TRUE.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/project_selected.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Project selected models to multiple sets of new data (scenarios) — project_selected","text":"model_projections object provides paths raster files projection results corresponding thresholds used binarize predictions.","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/project_selected.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Project selected models to multiple sets of new data (scenarios) — project_selected","text":"","code":"# Step 1: Organize variables for current projection ## Import current variables (used to fit models) var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  ## Create a folder in a temporary directory to copy the variables out_dir_current <- file.path(tempdir(), \"Current_raw_wc\") dir.create(out_dir_current, recursive = TRUE)  ## Save current variables in temporary directory terra::writeRaster(var, file.path(out_dir_current, \"Variables.tif\"))   # Step 2: Organize future climate variables (example with WorldClim) ## Directory containing the downloaded future climate variables (example) in_dir <- system.file(\"extdata\", package = \"kuenm2\")  ## Create a folder in a temporary directory to copy the future variables out_dir_future <- file.path(tempdir(), \"Future_raw_wc\")  ## Organize and rename the future climate data (structured by year and GCM) ### 'SoilType' will be appended as a static variable in each scenario organize_future_worldclim(input_dir = in_dir, output_dir = out_dir_future,                           name_format = \"bio_\", fixed_variables = var$SoilType) #>    |                                                                               |                                                                      |   0%   |                                                                               |=========                                                             |  12%   |                                                                               |==================                                                    |  25%   |                                                                               |==========================                                            |  38%   |                                                                               |===================================                                   |  50%   |                                                                               |============================================                          |  62%   |                                                                               |====================================================                  |  75%   |                                                                               |=============================================================         |  88%   |                                                                               |======================================================================| 100% #>  #> Variables successfully organized in directory: #> /tmp/Rtmp3eUeuI/Future_raw_wc  # Step 3: Prepare data to run multiple projections ## An example with maxnet models ## Import example of fitted_models (output of fit_selected()) data(fitted_model_maxnet, package = \"kuenm2\")  ## Prepare projection data using fitted models to check variables pr <- prepare_projection(models = fitted_model_maxnet,                          present_dir = out_dir_current,                          future_dir = out_dir_future,                          future_period = c(\"2081-2100\"),                          future_pscen = c(\"ssp126\", \"ssp585\"),                          future_gcm = c(\"ACCESS-CM2\", \"MIROC6\"),                          raster_pattern = \".tif*\")  # Step 4: Run multiple model projections ## A folder to save projection results out_dir <- file.path(tempdir(), \"Projection_results/maxnet_projections\") dir.create(out_dir, recursive = TRUE)  ## Project selected models to multiple scenarios p <- project_selected(models = fitted_model_maxnet, projection_data = pr,                       out_dir = out_dir) #>    |                                                                               |                                                                      |   0%   |                                                                               |==============                                                        |  20%   |                                                                               |============================                                          |  40%   |                                                                               |==========================================                            |  60%   |                                                                               |========================================================              |  80%   |                                                                               |======================================================================| 100%"},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_changes.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute changes of suitable areas between scenarios — projection_changes","title":"Compute changes of suitable areas between scenarios — projection_changes","text":"function performs map algebra operations represent suitable areas change compared scenario model trained. Changes identified loss (contraction), gain (expansion) stability. multiple climate models (GCM) used, calculates level agreement among emission scenario.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_changes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute changes of suitable areas between scenarios — projection_changes","text":"","code":"projection_changes(model_projections, reference_id = 1, consensus = \"median\",                    include_id = NULL, user_threshold = NULL, by_gcm = TRUE,                    by_change = TRUE, general_summary = TRUE,                    force_resample = TRUE, write_results = TRUE,                    output_dir = NULL, overwrite = FALSE,                    write_bin_models = FALSE, return_raster = FALSE)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_changes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute changes of suitable areas between scenarios — projection_changes","text":"model_projections model_projections object generated project_selected() function. object contains file paths raster projection results thresholds used binarization predictions. reference_id (numeric) reference ID projections corresponding current time model_projections. Default 1. See details section information. consensus (character) consensus measure use calculating changes. Available options 'mean', 'median', 'range', 'stdev' (standard deviation). Default 'median'. include_id (numeric) vector containing reference IDs include computing changes. Default NULL, meaning projections included. See details section information. user_threshold (numeric) optional threshold binarizing predictions. Default NULL, meaning function apply thresholds stored model_projections, calculated earlier using omission rate calibration(). by_gcm (logical) whether compute changes across GCMs. Default TRUE. by_change (logical) whether compute results separately change, identifying areas gain, loss, stability GCM. Default TRUE. general_summary (logical) whether generate general summary, mapping many GCMs project gain, loss, stability scenario. Default TRUE. force_resample (logical) whether force projection rasters extent resolution raster corresponding reference_id, represents current projections. Default TRUE. write_results (logical) whether write raster files containing computed changes disk. Default TRUE. output_dir (character) directory path resulting raster files containing computed changes saved. relevant write_results = TRUE. overwrite (logical) whether overwrite SpatRaster already exist. applicable write_results set TRUE. Default FALSE. write_bin_models (logical) whether write binarized models GCM disk. Default FALSE. return_raster (logical) whether return list containing SpatRasters computed changes. Default FALSE, meaning function return NULL object. Setting argument TRUE using multiple GCMs large extent fine resolution may overload RAM.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_changes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute changes of suitable areas between scenarios — projection_changes","text":"changes_projections object. return_raster = TRUE, function returns list containing SpatRasters computed changes. list includes following elements: Binarized: binarized models GCM. Results_by_gcm: computed changes GCM. Results_by_change: list SpatRaster represents specific change. Summary_changes: general summary indicates many GCMs project gain, loss, stability scenario root_directory: path directory results saved write_results set TRUE return_raster = FALSE, function returns NULL object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_changes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute changes of suitable areas between scenarios — projection_changes","text":"projecting niche model different temporal scenarios (past future), species’ areas can classified three categories relative current baseline: gain, loss stability. interpretation categories depends temporal direction projection. projecting future scenarios: Gain: Areas currently unsuitable become suitable future. Loss: Areas currently suitable become unsuitable future. Stability: Areas retain current classification future, whether suitable unsuitable. projecting past scenarios: Gain: Areas unsuitable past now suitable present. Loss: Areas suitable past now unsuitable present. Stability: Areas retain past classification present, whether suitable unsuitable. reference scenario (current conditions) can accessed paths element model_projections object (model_projections$path). ID differ 1 one projection current conditions. Specific projections can included excluded analysis using include_id argument. example, setting 'include_id = c(3, 5, 7)' compute changes scenarios 3, 5, 7. Conversely, setting 'include_id = -c(3, 5, 7)' exclude scenarios 3, 5, 7 analysis.","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_changes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute changes of suitable areas between scenarios — projection_changes","text":"","code":"# Step 1: Organize variables for current projection ## Import current variables (used to fit models) var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  ## Create a folder in a temporary directory to copy the variables out_dir_current <- file.path(tempdir(), \"Current_raw3\") dir.create(out_dir_current, recursive = TRUE)  ## Save current variables in temporary directory terra::writeRaster(var, file.path(out_dir_current, \"Variables.tif\"))   # Step 2: Organize future climate variables (example with WorldClim) ## Directory containing the downloaded future climate variables (example) in_dir <- system.file(\"extdata\", package = \"kuenm2\")  ## Create a folder in a temporary directory to copy the future variables out_dir_future <- file.path(tempdir(), \"Future_raw3\")  ## Organize and rename the future climate data (structured by year and GCM) ### 'SoilType' will be appended as a static variable in each scenario organize_future_worldclim(input_dir = in_dir, output_dir = out_dir_future,                           name_format = \"bio_\", fixed_variables = var$SoilType) #>    |                                                                               |                                                                      |   0%   |                                                                               |=========                                                             |  12%   |                                                                               |==================                                                    |  25%   |                                                                               |==========================                                            |  38%   |                                                                               |===================================                                   |  50%   |                                                                               |============================================                          |  62%   |                                                                               |====================================================                  |  75%   |                                                                               |=============================================================         |  88%   |                                                                               |======================================================================| 100% #>  #> Variables successfully organized in directory: #> /tmp/Rtmp3eUeuI/Future_raw3  # Step 3: Prepare data to run multiple projections ## An example with maxnet models ## Import example of fitted_models (output of fit_selected()) data(fitted_model_maxnet, package = \"kuenm2\")  ## Prepare projection data using fitted models to check variables pr <- prepare_projection(models = fitted_model_maxnet,                          present_dir = out_dir_current,                          future_dir = out_dir_future,                          future_period = c(\"2081-2100\"),                          future_pscen = c(\"ssp585\"),                          future_gcm = c(\"ACCESS-CM2\", \"MIROC6\"),                          raster_pattern = \".tif*\")  # Step 4: Run multiple model projections ## A folder to save projection results out_dir <- file.path(tempdir(), \"Projection_results/maxnet1\") dir.create(out_dir, recursive = TRUE)  ## Project selected models to multiple scenarios p <- project_selected(models = fitted_model_maxnet, projection_data = pr,                       out_dir = out_dir) #>    |                                                                               |                                                                      |   0%   |                                                                               |=======================                                               |  33%   |                                                                               |===============================================                       |  67%   |                                                                               |======================================================================| 100%  # Step 5: Identify areas of change in projections ## Contraction, expansion and stability changes <- projection_changes(model_projections = p, write_results = FALSE,                               return_raster = TRUE)  terra::plot(changes$Binarized)  # SpatRaster with the binarized predictions  terra::plot(changes$Results_by_gcm)  # SpatRaster with changes by GCM  changes$Results_by_change  # List of SpatRaster(s) by changes with GCM agreement #> $`Future_2081-2100_ssp585` #> class       : SpatRaster  #> size        : 52, 40, 3  (nrow, ncol, nlyr) #> resolution  : 0.1666667, 0.1666667  (x, y) #> extent      : -53.5, -46.83333, -30.83333, -22.16667  (xmin, xmax, ymin, ymax) #> coord. ref. : lon/lat WGS 84 (EPSG:4326)  #> source(s)   : memory #> names       :           Stable unsuitable,           Loss,           Stable suitable  #> min values  : Stable unsuitable in 0 GCMs, Loss in 0 GCMs, Stable suitable in 0 GCMs  #> max values  : Stable unsuitable in 2 GCMs, Loss in 2 GCMs, Stable suitable in 2 GCMs  #>  terra::plot(changes$Results_by_change$`Future_2081-2100_ssp585`)  # an example of the previous  terra::plot(changes$Summary_changes)  # SpatRaster with a general summary"},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_mop.html","id":null,"dir":"Reference","previous_headings":"","what":"Analysis of extrapolation risks in projections using the MOP metric — projection_mop","title":"Analysis of extrapolation risks in projections using the MOP metric — projection_mop","text":"Calculates mobility-oriented parity metric sub-products represent dissimilarities non-analogous conditions comparing set reference conditions (M) model projection conditions (G).","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_mop.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Analysis of extrapolation risks in projections using the MOP metric — projection_mop","text":"","code":"projection_mop(data, projection_data, out_dir,                subset_variables = FALSE, mask = NULL, type = \"basic\",                na_in_range = TRUE, calculate_distance = FALSE,                where_distance = \"in_range\", distance = \"euclidean\",                scale = FALSE, center = FALSE, fix_NA = TRUE, percentage = 1,                comp_each = 2000, tol = NULL, rescale_distance = FALSE,                parallel = FALSE, ncores = NULL, progress_bar = TRUE,                overwrite = FALSE)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_mop.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Analysis of extrapolation risks in projections using the MOP metric — projection_mop","text":"data object class fitted_models returned fit_selected() function object class prepared_data returned prepare_data() function. projection_data object class projection_data returned prepare_projection()function. file contains paths rasters representing scenario. out_dir (character) path root directory saving raster file projection. subset_variables (logical) whether include analysis variables present selected models. Default FALSE mask (SpatRaster, SpatVector, SpatExtent) spatial object used mask variables (optional). Default NULL. type (character) type MOP analysis performed. Options available \"basic\", \"simple\" \"detailed\". See Details information. na_in_range (logical) whether assign NA regions within projected area (G) environmental conditions fall within range calibration data (M). TRUE (default), regions assigned NA. FALSE, assigned 0 simple basic MOP outputs, \"within ranges\" detailed MOP output. calculate_distance (logical) whether calculate distances (dissimilarities) m g. default, FALSE, runs rapidly assess dissimilarity levels. where_distance (character) calculate distances, considering conditions g positioned comparison range conditions m. Options available \"in_range\", \"out_range\" \"\". Default \"in_range\". distance (character) distances calculated, euclidean mahalanobis. applicable calculate_distance = TRUE. scale (logical numeric) whether scale scale. Default FALSE. center (logical numeric) whether center scale. Default FALSE. fix_NA (logical) whether fix layers cells NA values layers. Setting FALSE may save time rasters big NA matching problems. Default TRUE. percentage (numeric) percentage m closest conditions used derive mean environmental distances combination conditions g. comp_each (numeric) number combinations g used distance calculations time. Increasing number requires RAM tol (numeric) tolerance detect linear dependencies calculating Mahalanobis distances. default, NULL, uses .Machine$double.eps. rescale_distance (logical) whether re-scale distances 0-1. Re-scaling prevents comparisons dissimilarity values obtained runs different values percentage. parallel (logical) whether fit candidate models parallel. Default FALSE. ncores (numeric) number cores use parallel processing. Default NULL uses available cores - 1. applicable parallel = TRUE. progress_bar (logical) whether display progress bar processing. Default TRUE. overwrite (logical) whether overwrite SpatRaster already exists. applicable write_files set TRUE. Default FALSE.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_mop.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Analysis of extrapolation risks in projections using the MOP metric — projection_mop","text":"object class mop_projections, root directory dataframe containing file paths results stored scenario. paths contain following files: summary - data.frame details data used analysis: variables - names variables considered. type - type MOP analysis performed. scale - value according argument scale. center - value according argument center. calculate_distance - value according argument calculate_distance. distance - option regarding distance used. percentage - percentage m used reference distance calculation. rescale_distance - value according argument rescale_distance. fix_NA - value according argument fix_NA. N_m - total number elements (cells values valid rows) m. N_g - total number elements (cells values valid rows) g. m_min - minimum values (lower limit) variables reference conditions (m). m_max - maximum values (upper limit) variables reference conditions (m). mop_distances - calculate_distance = TRUE, SpatRaster vector distance values set interest (g). Higher values represent greater dissimilarity compared set reference (m). mop_basic - SpatRaster vector, set interest, representing conditions least one variables non-analogous set reference. Values : 1 non-analogous conditions, NA conditions inside ranges reference set. mop_simple - SpatRaster vector, set interest, representing many variables set interest non-analogous reference set. NA used conditions inside ranges reference set. mop_detailed - list containing: interpretation_combined - data.frame help identify combinations variables towards_low_combined towards_high_combined non-analogous m. towards_low_end - SpatRaster matrix variables representing non-analogous conditions found towards low values variable. towards_high_end - SpatRaster matrix variables representing non-analogous conditions found towards high values variable. towards_low_combined - SpatRaster vector values representing identity variables found non-analogous conditions towards low values. vector, interpretation requires use data.frame interpretation_combined. towards_high_combined - SpatRaster vector values representing identity variables found non-analogous conditions towards high values. vector, interpretation requires use data.frame interpretation_combined.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_mop.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Analysis of extrapolation risks in projections using the MOP metric — projection_mop","text":"type options return results differ detail non-analogous conditions identified. basic - makes calculation proposed Owens et al. (2013) doi:10.1016/j.ecolmodel.2013.04.011. simple - calculates many variables set interest non-analogous reference set. detailed - calculates five additional extrapolation metrics. See mop_detailed Value full details. where_distance options determine values used calculate dissimilarity in_range - conditions inside m ranges out_range - conditions outside m ranges - conditions variables used represent conditions different units, scaling centering recommended. step valid Euclidean distances used.","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_mop.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Analysis of extrapolation risks in projections using the MOP metric — projection_mop","text":"","code":"# Step 1: Organize variables for current projection ## Import current variables (used to fit models) var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  ## Create a folder in a temporary directory to copy the variables out_dir_current <- file.path(tempdir(), \"Current_raw4\") dir.create(out_dir_current, recursive = TRUE)  ## Save current variables in temporary directory terra::writeRaster(var, file.path(out_dir_current, \"Variables.tif\"))   # Step 2: Organize future climate variables (example with WorldClim) ## Directory containing the downloaded future climate variables (example) in_dir <- system.file(\"extdata\", package = \"kuenm2\")  ## Create a folder in a temporary directory to copy the future variables out_dir_future <- file.path(tempdir(), \"Future_raw4\")  ## Organize and rename the future climate data (structured by year and GCM) ### 'SoilType' will be appended as a static variable in each scenario organize_future_worldclim(input_dir = in_dir, output_dir = out_dir_future,                           name_format = \"bio_\", fixed_variables = var$SoilType) #>    |                                                                               |                                                                      |   0%   |                                                                               |=========                                                             |  12%   |                                                                               |==================                                                    |  25%   |                                                                               |==========================                                            |  38%   |                                                                               |===================================                                   |  50%   |                                                                               |============================================                          |  62%   |                                                                               |====================================================                  |  75%   |                                                                               |=============================================================         |  88%   |                                                                               |======================================================================| 100% #>  #> Variables successfully organized in directory: #> /tmp/Rtmp3eUeuI/Future_raw4  # Step 3: Prepare data to run multiple projections ## An example with maxnet models ## Import example of fitted_models (output of fit_selected()) data(fitted_model_maxnet, package = \"kuenm2\")  ## Prepare projection data using fitted models to check variables pr <- prepare_projection(models = fitted_model_maxnet,                          present_dir = out_dir_current,                          future_dir = out_dir_future,                          future_period = c(\"2041-2060\", \"2081-2100\"),                          future_pscen = c(\"ssp126\", \"ssp585\"),                          future_gcm = c(\"ACCESS-CM2\", \"MIROC6\"),                          raster_pattern = \".tif*\")  # Step 4: Perform MOP for all projection scenarios ## Create a folder to save MOP results out_dir <- file.path(tempdir(), \"MOPresults\") dir.create(out_dir, recursive = TRUE)  ## Run MOP kmop <- projection_mop(data = fitted_model_maxnet, projection_data = pr,                        out_dir = out_dir, type = \"detailed\") #>    |                                                                               |                                                                      |   0%   |                                                                               |========                                                              |  11%   |                                                                               |================                                                      |  22%   |                                                                               |=======================                                               |  33%   |                                                                               |===============================                                       |  44%   |                                                                               |=======================================                               |  56%   |                                                                               |===============================================                       |  67%   |                                                                               |======================================================                |  78%   |                                                                               |==============================================================        |  89%   |                                                                               |======================================================================| 100%"},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_variability.html","id":null,"dir":"Reference","previous_headings":"","what":"Explores variance coming from distinct sources in model predictions — projection_variability","title":"Explores variance coming from distinct sources in model predictions — projection_variability","text":"Calculates variance model predictions, distinguishing different sources variation. Potential sources include replicates, model parameterizations, general circulation models (GCMs).","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_variability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Explores variance coming from distinct sources in model predictions — projection_variability","text":"","code":"projection_variability(model_projections, by_replicate = TRUE, by_gcm = TRUE,                        by_model = TRUE, consensus = \"median\",                        write_files = FALSE, output_dir = NULL,                        return_rasters = TRUE, progress_bar = FALSE,                        verbose = TRUE, overwrite = FALSE)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_variability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Explores variance coming from distinct sources in model predictions — projection_variability","text":"model_projections model_projections object generated project_selected() function. object contains file paths raster projection results thresholds used binarizing predictions. by_replicate (logical) whether compute variance originating replicates. by_gcm (logical) whether compute variance originating general circulation models (GCMs) by_model (logical) whether compute variance originating model parameterizations. consensus (character) (character) consensus measure use calculating changes. Available options 'mean', 'median', 'range', 'stdev' (standard deviation). Default 'median'. write_files (logical) whether write raster files containing computed variance disk. Default FALSE. output_dir (character) directory path resulting raster files containing computed changes saved. relevant write_results = TRUE. return_rasters (logical) whether return list containing SpatRasters computed changes. Default TRUE. Setting argument FALSE returns NULL object. progress_bar (logical) whether display progress bar processing. Default TRUE. verbose (logical) whether display messages processing. Default TRUE. overwrite whether overwrite SpatRaster already exists. applicable write_files set TRUE. Default FALSE.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_variability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Explores variance coming from distinct sources in model predictions — projection_variability","text":"object class variability_projections. return_rasters = TRUE, function returns list containing SpatRasters computed variances, categorized replicate, model, GCMs. write_files = TRUE, also returns directory path computed rasters saved disk, object can used import files later import_projections() function. return_rasters = FALSE write_files = FALSE, function returns NULL","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/projection_variability.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Explores variance coming from distinct sources in model predictions — projection_variability","text":"","code":"# Step 1: Organize variables for current projection ## Import current variables (used to fit models) var <- terra::rast(system.file(\"extdata\", \"Current_variables.tif\",                                package = \"kuenm2\"))  ## Create a folder in a temporary directory to copy the variables out_dir_current <- file.path(tempdir(), \"Current_raw5\") dir.create(out_dir_current, recursive = TRUE)  ## Save current variables in temporary directory terra::writeRaster(var, file.path(out_dir_current, \"Variables.tif\"))   # Step 2: Organize future climate variables (example with WorldClim) ## Directory containing the downloaded future climate variables (example) in_dir <- system.file(\"extdata\", package = \"kuenm2\")  ## Create a folder in a temporary directory to copy the future variables out_dir_future <- file.path(tempdir(), \"Future_raw5\")  ## Organize and rename the future climate data (structured by year and GCM) ### 'SoilType' will be appended as a static variable in each scenario organize_future_worldclim(input_dir = in_dir, output_dir = out_dir_future,                           name_format = \"bio_\", fixed_variables = var$SoilType) #>    |                                                                               |                                                                      |   0%   |                                                                               |=========                                                             |  12%   |                                                                               |==================                                                    |  25%   |                                                                               |==========================                                            |  38%   |                                                                               |===================================                                   |  50%   |                                                                               |============================================                          |  62%   |                                                                               |====================================================                  |  75%   |                                                                               |=============================================================         |  88%   |                                                                               |======================================================================| 100% #>  #> Variables successfully organized in directory: #> /tmp/Rtmp3eUeuI/Future_raw5  # Step 3: Prepare data to run multiple projections ## An example with maxnet models ## Import example of fitted_models (output of fit_selected()) data(fitted_model_maxnet, package = \"kuenm2\")  ## Prepare projection data using fitted models to check variables pr <- prepare_projection(models = fitted_model_maxnet,                          present_dir = out_dir_current,                          future_dir = out_dir_future,                          future_period = \"2041-2060\",                          future_pscen = \"ssp126\",                          future_gcm = c(\"ACCESS-CM2\", \"MIROC6\"),                          raster_pattern = \".tif*\")  # Step 4: Run multiple model projections ## A folder to save projection results out_dir <- file.path(tempdir(), \"Projection_results/maxnet3\") dir.create(out_dir, recursive = TRUE)  ## Project selected models to multiple scenarios p <- project_selected(models = fitted_model_maxnet, projection_data = pr,                       out_dir = out_dir) #>    |                                                                               |                                                                      |   0%   |                                                                               |=======================                                               |  33%   |                                                                               |===============================================                       |  67%   |                                                                               |======================================================================| 100%  # Step 5: Compute variance from distinct sources v <- projection_variability(model_projections = p, by_replicate = FALSE) #> Calculating variability from distinct models: scenario 1 of 2 #> Calculating variability from distinct models: scenario 2 of 2 #> Calculating variability from distinct GCMs: scenario 2 of 2  #terra::plot(v$Present$by_replicate)  # Variance from replicates, present projection terra::plot(v$Present$by_model)  # From models  #terra::plot(v$`Future_2041-2060_ssp126`$by_replicate)  # From replicates in future projection terra::plot(v$`Future_2041-2060_ssp126`$by_model)  # From models  terra::plot(v$`Future_2041-2060_ssp126`$by_gcm)  # From GCMs"},{"path":"https://marlonecobos.github.io/kuenm2/reference/response_curve.html","id":null,"dir":"Reference","previous_headings":"","what":"Variable response curves for fitted models — response_curve","title":"Variable response curves for fitted models — response_curve","text":"Plot variable responses fitted models. Responses based single multiple models can plotted.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/response_curve.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Variable response curves for fitted models — response_curve","text":"","code":"# Single variable response curves response_curve(models, variable, modelID = NULL, n = 100,                show_variability = FALSE, show_lines = FALSE, data = NULL,                new_data = NULL, averages_from = \"pr_bg\", extrapolate = TRUE,                extrapolation_factor = 0.1, add_points = FALSE, p_col = NULL,                l_limit = NULL, u_limit = NULL, xlab = NULL,                ylab = \"Suitability\", col = \"darkblue\", ...)  # Response curves for all variables in all or individual models all_response_curves(models, modelID = NULL, n = 100, show_variability = FALSE,                     show_lines = FALSE, data = NULL, new_data = NULL,                     averages_from = \"pr_bg\", extrapolate = TRUE,                     extrapolation_factor = 0.1, add_points = FALSE,                     p_col = NULL, l_limit = NULL, u_limit = NULL,                     xlab = NULL, ylab = \"Suitability\", col = \"darkblue\",                     ylim = NULL, mfrow = NULL, ...)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/response_curve.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Variable response curves for fitted models — response_curve","text":"models object class fitted_models returned fit_selected() function. variable (character) name variable plotted. modelID (character) ModelID(s) considered. default IDs models included. Default = NULL. n (numeric) integer guiding number breaks produce curve. Default = 100. show_variability (logical) modelID defined, shows variability response curves considering replicates. modelID defined, default, FALSE, always shows variability multiple models present models. show_lines (logical) whether show variability plotting lines models replicates. default = FALSE, uses GAM characterize median trend variation among modes replicates. Ignored show_variability = FALSE modelID defined. data data.frame matrix data used model calibration step. default, NULL, uses data stored models. new_data SpatRaster, data.frame,  matrix values variables representing area scenario interest model projection. Default = NULL. averages_from (character) specifies averages modes variables calculated producing responses variable interest. Options \"pr\" (presences) \"pr_bg\" (presences background). Default \"pr_bg\". See details. extrapolate (logical) whether allow extrapolation response outside training conditions. Ignored new_data defined. Default = TRUE. extrapolation_factor (numeric) value used calculate much expand training region extrapolation. Larger values produce extrapolation farther training limits. Default = 0.1. add_points (logical) TRUE, adds original observed points (0/1) plot. also sets ylim = c(0, 1), unless limits defined part .... Default = FALSE. p_col (character) color observed points add_points = TRUE. valid R color name hexadecimal code. Default = \"black\". l_limit (numeric) directly specifies lower limit variable. Default = NULL, meaning lower limit calculated existing data. (extrapolation = TRUE). u_limit (numeric) directly specifies upper limit variable. Default = NULL, meaning aupper limit calculated existing data. (extrapolation = TRUE). xlab (character) label x axis. default, NULL, uses name defined variable. ylab (character) label y axis. Default = \"Suitability\". col (character) color lines. Default = \"darkblue\". ... additional arguments passed plot. ylim (numeric) vector length two limits y axis. Directly used all_response_curves. Default = NULL. mfrow (numeric) vector specifying number rows columns plot layout, e.g., c(rows, columns). Default NULL, meaning grid arranged automatically based number plots.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/response_curve.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Variable response curves for fitted models — response_curve","text":"response_curve(), plot response curve variable. all_response_curves(), multipanel plot response curves fro variables models.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/response_curve.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Variable response curves for fitted models — response_curve","text":"response curve variable interest generated variables set mean values (mode categorical variables), calculated either presence records (averages_from = \"pr\") combined set presence background records (averages_from = \"pr_bg\"). categorical variables, bar plot generated error bars showing variability across models (multiple models included).","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/response_curve.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Variable response curves for fitted models — response_curve","text":"","code":"# Example with maxnet # Import example of fitted_models (output of fit_selected()) data(fitted_model_maxnet, package = \"kuenm2\")  # Response curves for one variable at a time response_curve(models = fitted_model_maxnet, variable = \"bio_1\")  response_curve(models = fitted_model_maxnet, variable = \"bio_1\",                add_points = TRUE)  response_curve(models = fitted_model_maxnet, variable = \"bio_1\",                show_lines = TRUE)   response_curve(models = fitted_model_maxnet, variable = \"bio_1\",                modelID = \"Model_192\", show_variability = TRUE)  response_curve(models = fitted_model_maxnet, variable = \"bio_1\",                modelID = \"Model_192\", show_variability = TRUE,                show_lines = TRUE)   # Example with maxnet # Import example of fitted_models (output of fit_selected()) data(fitted_model_maxnet, package = \"kuenm2\")  # Response curves for all variables at once all_response_curves(fitted_model_maxnet)  all_response_curves(fitted_model_maxnet, show_lines = TRUE)  all_response_curves(fitted_model_maxnet, show_lines = TRUE,                     add_points = TRUE)   all_response_curves(fitted_model_maxnet, modelID = \"Model_192\",                     show_variability = TRUE, show_lines = TRUE)  all_response_curves(fitted_model_maxnet, modelID = \"Model_192\",                     show_variability = TRUE, show_lines = TRUE,                     add_points = TRUE)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/select_models.html","id":null,"dir":"Reference","previous_headings":"","what":"Select models that perform the best among candidates — select_models","title":"Select models that perform the best among candidates — select_models","text":"function selects best models according user-defined criteria, evaluating statistical significance (partial ROC), predictive ability (omission rates), model complexity (AIC).","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/select_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select models that perform the best among candidates — select_models","text":"","code":"select_models(calibration_results = NULL, candidate_models = NULL, data = NULL,               algorithm = NULL, compute_proc = FALSE,               addsamplestobackground = TRUE, weights = NULL,               remove_concave = FALSE, omission_rate = NULL,               allow_tolerance = TRUE, tolerance = 0.01,               significance = 0.05, delta_aic = 2, parallel = FALSE,               ncores = NULL, progress_bar = FALSE,verbose = TRUE)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/select_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select models that perform the best among candidates — select_models","text":"calibration_results object class calibration_results returned calibration() function. Default NULL. candidate_models (data.frame) summary evaluation metrics candidate model. Required calibration_results NULL. output calibration(), data.frame located $calibration_results$Summary. Default NULL. data object class prepared_data returned prepare_data() function. Required calibration_results NULL compute_proc TRUE. algorithm (character) model algorithm, either \"glm\" \"maxnet\". default, NULL, uses one defined part calibration_results, data. arguments used, algorithm must defined. compute_proc (logical) whether compute partial ROC tests selected models. required partial ROC calculated candidate models calibration. Default FALSE. addsamplestobackground (logical) whether add background presence sample already . Required compute_proc TRUE calibration_results NULL.Default TRUE. weights (numeric) numeric vector specifying weights occurrence records. Required compute_proc TRUE calibration_results NULL. Default NULL. remove_concave (logical) whether remove candidate models presenting concave curves. Default FALSE. omission_rate (numeric) maximum omission rate candidate model can considered potentially selected model. default, NULL, uses value provided part calibration_results. purposes selection existing results evaluation, value must match one values used omission tests, must manually defined. allow_tolerance (logical) whether allow selection models minimum values omission rates even omission rate surpasses omission_rate. applicable candidate models omission rates higher omission_rate. Default TRUE. tolerance (numeric) value added minimum omission rate exceeds omission_rate. allow_tolerance = TRUE, selected models omission rate equal less minimum rate plus tolerance. Default 0.01. significance (numeric) significance level select models based partial ROC (pROC). Default 0.05. See Details. delta_aic (numeric) value delta AIC used threshold select models. Default 2. parallel (logical) whether calculate PROC candidate models parallel. Default FALSE. ncores (numeric) number cores use parallel processing. Default NULL uses available cores - 1. applicable parallel = TRUE. progress_bar (logical) whether display progress bar processing. Default TRUE. verbose (logical) whether display messages processing. Default TRUE.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/select_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select models that perform the best among candidates — select_models","text":"calibration_results provided, returns new calibration_results new selected models summary. calibration_results NULL, returns list containing following elements: selected_models: data frame ID summary evaluation metrics selected models. summary: list containing delta AIC values model selection, ID values models failed fit, concave curves, non-significant pROC values, omission rates threshold, delta AIC values threshold, selected models.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/select_models.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Select models that perform the best among candidates — select_models","text":"Partial ROC calculated following Peterson et al. (2008).","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/select_models.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Select models that perform the best among candidates — select_models","text":"","code":"# Import example of calibration results (output of calibration function) ## GLM data(calib_results_glm, package = \"kuenm2\")  #Select new best models based on another value of omission rate new_best_model <- select_models(calibration_results = calib_results_glm,                                 algorithm = \"glm\", compute_proc = TRUE,                                 omission_rate = 10)  # Omission error of 10 #> Selecting best among 122 models. #> Calculating pROC... #>  #> Filtering 122 models. #> Removing 0 model(s) because they failed to fit. #> 21 model(s) were selected with omission rate below 10%. #> Selecting 1 final model(s) with delta AIC <2. #> Validating pROC of selected models... #>  #> All selected models have significant pROC values.  # Compare with best models selected previously calib_results_glm$summary$Selected  # Model 86 selected #> [1] 85 new_best_model$summary$Selected  # Models 64, 73 and 86 selected #> [1] 85"},{"path":"https://marlonecobos.github.io/kuenm2/reference/sp_swd.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepared Data for maxnet models — sp_swd","title":"Prepared Data for maxnet models — sp_swd","text":"prepared_data object resulted prepare_data() calibrate models using 'glm' algorithm.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/sp_swd.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepared Data for maxnet models — sp_swd","text":"","code":"data(\"sp_swd\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/sp_swd.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Prepared Data for maxnet models — sp_swd","text":"prepared_data object following elements: species Species names calibration_data data.frame containing variables extracted presence background points formula_grid data.frame ID, formulas, regularization multipliers candidate model part_data list partition data, element corresponds replicate contains indices test points replicate partition_method character indicating partition method n_replicates numeric value indicating number replicates k-folds train_proportion numeric value indicating proportion occurrences used train points partition method 'subsample' 'boostrap' data_xy data.frame coordinates occurrence bakground points continuous_variables character indicating names continuous variables categorical_variables character indicating names categorical variables weights numeric value specifying weights occurrence records. NULL, meaning set weights. pca prcomp object storing PCA information. NULL, meaning PCA performed algorithm character indicanting algorithm (glm)","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/sp_swd_glm.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepared Data for glm models — sp_swd_glm","title":"Prepared Data for glm models — sp_swd_glm","text":"prepared_data object resulted prepare_data() calibrate models using 'glm' algorithm.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/sp_swd_glm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepared Data for glm models — sp_swd_glm","text":"","code":"data(\"sp_swd_glm\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/sp_swd_glm.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Prepared Data for glm models — sp_swd_glm","text":"prepared_data object following elements: species Species names calibration_data data.frame containing variables extracted presence background points formula_grid data.frame ID, formulas, regularization multipliers candidate model part_data list partition data, element corresponds replicate contains indices test points replicate partition_method character indicating partition method n_replicates numeric value indicating number replicates k-folds train_proportion numeric value indicating proportion occurrences used train points partition method 'subsample' 'boostrap' data_xy data.frame coordinates occurrence bakground points continuous_variables character indicating names continuous variables categorical_variables character indicating names categorical variables weights numeric value specifying weights occurrence records. NULL, meaning set weights. pca prcomp object storing PCA information. NULL, meaning PCA performed algorithm character indicanting algorithm (glm)","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/swd_spatial_block.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepared data with spatial blocks created with ENMeval — swd_spatial_block","title":"Prepared data with spatial blocks created with ENMeval — swd_spatial_block","text":"prepared_data object resulted prepare_data() calibrate models using 'glmnet' algorithm. object, original partitioning replaced spatial blocks generated using get.block() method ENMeval R package.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/swd_spatial_block.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepared data with spatial blocks created with ENMeval — swd_spatial_block","text":"","code":"data(\"sp_swd\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/swd_spatial_block.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Prepared data with spatial blocks created with ENMeval — swd_spatial_block","text":"object class prepared_data length 13.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/user_data.html","id":null,"dir":"Reference","previous_headings":"","what":"User Custom Calibration Data — user_data","title":"User Custom Calibration Data — user_data","text":"data.frame containing presence background records along environmental variables used demonstrate data preparation user-supplied data.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/user_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"User Custom Calibration Data — user_data","text":"","code":"data(\"user_data\")"},{"path":"https://marlonecobos.github.io/kuenm2/reference/user_data.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"User Custom Calibration Data — user_data","text":"data.frame following columns: pr_bg Column indicating presences (1) background (0). bio_1 extracted values variable bio_1 presence background points. bio_7 extracted values variable bio_12 presence background points. bio_12 extracted values variable bio_12 presence background points. bio_15 extracted values variable bio_15 presence background points. bio_15 extracted values variable soilType presence background points.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/var.html","id":null,"dir":"Reference","previous_headings":"","what":"SpatRaster Representing present-day Conditions (WorldClim) — var","title":"SpatRaster Representing present-day Conditions (WorldClim) — var","text":"Raster layer containing bioclimatic variables representing present-day climatic conditions. variables obtained 10 arc-minute resolution masked using m region provided package. Data sourced WorldClim: https://worldclim.org/data/worldclim21.html","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/var.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SpatRaster Representing present-day Conditions (WorldClim) — var","text":"SpatRaster object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/var.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SpatRaster Representing present-day Conditions (WorldClim) — var","text":"return value. Used function rast bring raster variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/var.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SpatRaster Representing present-day Conditions (WorldClim) — var","text":"","code":"var <- terra::rast(system.file(\"extdata\",                                \"Current_variables.tif\",                                 package = \"kuenm2\")) terra::plot(var)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/variable_importance.html","id":null,"dir":"Reference","previous_headings":"","what":"Variable importance — variable_importance","title":"Variable importance — variable_importance","text":"Variable importance","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/variable_importance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Variable importance — variable_importance","text":"","code":"variable_importance(models, modelID = NULL, by_terms = FALSE,                     parallel = FALSE, ncores = NULL,                     progress_bar = TRUE, verbose = TRUE)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/variable_importance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Variable importance — variable_importance","text":"models object class fitted_models returned fit_selected() function. modelID (character). Default = NULL. by_terms (logical) whether calculate importance model terms (e.g., bio1, (bio1^2), hinge(bio1)) instead aggregating variable. Default = FALSE. parallel (logical) whether calculate importance parallel. Default FALSE. ncores (numeric) number cores use parallel processing. Default NULL uses available cores - 1. applicable parallel = TRUE. progress_bar (logical) whether display progress bar processing. Default TRUE. verbose (logical) whether display detailed messages processing. Default TRUE.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/variable_importance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Variable importance — variable_importance","text":"data.frame containing relative contribution variable (term by_terms = TRUE). identification distinct models added fitted contains multiple models.","code":""},{"path":[]},{"path":"https://marlonecobos.github.io/kuenm2/reference/variable_importance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Variable importance — variable_importance","text":"","code":"# Example with maxnet # Import example of fitted_models (output of fit_selected()) data(fitted_model_maxnet, package = \"kuenm2\")  # Variable importance imp_maxnet <- variable_importance(models = fitted_model_maxnet) #>  #> Calculating variable contribution for model 1 of 2 #>    |                                                                               |                                                                      |   0%   |                                                                               |=======================                                               |  33%   |                                                                               |===============================================                       |  67%   |                                                                               |======================================================================| 100% #>  #> Calculating variable contribution for model 2 of 2 #>    |                                                                               |                                                                      |   0%   |                                                                               |==================                                                    |  25%   |                                                                               |===================================                                   |  50%   |                                                                               |====================================================                  |  75%   |                                                                               |======================================================================| 100%  # Plot plot_importance(imp_maxnet)   # Example with glm # Import example of fitted_models (output of fit_selected()) data(fitted_model_glm, package = \"kuenm2\")  # Variable importance imp_glm <- variable_importance(models = fitted_model_glm) #>  #> Calculating variable contribution for model 1 of 1 #>    |                                                                               |                                                                      |   0%   |                                                                               |=======================                                               |  33%   |                                                                               |===============================================                       |  67%   |                                                                               |======================================================================| 100%  # Plot plot_importance(imp_glm)"},{"path":"https://marlonecobos.github.io/kuenm2/reference/world.html","id":null,"dir":"Reference","previous_headings":"","what":"World country polygons from Natural Earth — world","title":"World country polygons from Natural Earth — world","text":"spatial vector world countries. simplified version countries110 rnaturalearth R package.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/world.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"World country polygons from Natural Earth — world","text":"Spatvector object.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/world.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"World country polygons from Natural Earth — world","text":"return value. Used function vect bring vector variables analysis.","code":""},{"path":"https://marlonecobos.github.io/kuenm2/reference/world.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"World country polygons from Natural Earth — world","text":"","code":"m <- terra::vect(system.file(\"extdata\",                              \"world.gpkg\",                               package = \"kuenm2\")) terra::plot(m)"}]
